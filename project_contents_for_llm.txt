--- File: requirements.txt ---
# onto_rag/requirements.txt

# Core Ontology Processing
rdflib>=6.0.0

# Lexical Indexing
whoosh>=2.7.0

# Embeddings & Semantic Search
sentence-transformers>=2.2.0
faiss-cpu>=1.7.0   # Or faiss-gpu if you have a CUDA-enabled GPU and want to use it

# Utilities
numpy>=1.21.0     # Often a dependency, good to specify
pandas>=1.3.0     # Dependency for libraries like 'datasets' used by sentence-transformers
python-dotenv>=0.19.0 # For loading .env files
tqdm>=4.60.0      # For progress bars in long operations

# Testing (optional, but good for development)
pytest>=6.2.0
--- END File: requirements.txt ---

--- File: README.md ---
# onto_rag
A Retrieval Augmented Generation system for matching extracted entities to ontologies

--- END File: README.md ---

--- File: .gitignore ---
# Byte-compiled / cache
__pycache__/
*.py[cod]
.pytest_cache/

# Environment & secrets
.env
api_key.json

# VSCode, macOS
.vscode/
.DS_Store

# Large generated data and ontology files
/data/
/ontologies/
/models/
--- END File: .gitignore ---

--- File: export_files_contents.py ---
import os
import argparse

# --- Configuration ---
# Files or directories to explicitly skip by their exact name or path (relative to root_dir)
EXCLUDE_ITEMS_EXACT = {
    ".git",
    "__pycache__",
    "data",               # <<< Exclude the entire data directory
    "docs",               # <<< Exclude the entire docs directory
    "ontologies",         # <<< Exclude the entire ontologies directory
    "api_key.json",       # Sensitive file
    ".env",               # Sensitive file (though .env.example is fine)
    "models",
    ".DS_Store",
    # Add other specific files or directories if needed
    # e.g. "some_large_binary_asset.dat"
}

# File extensions to skip (typically binary or non-text files)
EXCLUDE_EXTENSIONS = {
    ".pyc",
    ".pyo",
    ".bin",
    ".exe",
    ".dll",
    ".so",
    ".o",
    ".a",
    ".lib",
    ".jar",
    ".war",
    ".ear",
    ".class",
    ".swo",
    ".swp",
    # Image/Media files
    ".jpg", ".jpeg", ".png", ".gif", ".bmp", ".tiff", ".webp",
    ".mp3", ".wav", ".ogg", ".mp4", ".avi", ".mkv", ".mov",
    # Archives
    ".zip", ".tar", ".gz", ".bz2", ".rar", ".7z",
    # Other common binary formats
    ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx",
    ".odt", ".ods", ".odp",
    ".sqlite", ".db",
    ".pkl", ".joblib", # Python pickled objects
    ".pt", ".pth", ".pb", ".onnx", # Model files
    ".DS_Store" # macOS specific
}

# --- End Configuration ---

def should_skip(item_path, root_dir, output_filename):
    """
    Determines if a file or directory should be skipped.
    item_path: absolute path to the item.
    root_dir: absolute path to the project's root directory.
    output_filename: name of the script's output file.
    """
    relative_item_path = os.path.relpath(item_path, root_dir)

    # Skip the output file itself
    if os.path.basename(item_path) == output_filename:
        return True

    # Check against exact items to exclude (can be dir names or file names or relative paths)
    # Normalize paths for comparison
    normalized_relative_item_path = relative_item_path.replace(os.sep, "/")
    for excluded in EXCLUDE_ITEMS_EXACT:
        normalized_excluded = excluded.replace(os.sep, "/")
        if normalized_relative_item_path == normalized_excluded or \
           normalized_relative_item_path.startswith(normalized_excluded + "/"):
            # print(f"Skipping '{relative_item_path}' due to exact match/prefix with '{excluded}'")
            return True
        # Also check just the basename for simple directory/file names at the root level
        # if os.path.dirname(relative_item_path) == "" and os.path.basename(item_path) == excluded:
        # The above check for basename is now effectively covered by the first part of the condition
        # if normalized_relative_item_path (e.g., "data") == normalized_excluded (e.g., "data")


    # If it's a file, check its extension
    if os.path.isfile(item_path):
        _, ext = os.path.splitext(item_path)
        if ext.lower() in EXCLUDE_EXTENSIONS:
            # print(f"Skipping '{relative_item_path}' due to extension '{ext}'")
            return True

    return False

def generate_project_context(root_dir, output_filename="project_contents_for_llm.txt"):
    """
    Generates a text file containing the names and contents of project files.
    """
    root_dir_abs = os.path.abspath(root_dir)
    output_file_abs_path = os.path.join(root_dir_abs, output_filename)


    print(f"Starting project context generation for: {root_dir_abs}")
    print(f"Output will be saved to: {output_file_abs_path}")
    print(f"Excluded items (exact name/path relative to root): {EXCLUDE_ITEMS_EXACT}")
    print(f"Excluded extensions: {EXCLUDE_EXTENSIONS}")
    print("-" * 30)

    collected_files_count = 0
    skipped_files_count = 0
    skipped_dirs_count = 0

    with open(output_file_abs_path, "w", encoding="utf-8", errors="replace") as outfile:
        for dirpath, dirnames, filenames in os.walk(root_dir_abs, topdown=True):
            # Modify dirnames in-place to skip directories
            # This is important for os.walk(topdown=True)
            original_dirnames_count = len(dirnames)
            current_dir_relative_to_root = os.path.relpath(dirpath, root_dir_abs)

            # Filter out directories to be skipped
            dirs_to_keep = []
            for d in dirnames:
                dir_full_path = os.path.join(dirpath, d)
                if not should_skip(dir_full_path, root_dir_abs, output_filename):
                    dirs_to_keep.append(d)
                else:
                    # Print skipped directory relative to root for clarity
                    skipped_dir_relative_path = os.path.relpath(dir_full_path, root_dir_abs).replace(os.sep, "/")
                    print(f"Skipping directory (and its contents): {skipped_dir_relative_path}")
                    skipped_dirs_count += 1
            dirnames[:] = dirs_to_keep


            for filename in filenames:
                file_abs_path = os.path.join(dirpath, filename)
                relative_file_path = os.path.relpath(file_abs_path, root_dir_abs)
                # Normalize for display
                display_path = relative_file_path.replace(os.sep, "/")


                if should_skip(file_abs_path, root_dir_abs, output_filename):
                    # This check might be redundant for files if their parent dir was already skipped,
                    # but good for files directly in an otherwise included dir that match other skip criteria.
                    if not any(display_path.startswith(excluded_dir + "/") for excluded_dir in EXCLUDE_ITEMS_EXACT if os.path.isdir(os.path.join(root_dir_abs, excluded_dir))):
                         print(f"Skipping file: {display_path}") # Only print if not part of an already reported skipped dir
                    skipped_files_count += 1
                    continue

                print(f"Processing file: {display_path}")
                outfile.write(f"--- File: {display_path} ---\n")
                try:
                    with open(file_abs_path, "r", encoding="utf-8", errors="ignore") as infile:
                        content = infile.read()
                        outfile.write(content)
                except Exception as e:
                    outfile.write(f"[Error reading file: {e}]\n")
                outfile.write(f"\n--- END File: {display_path} ---\n\n")
                collected_files_count += 1

    print("-" * 30)
    print(f"Project context generation complete.")
    print(f"Collected content from {collected_files_count} files.")
    print(f"Skipped {skipped_files_count} files (may include files within explicitly skipped dirs).")
    print(f"Skipped {skipped_dirs_count} directories (and their contents).")
    print(f"Output saved to: {output_file_abs_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Scans a project directory and saves file names and contents to a text file for LLM context."
    )
    parser.add_argument(
        "root_dir",
        nargs="?",
        default=".",
        help="The root directory of the project to scan (default: current directory).",
    )
    parser.add_argument(
        "--output",
        default="project_contents_for_llm.txt",
        help="The name of the output file (default: project_contents_for_llm.txt).",
    )
    args = parser.parse_args()

    generate_project_context(args.root_dir, args.output)
--- END File: export_files_contents.py ---

--- File: .env.example ---
# OpenAI & vector DB credentials
OPENAI_API_KEY=your_openai_api_key_here
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_ENV=your_pinecone_env

--- END File: .env.example ---

--- File: .pytest_cache/CACHEDIR.TAG ---
Signature: 8a477f597d28d172789f06886806bc55
# This file is a cache directory tag created by pytest.
# For information about cache directory tags, see:
#	https://bford.info/cachedir/spec.html

--- END File: .pytest_cache/CACHEDIR.TAG ---

--- File: .pytest_cache/README.md ---
# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.

--- END File: .pytest_cache/README.md ---

--- File: .pytest_cache/.gitignore ---
# Created by pytest automatically.
*

--- END File: .pytest_cache/.gitignore ---

--- File: .pytest_cache/v/cache/nodeids ---
[
  "src/tests/test_lexical_index.py::test_index_exists",
  "src/tests/test_lexical_index.py::test_query_by_definition_keyword",
  "src/tests/test_lexical_index.py::test_query_by_label_exact",
  "src/tests/test_lexical_index.py::test_query_by_label_fuzzy",
  "src/tests/test_lexical_index.py::test_query_by_relations_keyword",
  "src/tests/test_lexical_index.py::test_query_by_synonym",
  "src/tests/test_parse_ontology.py::test_extract_definitions",
  "src/tests/test_parse_ontology.py::test_extract_hierarchy",
  "src/tests/test_parse_ontology.py::test_extract_labels_and_synonyms",
  "src/tests/test_parse_ontology.py::test_extract_relations",
  "src/tests/test_parse_ontology.py::test_load_ontology"
]
--- END File: .pytest_cache/v/cache/nodeids ---

--- File: .pytest_cache/v/cache/lastfailed ---
{
  "src/tests/test_lexical_index.py::test_index_exists": true,
  "src/tests/test_lexical_index.py::test_query_by_label_exact": true,
  "src/tests/test_lexical_index.py::test_query_by_label_fuzzy": true,
  "src/tests/test_lexical_index.py::test_query_by_synonym": true,
  "src/tests/test_lexical_index.py::test_query_by_definition_keyword": true,
  "src/tests/test_lexical_index.py::test_query_by_relations_keyword": true,
  "src/tests/test_parse_ontology.py::test_extract_labels_and_synonyms": true,
  "src/tests/test_parse_ontology.py::test_extract_definitions": true,
  "src/tests/test_parse_ontology.py::test_extract_hierarchy": true,
  "src/tests/test_parse_ontology.py::test_extract_relations": true
}
--- END File: .pytest_cache/v/cache/lastfailed ---

--- File: .pytest_cache/v/cache/stepwise ---
[]
--- END File: .pytest_cache/v/cache/stepwise ---

--- File: scripts/find_namespaces.py ---
# scripts/find_namespaces.py
import rdflib
from rdflib import URIRef
from collections import Counter
import re
import os
import sys

# Add project root to allow importing src.config
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.append(PROJECT_ROOT)
from src.config import FOODON_PATH # Or any other OWL file you want to inspect

def get_base_uri(uri_str):
    """
    Tries to extract a base URI from a full URI.
    e.g., http://purl.obolibrary.org/obo/FOODON_0000123 -> http://purl.obolibrary.org/obo/FOODON_
    e.g., http://www.w3.org/2000/01/rdf-schema#label -> http://www.w3.org/2000/01/rdf-schema#
    """
    if '#' in uri_str:
        return uri_str.rsplit('#', 1)[0] + '#'
    # Common OBO pattern: ends with an underscore followed by digits
    match_obo = re.match(r'(.+[_A-Z]+_)(\d+)$', uri_str)
    if match_obo:
        return match_obo.group(1)
    # General pattern: up to the last slash
    if '/' in uri_str:
        return uri_str.rsplit('/', 1)[0] + '/'
    return None

def main(ontology_path):
    print(f"Loading ontology from: {ontology_path}...")
    g = rdflib.Graph()
    try:
        g.parse(ontology_path) # format will be auto-detected
        print(f"Successfully parsed. Found {len(g)} triples.")
    except Exception as e:
        print(f"Error parsing ontology: {e}")
        return

    uris = set()
    for s, p, o in g:
        if isinstance(s, URIRef):
            uris.add(str(s))
        if isinstance(p, URIRef):
            uris.add(str(p))
        if isinstance(o, URIRef):
            uris.add(str(o))

    print(f"\nFound {len(uris)} unique URIs.")

    base_uri_counts = Counter()
    for uri in uris:
        base = get_base_uri(uri)
        if base:
            base_uri_counts[base] += 1

    print("\nPotential Base URIs (with counts of distinct full URIs using them):")
    # Sort by count descending
    for base, count in base_uri_counts.most_common():
        print(f"- \"{base}\": (used in {count} URIs)")

    print("\n--- rdflib's known namespaces (Prefix: Namespace) ---")
    for prefix, namespace in g.namespaces():
        print(f"  {prefix}: {namespace}")


if __name__ == "__main__":
    # You can make the ontology path an argument if you like
    # For now, using FOODON_PATH from config
    if not os.path.exists(FOODON_PATH):
        print(f"ERROR: FoodON ontology file not found at {FOODON_PATH}")
    else:
        main(FOODON_PATH)
--- END File: scripts/find_namespaces.py ---

--- File: scripts/rebuild_base.bash ---
#!/usr/bin/env bash
#
# Rebuild the whole Onto-RAG pipeline from raw ontology â†’ evaluation
# Usage: ./scripts/rebuild_all.sh            # runs with defaults
#        ./scripts/rebuild_all.sh --skip-eval   # skip final recall test
#
set -euo pipefail

ROOT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )/.." && pwd )"
cd "${ROOT_DIR}"

DATA_DIR="data"

echo "ðŸ§¹  Cleaning old artefacts â€¦"
rm -f "${DATA_DIR}"/faiss_index.bin  "${DATA_DIR}"/faiss_metadata.json
rm -rf "${DATA_DIR}/whoosh_index"

echo "â‘   Parsing ontology â€¦"
python -m src.ingestion.parse_ontology

echo "â‘¡  Creating enriched documents â€¦"
python -m src.ingestion.enrich_documents

echo "â‘¢  Embedding documents â€¦"
python -m src.embeddings.embed_docs

echo "â‘£  Building FAISS index â€¦"
python -m src.vector_store.faiss_store

echo "â‘¤  Building Whoosh index â€¦"
python -m src.ingestion.build_lexical_index

if [[ "${1-}" != "--skip-eval" ]]; then
  echo "â‘¥  Running evaluator â€¦"
  python -m src.evaluation.evaluate_retriever_recall
fi

echo "âœ…  Pipeline finished."

--- END File: scripts/rebuild_base.bash ---

--- File: src/config.py ---
# src/config.py
import os
from rdflib import Namespace # This is fine, though rdflib.Namespace is not directly used for string constants below.

# Project Root Directory
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "."))
# print(f"Project root directory: {PROJECT_ROOT}") # Keep for debugging if you like

# Data Directory (for ontology dump, indexes, etc.)
DATA_DIR = os.path.join(PROJECT_ROOT, "data")
os.makedirs(DATA_DIR, exist_ok=True)

# Ontologies Directory
ONTOLOGIES_DIR = os.path.join(PROJECT_ROOT, "ontologies")
FOODON_PATH = os.path.join(ONTOLOGIES_DIR, "foodon.owl")
TEST_FOODON_SNIPPET_PATH = os.path.join(ONTOLOGIES_DIR, "test_foodon_snippet.owl")


# Output file from parse_ontology.py
ONTOLOGY_DUMP_JSON = os.path.join(DATA_DIR, "ontology_dump.json")

# Output file for enriched documents
ENRICHED_DOCUMENTS_FILE = os.path.join(DATA_DIR, "enriched_documents.json")

# Output file for embeddings (used for building FAISS index)
EMBEDDINGS_FILE = os.path.join(DATA_DIR, "embeddings.json")

# Whoosh Index Directory
WHOOSH_INDEX_DIR = os.path.join(DATA_DIR, "whoosh_index")
os.makedirs(WHOOSH_INDEX_DIR, exist_ok=True)

# FAISS Index Paths
FAISS_INDEX_PATH = os.path.join(DATA_DIR, "faiss_index.bin")
FAISS_METADATA_PATH = os.path.join(DATA_DIR, "faiss_metadata.json")

# Embedding Model Configuration
# EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
EMBEDDING_MODEL_NAME = "Lajavaness/bilingual-embedding-large" 
# RERANKER_MODEL_NAME = "intfloat/e5-mistral-7b-instruct"
RERANKER_MODEL_NAME = "cross-encoder/ms-marco-MiniLM-L-6-v2"
EMBEDDING_BATCH_SIZE = 32 # For batch embedding script
EMBEDDING_DEVICE = 'cpu'  # or 'cuda' if available, for embedding script

# Default K values for retrieval
DEFAULT_K_LEXICAL = 20
DEFAULT_K_VECTOR = 20
DEFAULT_RERANK_K = DEFAULT_K_LEXICAL + DEFAULT_K_VECTOR

# Namespaces (using string constants for broader compatibility if rdflib not always imported)
RDFS_NS_STR = "http://www.w3.org/2000/01/rdf-schema#"
RDF_NS_STR = "http://www.w3.org/1999/02/22-rdf-syntax-ns#"
OWL_NS_STR = "http://www.w3.org/2002/07/owl#"
SKOS_NS_STR = "http://www.w3.org/2004/02/skos/core#"
OBO_NS_STR = "http://purl.obolibrary.org/obo/"
OBOINOWL_NS_STR = "http://www.geneontology.org/formats/oboInOwl#"
IAO_NS_STR = "http://purl.obolibrary.org/obo/IAO_"

# For rdflib usage where Namespace objects are preferred:
RDFS_NS = Namespace(RDFS_NS_STR)
RDF_NS = Namespace(RDF_NS_STR)
OWL_NS = Namespace(OWL_NS_STR)
SKOS_NS = Namespace(SKOS_NS_STR)
OBO_NS = Namespace(OBO_NS_STR)
OBOINOWL_NS = Namespace(OBOINOWL_NS_STR)
IAO_NS = Namespace(IAO_NS_STR)


# Mapping of common relation URIs/CURIEs to human-readable names and default prefixes
# Used by parse_ontology.py and potentially enrich_documents.py
RELATION_CONFIG = {
    "obo:BFO_0000050": {"label": "part of", "prefix": "obo"},
    "obo:RO_0001000": {"label": "derives from", "prefix": "obo"},
    "obo:RO_0002200": {"label": "has phenotype", "prefix": "obo"},
    "obo:RO_0002451": {"label": "has part", "prefix": "obo"},
    "obo:FOODON_0000246": {"label": "has ingredient", "prefix": "obo"},
    "obo:RO_0000056": {"label": "participates in", "prefix": "obo"},
    "obo:RO_0000057": {"label": "has participant", "prefix": "obo"},
    "obo:RO_0000085": {"label": "functionally related to", "prefix": "obo"},
    "obo:RO_0002090": {"label": "adjacent to", "prefix": "obo"},
    "obo:RO_0002131": {"label": "overlaps", "prefix": "obo"},
    "obo:RO_0002150": {"label": "connected to", "prefix": "obo"},
    "obo:RO_0002202": {"label": "develops from", "prefix": "obo"},
    "obo:RO_0002350": {"label": "member of", "prefix": "obo"},
    "obo:RO_0002351": {"label": "has member", "prefix": "obo"},
    "obo:RO_0002353": {"label": "output of", "prefix": "obo"},
    "obo:RO_0002440": {"label": "causally related to", "prefix": "obo"},
    "obo:RO_0002450": {"label": "contains", "prefix": "obo"},
    "obo:RO_0002500": {"label": "surrounds", "prefix": "obo"},
    "obo:RO_0002573": {"label": "has quality", "prefix": "obo"},
    "obo:RO_0002606": {"label": "is transformation of", "prefix": "obo"},
    "obo:RO_0002607": {"label": "has input", "prefix": "obo"},
    "obo:RO_0003000": {"label": "used in", "prefix": "obo"},
    "obo:FOODON_0000380": {"label": "has preparation method", "prefix": "obo"},
    "obo:FOODON_0000226": {"label": "has food source", "prefix": "obo"},
    "obo:FOODON_0000388": {"label": "has maturity state", "prefix": "obo"},
    "obo:FOODON_0000286": {"label": "has packaging", "prefix": "obo"},
    "obo:FOODON_0000240": {"label": "has preservation method", "prefix": "obo"},
    "obo:FOODON_0000440": {"label": "has physical state", "prefix": "obo"},
    # "obo:FOODON_": {"label": "FoodON specific relation", "prefix": "obo"}, # This generic one might be too broad
    "obo:ERO_0000039": {"label": "has nutrient", "prefix": "obo"},
    "obo:ERO_0000589": {"label": "dietary context of", "prefix": "obo"},
    "obo:NCIT_C25277": {"label": "is allergen of", "prefix": "obo"},
    "obo:NCIT_C48670": {"label": "has active ingredient", "prefix": "obo"},
    "obo:NCIT_C64548": {"label": "part of human diet", "prefix": "obo"},
    "obo:PATO_0000001": {"label": "has quality PATO", "prefix": "obo"},
}

TARGET_RELATIONS_CURIES = list(RELATION_CONFIG.keys())

# CURIE Prefix Map for uri_to_curie and curie_to_uri conversions
# Ensure the keys are the *base URIs* that prefixes are expected for.
CURIE_PREFIX_MAP = {
    "http://purl.obolibrary.org/obo/FOODON_": "FOODON",
    "http://purl.obolibrary.org/obo/BFO_": "BFO",
    "http://purl.obolibrary.org/obo/RO_": "RO",
    "http://purl.obolibrary.org/obo/ERO_": "ERO",
    "http://purl.obolibrary.org/obo/NCIT_": "NCIT",
    "http://purl.obolibrary.org/obo/PATO_": "PATO",
    "http://purl.obolibrary.org/obo/IAO_": "IAO",
    "http://www.w3.org/2000/01/rdf-schema#": "rdfs",
    "http://www.w3.org/1999/02/22-rdf-syntax-ns#": "rdf",
    "http://www.w3.org/2002/07/owl#": "owl",
    "http://www.w3.org/2004/02/skos/core#": "skos",
    "http://www.geneontology.org/formats/oboInOwl#": "oboInOwl",
    "http://purl.obolibrary.org/obo/HANCESTRO_": "HANCESTRO",  
    "http://purl.obolibrary.org/obo/GAZ_": "GAZ",
    "http://purl.obolibrary.org/obo/CHEBI_": "CHEBI",
    "http://purl.obolibrary.org/obo/NCBITaxon_": "NCBITaxon",
    "http://purl.obolibrary.org/obo/UBERON_": "UBERON",
    "http://purl.obolibrary.org/obo/ENVO_": "ENVO",
    "http://purl.obolibrary.org/obo/HP_": "HP",
    "http://purl.obolibrary.org/obo/GO_": "GO",
    # General OBO prefix - should be last or handled carefully to avoid overly broad matches
    # if specific OBO sub-ontologies are listed above.
    "http://purl.obolibrary.org/obo/": "obo",
}

# LLM API Key (placeholders)
# GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Logging configuration
LOG_LEVEL = "INFO"
LOG_FILE = os.path.join(PROJECT_ROOT, "app.log") # Example log file in project root
--- END File: src/config.py ---

--- File: src/__init__.py ---

--- END File: src/__init__.py ---

--- File: src/main.py ---

--- END File: src/main.py ---

--- File: src/reranker/__init__.py ---
# src/reranker/__init__.py
# This file makes Python treat the directory 'reranker' as a package.

from .llm_reranker import LLMReranker

__all__ = ["LLMReranker"]
--- END File: src/reranker/__init__.py ---

--- File: src/reranker/llm_reranker.py ---
# src/reranker/llm_reranker.py

import json
import logging
from typing import List, Dict, Optional, Tuple, Any

from sentence_transformers import CrossEncoder
import torch

# Configure logging for the module
logger = logging.getLogger(__name__)
# Basic configuration for the logger if no handlers are configured by the calling application
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')


class LLMReranker:
    """
    Reranks documents using a CrossEncoder model.
    It takes a query and a list of candidate documents, computes a relevance score
    for each query-document pair, and returns the documents sorted by these scores.
    """

    def __init__(self,
                 model_name: str,
                 enriched_docs_path: str,
                 device: Optional[str] = None):
        """
        Initializes the LLMReranker.

        Args:
            model_name (str): The name of the CrossEncoder model to use
                              (e.g., "cross-encoder/ms-marco-MiniLM-L-6-v2").
            enriched_docs_path (str): Path to the JSON file containing enriched documents,
                                      expected to be a list of dicts with "id" and "text" keys.
            device (Optional[str]): The device to run the model on ("cuda", "cpu").
                                    If None, tries to use CUDA, otherwise CPU.
        """
        self.model_name = model_name
        self.enriched_docs_path = enriched_docs_path

        if device:
            self.device = device
        elif torch.cuda.is_available():
            self.device = "cuda"
            logger.info("CUDA available. Using CUDA for reranker.")
        else:
            self.device = "cpu"
            logger.info("CUDA not available. Using CPU for reranker.")
        
        try:
            self.model = CrossEncoder(self.model_name, device=self.device, trust_remote_code=True) # Added trust_remote_code for models like e5
            logger.info(f"Successfully loaded CrossEncoder model: {self.model_name} on {self.device}")
            model_max_length = self.model.tokenizer.model_max_length
            if model_max_length > 10000: # often indicates not properly set, e.g. 1e30
                model_max_length = 512 # Fallback to a common default
                logger.warning(f"Model tokenizer returned a very large max_length. Using fallback: {model_max_length}")
            self.model.max_length = model_max_length
            logger.info(f"Set CrossEncoder max_length to: {self.model.max_length}")

        except Exception as e:
            logger.error(f"Failed to load CrossEncoder model {self.model_name}: {e}")
            if "intfloat/e5-mistral-7b-instruct" in self.model_name:
                 logger.warning(
                    f"Note: '{self.model_name}' is primarily an encoder model. "
                    "Using it directly with `CrossEncoder` might not yield optimal "
                    "results unless it's a variant fine-tuned for sequence-pair classification "
                    "or requires `trust_remote_code=True`. "
                    "Consider a bi-encoder approach (separate embeddings + cosine similarity) "
                    "or a prompt-based LLM reranking for this model type if standard CrossEncoder fails."
                )
            raise

        self.doc_texts = self._load_enriched_documents()
        if not self.doc_texts:
            logger.error(f"Enriched documents could not be loaded from {self.enriched_docs_path}. Reranker might not function correctly.")


    def _load_enriched_documents(self) -> Dict[str, str]:
        """
        Loads enriched documents and creates a mapping from document ID to text.

        Returns:
            Dict[str, str]: A dictionary mapping document CURIEs to their enriched text.
        """
        try:
            with open(self.enriched_docs_path, 'r', encoding='utf-8') as f:
                enriched_data = json.load(f)
            
            doc_map = {}
            for item in enriched_data:
                if "id" in item and "text" in item:
                    doc_map[item["id"]] = item["text"]
                else:
                    logger.warning(f"Skipping item due to missing 'id' or 'text' in {self.enriched_docs_path}: {item}")
            logger.info(f"Loaded {len(doc_map)} enriched documents for reranking from {self.enriched_docs_path}.")
            return doc_map
        except FileNotFoundError:
            logger.error(f"Enriched documents file not found: {self.enriched_docs_path}")
            return {}
        except json.JSONDecodeError:
            logger.error(f"Error decoding JSON from {self.enriched_docs_path}")
            return {}
        except Exception as e:
            logger.error(f"An unexpected error occurred while loading enriched documents: {e}")
            return {}

    def _get_document_text_for_reranking(self, doc_id: str) -> Optional[str]:
        """
        Retrieves the pre-loaded enriched text for a given document ID.

        Args:
            doc_id (str): The CURIE of the document.

        Returns:
            Optional[str]: The enriched text of the document, or None if not found.
        """
        return self.doc_texts.get(doc_id)

    def rerank(self, query: str, documents: List[Dict[str, Any]], top_n: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        Reranks a list of documents based on their relevance to a query.

        Args:
            query (str): The search query.
            documents (List[Dict[str, Any]]): A list of document dictionaries retrieved
                                              from a previous stage (e.g., HybridRetriever).
                                              Each dict must contain an 'id' key.
            top_n (Optional[int]): The maximum number of documents to return after reranking.
                                   If None, all reranked documents are returned.

        Returns:
            List[Dict[str, Any]]: The list of documents, sorted by reranked scores,
                                  with a new 'rerank_score' key added to each document.
                                  Documents for which text could not be found are excluded.
        """
        if not query:
            logger.warning("Rerank called with an empty query. Returning original documents.")
            return documents[:top_n] if top_n is not None else documents
        
        if not documents:
            logger.info("Rerank called with no documents. Returning empty list.")
            return []

        if not self.doc_texts:
            logger.error("No enriched document texts loaded. Cannot perform reranking. Returning original documents.")
            return documents[:top_n] if top_n is not None else documents

        sentence_pairs: List[Tuple[str, str]] = []
        valid_documents_for_reranking: List[Dict[str, Any]] = []

        for doc in documents:
            doc_id = doc.get("id")
            if not doc_id:
                logger.warning(f"Document missing 'id' field, cannot rerank: {doc}")
                continue

            doc_text = self._get_document_text_for_reranking(doc_id)
            if doc_text:
                sentence_pairs.append((query, doc_text))
                valid_documents_for_reranking.append(doc)
            else:
                logger.warning(f"Could not find enriched text for document ID '{doc_id}'. Skipping for reranking.")
        
        if not sentence_pairs:
            logger.warning("No valid documents found to create sentence pairs for reranking. Returning original documents that were passed in.")
            return documents[:top_n] if top_n is not None else documents

        logger.info(f"Reranking {len(sentence_pairs)} document(s) for query: '{query}'")
        
        try:
            scores = self.model.predict(sentence_pairs, 
                                        show_progress_bar=False, # Set to True for verbose progress
                                        batch_size=32) # Adjust batch_size based on VRAM
        except Exception as e:
            logger.error(f"Error during CrossEncoder prediction: {e}")
            # Fallback: return original documents without reranking scores
            return documents[:top_n] if top_n is not None else documents

        # Add scores to documents and sort
        for i, doc in enumerate(valid_documents_for_reranking):
            doc["rerank_score"] = float(scores[i]) # Ensure score is float

        # Sort documents by rerank_score in descending order
        reranked_documents = sorted(valid_documents_for_reranking, key=lambda x: x.get("rerank_score", -float('inf')), reverse=True)

        if top_n is not None:
            reranked_documents = reranked_documents[:top_n]
            logger.info(f"Returning top {len(reranked_documents)} reranked documents.")
        else:
            logger.info(f"Returning all {len(reranked_documents)} reranked documents.")
            
        return reranked_documents


if __name__ == '__main__':
    import sys
    from pathlib import Path
    import random # For dummy retriever scores

    # --- Path Setup & Config Import ---
    # Add project root to sys.path to allow imports like src.config
    # Assumes this script is in onto_rag/src/reranker/
    PROJECT_ROOT_FOR_MAIN = Path(__file__).resolve().parent.parent.parent
    if str(PROJECT_ROOT_FOR_MAIN) not in sys.path:
        sys.path.append(str(PROJECT_ROOT_FOR_MAIN))

    try:
        from src.config import (
            ENRICHED_DOCUMENTS_FILE,
            RERANKER_MODEL_NAME,
            EMBEDDING_DEVICE,
            DEFAULT_RERANK_K,
            LOG_LEVEL, # Optional: if you want to use config's log level
            LOG_FILE   # Optional: if you want to use config's log file
        )
    except ImportError as e:
        print(f"Error importing from src.config: {e}")
        print("Please ensure that src/config.py exists and the script is run from the project root,")
        print("or that the PROJECT_ROOT_FOR_MAIN path is correctly set for your structure.")
        sys.exit(1)

    # --- Basic Logging Setup for the Example ---
    # Uses the main logger configured at the top of the file.
    # You can customize this further if needed, e.g., by setting level from config.LOG_LEVEL
    logger.setLevel(LOG_LEVEL if 'LOG_LEVEL' in locals() else "INFO")
    # Example of adding a file handler if LOG_FILE is configured:
    # if 'LOG_FILE' in locals() and LOG_FILE:
    #     fh = logging.FileHandler(LOG_FILE)
    #     fh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    #     logger.addHandler(fh)
    #     logging.getLogger().addHandler(fh) # also add to root logger if basicConfig was called

    logger.info("--- Running LLMReranker Example with Real Data ---")
    logger.info(f"Using ENRICHED_DOCUMENTS_FILE: {ENRICHED_DOCUMENTS_FILE}")
    logger.info(f"Using RERANKER_MODEL_NAME: {RERANKER_MODEL_NAME}")
    logger.info(f"Using EMBEDDING_DEVICE: {EMBEDDING_DEVICE}")
    logger.info(f"Using DEFAULT_RERANK_K: {DEFAULT_RERANK_K}")


    # --- Load Sample Documents from Real Enriched Data ---
    sample_documents_from_retriever: List[Dict[str, Any]] = []
    try:
        with open(ENRICHED_DOCUMENTS_FILE, 'r', encoding='utf-8') as f:
            all_enriched_docs = json.load(f)
        
        if not all_enriched_docs:
            logger.error(f"No documents found in {ENRICHED_DOCUMENTS_FILE}. Cannot proceed with the example.")
            sys.exit(1)

        # Select a few documents to simulate retriever output
        # Take up to 5, or fewer if the file has less
        num_docs_to_sample = min(len(all_enriched_docs), 5)
        if num_docs_to_sample == 0 :
             logger.error(f"The file {ENRICHED_DOCUMENTS_FILE} is empty. Cannot create sample documents.")
             sys.exit(1)
             
        for i in range(num_docs_to_sample):
            doc = all_enriched_docs[i]
            sample_documents_from_retriever.append({
                "id": doc.get("id"),
                "label": doc.get("label", "N/A"), # Get label if available
                "retriever_score": random.uniform(0.5, 1.0) # Dummy score
            })
        logger.info(f"Loaded {len(sample_documents_from_retriever)} sample documents for reranking.")

    except FileNotFoundError:
        logger.error(f"ERROR: The enriched documents file was not found: {ENRICHED_DOCUMENTS_FILE}")
        logger.error("Please ensure you have run the data ingestion and enrichment scripts first (e.g., `src.ingestion.enrich_documents.py`).")
        sys.exit(1)
    except json.JSONDecodeError:
        logger.error(f"ERROR: Could not decode JSON from {ENRICHED_DOCUMENTS_FILE}. The file might be corrupted.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"An unexpected error occurred while loading sample documents: {e}", exc_info=True)
        sys.exit(1)

    if not sample_documents_from_retriever:
        logger.error("No sample documents could be prepared. Exiting example.")
        sys.exit(1)
        
    # --- Reranker Initialization and Usage ---
    try:
        reranker = LLMReranker(
            model_name=RERANKER_MODEL_NAME,
            enriched_docs_path=str(ENRICHED_DOCUMENTS_FILE), # Ensure path is a string
            device=EMBEDDING_DEVICE
        )

        query = "Garlic" # Generic example, adjust to your data

        logger.info(f"\nOriginal sample documents (simulated retriever output) for query '{query}':")
        for doc in sample_documents_from_retriever:
            logger.info(f"  ID: {doc.get('id')}, Label: {doc.get('label')}, Retriever Score: {doc.get('retriever_score', 0.0):.4f}")

        reranked_results = reranker.rerank(query, sample_documents_from_retriever, top_n=DEFAULT_RERANK_K)

        logger.info(f"\nReranked documents (top {DEFAULT_RERANK_K} for query '{query}'):")
        if reranked_results:
            for doc in reranked_results:
                logger.info(f"  ID: {doc.get('id')}, Label: {doc.get('label')}, Rerank Score: {doc.get('rerank_score', 'N/A'):.4f}, Original Retriever Score: {doc.get('retriever_score', 'N/A')}")
        else:
            logger.info("  No results after reranking.")
            ``
        # Example of how you might switch to "intfloat/e5-mistral-7b-instruct"
        # Ensure it's set in your .env or config.py as RERANKER_MODEL_NAME
        # and that you have the resources for it.
        if RERANKER_MODEL_NAME != "intfloat/e5-mistral-7b-instruct" and False: # Set to True to test e5
            logger.info("\n--- Conceptual Test: Switching to intfloat/e5-mistral-7b-instruct ---")
            logger.warning("This is a large model and may require significant resources and download time.")
            logger.warning("Ensure 'intfloat/e5-mistral-7b-instruct' is configured as RERANKER_MODEL_NAME and 'trust_remote_code=True' might be needed.")
            
            try:
                e5_model_name = "intfloat/e5-mistral-7b-instruct"
                e5_reranker = LLMReranker(
                    model_name=e5_model_name,
                    enriched_docs_path=str(ENRICHED_DOCUMENTS_FILE),
                    device=EMBEDDING_DEVICE # "cuda" recommended for this model
                )
                e5_reranked_results = e5_reranker.rerank(query, sample_documents_from_retriever, top_n=DEFAULT_RERANK_K)
                logger.info(f"\nReranked documents with {e5_model_name} (top {DEFAULT_RERANK_K} for query '{query}'):")
                if e5_reranked_results:
                    for doc in e5_reranked_results:
                        logger.info(f"  ID: {doc.get('id')}, Label: {doc.get('label')}, Rerank Score: {doc.get('rerank_score', 'N/A'):.4f}")
                else:
                    logger.info("  No results after reranking with E5.")
            except Exception as e_e5:
                logger.error(f"Could not initialize or use {e5_model_name} reranker: {e_e5}", exc_info=True)
                logger.warning(f"Skipping {e5_model_name} reranking part of the example.")


    except Exception as e:
        logger.error(f"An error occurred in the main example: {e}", exc_info=True)

    logger.info("--- LLMReranker Example Finished ---")
--- END File: src/reranker/llm_reranker.py ---

--- File: src/ingestion/build_lexical_index.py ---
# src/ingestion/build_lexical_index.py
import sys
import os

# --- Add project root to sys.path ---
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
# --- End sys.path modification ---

import json
from whoosh.index import create_in, open_dir
from whoosh.fields import Schema, ID, TEXT
from whoosh.qparser import QueryParser
import traceback

# Now import using the 'src' package prefix
from src.config import ONTOLOGY_DUMP_JSON, WHOOSH_INDEX_DIR

# ... (rest of the build_lexical_index.py code, ensuring imports like `from src.config` are used)
def build_index(json_path: str, index_dir: str):
    print(f"Building lexical index from {json_path} into {index_dir}")

    schema = Schema(
        curie=ID(stored=True, unique=True),
        label=TEXT(stored=True, analyzer=None), # Keep None analyzer for exact matches if needed, or use default
        synonyms=TEXT(stored=True),
        definition=TEXT(stored=True),
        relations_text=TEXT(stored=False)
    )

    os.makedirs(index_dir, exist_ok=True)
    
    try:
        # create_in will overwrite if index exists. If you want to open, use open_dir.
        ix = create_in(index_dir, schema)
        print(f"Whoosh index schema created in {index_dir}")
    except Exception as e:
         print(f"Error creating Whoosh index directory or schema: {e}")
         traceback.print_exc()
         return

    writer = ix.writer()
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            ontology_data = json.load(f)

        print(f"Indexing {len(ontology_data)} entities...")
        indexed_count = 0
        for curie, data in ontology_data.items():
            label = data.get('label', '') or '' # Ensure string
            synonyms_list = data.get('synonyms', [])
            synonyms = " ".join(s for s in synonyms_list if s) # Join non-empty synonyms

            definition = data.get('definition', '') or ''

            relations_text = ""
            relations_dict = data.get('relations', {})
            if relations_dict:
                 relations_text = " ".join([
                     f"{prop.replace('_', ' ')} {' '.join(targets)}" for prop, targets in relations_dict.items()
                 ])
            
            # Skip adding document if all text fields are empty (optional)
            # if not label and not synonyms and not definition and not relations_text:
            #     print(f"Skipping empty document for CURIE: {curie}")
            #     continue

            writer.add_document(
                curie=curie,
                label=label,
                synonyms=synonyms,
                definition=definition,
                relations_text=relations_text
            )
            indexed_count += 1
            if indexed_count % 1000 == 0:
                 print(f"Indexed {indexed_count} entities...")
        
        print(f"Committing index with {indexed_count} documents.")
        writer.commit()
        print("Lexical index built successfully.")

    except FileNotFoundError:
        print(f"Error: Ontology dump file not found at {json_path}")
        traceback.print_exc()
        writer.cancel() 
    except Exception as e:
        print(f"An error occurred during index building: {e}")
        traceback.print_exc()
        writer.cancel()

if __name__ == "__main__":
    if not os.path.exists(ONTOLOGY_DUMP_JSON):
        print(f"Error: Ontology dump file {ONTOLOGY_DUMP_JSON} not found.")
        print("Please run src/ingestion/parse_ontology.py first.")
    else:
        build_index(ONTOLOGY_DUMP_JSON, WHOOSH_INDEX_DIR)
--- END File: src/ingestion/build_lexical_index.py ---

--- File: src/ingestion/__init__.py ---

--- END File: src/ingestion/__init__.py ---

--- File: src/ingestion/parse_ontology.py ---
# src/ingestion/parse_ontology.py
import sys
import os
import logging # Import logging

# --- Add project root to sys.path ---
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
# --- End sys.path modification ---

import rdflib
from rdflib import Graph, Namespace, URIRef, RDFS, OWL, RDF
from typing import Dict, List, Any
import json
import traceback

# Now import using the 'src' package prefix
from src.config import (
    FOODON_PATH,                # Adjusted: Was ONTOLOGY_FILE
    ONTOLOGY_DUMP_JSON,         # Adjusted: Was ONTOLOGY_DUMP_PATH
    CURIE_PREFIX_MAP,           # Adjusted: Was NAMESPACE_MAP
    RELATION_CONFIG,            # New: For relation names
    TARGET_RELATIONS_CURIES,    # New: For relation URIs
    IAO_NS_STR,                 # Using string constants for Namespace definitions
    OBOINOWL_NS_STR,
    # Add other NS_STR if needed for local Namespace objects
)
# Assuming src.utils.ontology_utils is already correct and uses CURIE_PREFIX_MAP
from src.utils.ontology_utils import uri_to_curie, curie_to_uri

# --- Logging Setup ---
# Using basicConfig as src.utils.logging.get_logger is "to be developed"
# You can customize this further if needed.
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s')
logger = logging.getLogger(__name__)


# Define commonly used namespaces (can still use these locally for convenience)
# It's safer to use the full URI string from config if available, or construct it.
IAO = Namespace(IAO_NS_STR)
OBOINOWL = Namespace(OBOINOWL_NS_STR)
# FOODON_BASE_URI = None # Find FOODON base URI from CURIE_PREFIX_MAP
# for base, prefix in CURIE_PREFIX_MAP.items():
#     if prefix == "FOODON":
#         FOODON_BASE_URI = base
#         break
# FOODON = Namespace(FOODON_BASE_URI) if FOODON_BASE_URI else None
# If FOODON Namespace object is not strictly needed for queries, direct URI construction is fine.

def load_ontology(path: str) -> rdflib.Graph:
    g = Graph()
    try:
        logger.info(f"Loading ontology from: {path}")
        try:
            g.parse(path, format="application/rdf+xml")
        except Exception as e_xml:
            logger.warning(f"Failed to parse as RDF/XML: {e_xml}. Trying Turtle...")
            try:
                g.parse(path, format="turtle")
            except Exception as e_ttl:
                logger.warning(f"Failed to parse as Turtle: {e_ttl}. Trying auto-detection...")
                g.parse(path)

        logger.info(f"Ontology loaded successfully. Contains {len(g)} triples.")
        return g
    except FileNotFoundError:
        logger.error(f"Error: Ontology file not found at {path}")
        raise
    except Exception as e:
        logger.error(f"Error parsing ontology file {path}: {e}")
        traceback.print_exc()
        raise

def get_ancestors(g: Graph, term_uri: URIRef, prefix_map: Dict[str, str], visited_uris: set = None) -> List[str]:
    if visited_uris is None:
        visited_uris = set()

    ancestor_curies = set()
    for parent_uri in g.objects(subject=term_uri, predicate=RDFS.subClassOf):
        if isinstance(parent_uri, URIRef) and parent_uri != OWL.Thing:
            if parent_uri not in visited_uris:
                visited_uris.add(parent_uri)
                # Pass the prefix_map explicitly
                parent_curie = uri_to_curie(parent_uri, prefix_map)
                if parent_curie and parent_curie != str(parent_uri):
                    ancestor_curies.add(parent_curie)
                    ancestor_curies.update(get_ancestors(g, parent_uri, prefix_map, visited_uris))
    return list(ancestor_curies)


def extract_labels_and_synonyms(g: Graph, prefix_map: Dict[str, str]) -> Dict[str, Dict[str, Any]]:
    data = {}
    relevant_predicates = [
        RDFS.label,
        OBOINOWL.hasExactSynonym, OBOINOWL.hasRelatedSynonym,
        OBOINOWL.hasNarrowSynonym, OBOINOWL.hasBroadSynonym
    ]
    
    processed_subjects = set()

    for pred in relevant_predicates:
        for s_uri in g.subjects(predicate=pred):
            if not isinstance(s_uri, URIRef) or s_uri in processed_subjects:
                continue
            
            # Pass the prefix_map explicitly
            curie = uri_to_curie(s_uri, prefix_map)
            # Optional: Filter for specific prefixes if desired, e.g., only FOODON, IAO, OBO terms.
            # This was present in your original script; keeping it commented for now for broader extraction.
            # if not any(curie.startswith(p) for p in ["FOODON:", "IAO:", "RO:", "BFO:", "obo:", "CHEBI:"]): # Example
            #     # logger.debug(f"Skipping non-ontology CURIE for labels/syns: {curie} from {s_uri}")
            #     continue
            if not curie or curie == str(s_uri): # Skip if not converted to a CURIE effectively
                continue

            if curie not in data:
                data[curie] = {"label": None, "synonyms": []}

            # Label
            label_val = g.value(subject=s_uri, predicate=RDFS.label)
            if label_val and isinstance(label_val, rdflib.Literal):
                data[curie]["label"] = str(label_val)

            # Synonyms
            current_synonyms = []
            for syn_prop in [OBOINOWL.hasExactSynonym, OBOINOWL.hasRelatedSynonym,
                             OBOINOWL.hasNarrowSynonym, OBOINOWL.hasBroadSynonym]:
                for syn_obj in g.objects(subject=s_uri, predicate=syn_prop):
                    if isinstance(syn_obj, rdflib.Literal):
                        current_synonyms.append(str(syn_obj))
            
            # Ensure synonyms list exists and extend uniquely
            if "synonyms" not in data[curie] or data[curie]["synonyms"] is None: # Handle if somehow becomes None
                data[curie]["synonyms"] = []
            for s in current_synonyms:
                if s not in data[curie]["synonyms"]:
                    data[curie]["synonyms"].append(s)

            processed_subjects.add(s_uri)
            
    final_data = {k: v for k, v in data.items() if v.get("label") or v.get("synonyms")}
    logger.info(f"Extracted labels and synonyms for {len(final_data)} terms.")
    return final_data


def extract_definitions(g: Graph, prefix_map: Dict[str, str]) -> Dict[str, str]:
    definitions = {}
    definition_prop_uri = IAO['0000115'] # IAO:0000115 is 'definition'
    for s_uri in g.subjects(predicate=definition_prop_uri):
        if not isinstance(s_uri, URIRef):
            continue
        
        # Pass the prefix_map explicitly
        curie = uri_to_curie(s_uri, prefix_map)
        # Optional: Filter for specific prefixes
        # if not any(curie.startswith(p) for p in ["FOODON:", "IAO:", "RO:", "BFO:", "obo:", "CHEBI:"]):
        #     # logger.debug(f"Skipping non-ontology CURIE for definitions: {curie} from {s_uri}")
        #     continue
        if not curie or curie == str(s_uri):
            continue
        
        def_obj = g.value(subject=s_uri, predicate=definition_prop_uri)
        if def_obj and isinstance(def_obj, rdflib.Literal):
            definitions[curie] = str(def_obj)
            
    logger.info(f"Extracted definitions for {len(definitions)} terms.")
    return definitions

def extract_hierarchy(g: Graph, prefix_map: Dict[str, str]) -> Dict[str, Dict[str, List[str]]]:
    hierarchy_data = {}
    all_terms_in_hierarchy = set()
    for s, p, o in g.triples((None, RDFS.subClassOf, None)):
        if isinstance(s, URIRef): all_terms_in_hierarchy.add(s)
        if isinstance(o, URIRef): all_terms_in_hierarchy.add(o)
    
    for term_uri in all_terms_in_hierarchy:
        if term_uri == OWL.Thing:
            continue

        # Pass the prefix_map explicitly
        curie = uri_to_curie(term_uri, prefix_map)
        # Optional: Filter
        # if not any(curie.startswith(p) for p in ["FOODON:", "IAO:", "RO:", "BFO:", "obo:", "CHEBI:"]):
        #     # logger.debug(f"Skipping non-ontology CURIE for hierarchy: {curie} from {term_uri}")
        #     continue
        if not curie or curie == str(term_uri):
            continue

        direct_parent_curies = []
        for parent_uri in g.objects(subject=term_uri, predicate=RDFS.subClassOf):
            if isinstance(parent_uri, URIRef) and parent_uri != OWL.Thing:
                # Pass the prefix_map explicitly
                parent_curie = uri_to_curie(parent_uri, prefix_map)
                if parent_curie and parent_curie != str(parent_uri):
                    direct_parent_curies.append(parent_curie)
        
        ancestor_curies = get_ancestors(g, term_uri, prefix_map, visited_uris=set())
        
        if direct_parent_curies or ancestor_curies:
            hierarchy_data[curie] = {
                "parents": list(set(direct_parent_curies)),
                "ancestors": list(set(ancestor_curies))
            }
            
    logger.info(f"Extracted hierarchy data for {len(hierarchy_data)} terms.")
    return hierarchy_data


# Adjusted to match outline: props_to_extract: Dict[str, str] (readable_name -> URI_string)
def extract_relations(g: Graph, props_to_extract: Dict[str, str], prefix_map: Dict[str, str]) -> Dict[str, Dict[str, List[str]]]:
    relations_data = {}
    
    for term_uri in g.subjects(unique=True):
        if not isinstance(term_uri, URIRef):
            continue

        # Pass the prefix_map explicitly
        curie = uri_to_curie(term_uri, prefix_map)
        # Optional: Filter
        # if not any(curie.startswith(p) for p in ["FOODON:", "IAO:", "RO:", "BFO:", "obo:", "CHEBI:"]):
        #     # logger.debug(f"Skipping non-ontology CURIE for relations: {curie} from {term_uri}")
        #     continue
        if not curie or curie == str(term_uri):
            continue

        term_specific_relations = {}
        for rel_readable_name, rel_uri_str in props_to_extract.items(): # rel_name is readable_name
            rel_uri = URIRef(rel_uri_str)
            target_curies = []
            for target_obj in g.objects(subject=term_uri, predicate=rel_uri):
                if isinstance(target_obj, URIRef):
                    # Pass the prefix_map explicitly
                    target_curie = uri_to_curie(target_obj, prefix_map)
                    if target_curie and target_curie != str(target_obj):
                        target_curies.append(target_curie)
            
            if target_curies:
                term_specific_relations[rel_readable_name] = list(set(target_curies))

        if term_specific_relations:
            relations_data[curie] = term_specific_relations
            
    logger.info(f"Extracted relations for {len(relations_data)} terms based on {len(props_to_extract)} specified properties.")
    return relations_data


def main():
    logger.info("Starting ontology parsing...")
    # Adjusted: Use ONTOLOGY_DUMP_JSON for output path
    data_dir = os.path.dirname(ONTOLOGY_DUMP_JSON)
    os.makedirs(data_dir, exist_ok=True)

    try:
        # Adjusted: Use FOODON_PATH as input ontology file
        g = load_ontology(FOODON_PATH)

        # Prepare relation properties for extract_relations
        # extract_relations expects: Dict[readable_name, full_uri_string]
        # RELATION_CONFIG has: {curie_str: {"label": readable_name, "prefix": prefix_str}}
        # TARGET_RELATIONS_CURIES is List[curie_str]
        
        relation_properties_for_extraction = {}
        for rel_curie_str in TARGET_RELATIONS_CURIES:
            if rel_curie_str in RELATION_CONFIG:
                config_entry = RELATION_CONFIG[rel_curie_str]
                readable_name = config_entry.get("label", rel_curie_str) # Fallback to CURIE if no label
                
                # Convert relation CURIE to full URI using curie_to_uri and CURIE_PREFIX_MAP
                full_rel_uri = curie_to_uri(rel_curie_str, CURIE_PREFIX_MAP)
                if full_rel_uri:
                    relation_properties_for_extraction[readable_name] = str(full_rel_uri)
                else:
                    logger.warning(f"Could not convert relation CURIE {rel_curie_str} to URI. Skipping this relation.")
            else:
                logger.warning(f"Relation CURIE {rel_curie_str} from TARGET_RELATIONS_CURIES not found in RELATION_CONFIG.")
        
        logger.info(f"Prepared {len(relation_properties_for_extraction)} relation properties for extraction.")


        logger.info("\nExtracting data...")
        # Pass CURIE_PREFIX_MAP to all extraction functions
        labels_synonyms = extract_labels_and_synonyms(g, CURIE_PREFIX_MAP)
        definitions = extract_definitions(g, CURIE_PREFIX_MAP)
        hierarchy = extract_hierarchy(g, CURIE_PREFIX_MAP)
        relations = extract_relations(g, relation_properties_for_extraction, CURIE_PREFIX_MAP)

        logger.info("\nMerging extracted data...")
        merged_data = {}
        all_curies = set(labels_synonyms.keys()) | \
                     set(definitions.keys()) | \
                     set(hierarchy.keys()) | \
                     set(relations.keys())

        for curie_key in all_curies:
            # No need to filter again here if extraction functions already did or if we want all extracted CURIEs
            merged_data[curie_key] = {
                "label": labels_synonyms.get(curie_key, {}).get("label"),
                "synonyms": labels_synonyms.get(curie_key, {}).get("synonyms", []),
                "definition": definitions.get(curie_key),
                "parents": hierarchy.get(curie_key, {}).get("parents", []),
                "ancestors": hierarchy.get(curie_key, {}).get("ancestors", []),
                "relations": relations.get(curie_key, {})
            }
        
        final_merged_data = {}
        for curie_key, data_dict in merged_data.items():
            if any(data_dict.values()): # Check if any value in the dict is non-empty/non-None
                final_merged_data[curie_key] = data_dict

        logger.info(f"\nTotal merged entities with some data: {len(final_merged_data)}")

        # Adjusted: Use ONTOLOGY_DUMP_JSON for output
        logger.info(f"Writing merged data to {ONTOLOGY_DUMP_JSON}")
        with open(ONTOLOGY_DUMP_JSON, 'w', encoding='utf-8') as f:
            json.dump(final_merged_data, f, indent=4, ensure_ascii=False)

        logger.info("Ontology parsing and data dump complete.")

    except FileNotFoundError:
        # Adjusted: Use FOODON_PATH in error message
        logger.error(f"Parsing aborted: Ontology file not found at {FOODON_PATH}")
        # traceback.print_exc() # Already handled by load_ontology
    except Exception as e:
        logger.error(f"An error occurred during parsing: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()
--- END File: src/ingestion/parse_ontology.py ---

--- File: src/ingestion/enrich_documents.py ---
import json
import os
import logging
from typing import Dict, Any, List, Optional

# --- Start of corrected import block ---
import sys
# Calculate the project root directory based on the script's location
# For .../onto_rag/src/ingestion/enrich_documents.py, _PROJECT_ROOT becomes .../onto_rag
_PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))

# Add the project root to sys.path if it's not already there
if _PROJECT_ROOT not in sys.path:
    sys.path.insert(0, _PROJECT_ROOT) # Insert at the beginning

try:
    from src import config
    # from src.utils.ontology_utils import curie_to_uri # This specific utility is not used in this script,
                                                      # but this is how you'd import it if needed.
except ModuleNotFoundError as e:
    print(f"CRITICAL ERROR: Could not import project modules. Exception: {e}")
    print(f"This script expects to be run in a way that the 'src' package is discoverable.")
    print(f"Attempted to add project root '{_PROJECT_ROOT}' to sys.path.")
    print(f"Current sys.path: {sys.path}")
    print("Please ensure you are running this script from the project's root directory ('onto_rag/'), for example:")
    print("  python src/ingestion/enrich_documents.py")
    print("Also ensure that 'src/__init__.py' and 'src/utils/__init__.py' (if using utils) exist.")
    sys.exit(1)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def get_label_for_curie(curie: str, ontology_data: Dict[str, Dict[str, Any]], default_value: Optional[str] = None) -> Optional[str]:
    """
    Retrieves the label for a given CURIE from the ontology data.
    Args:
        curie: The CURIE string (e.g., "FOODON:00001234").
        ontology_data: The loaded ontology data dictionary.
        default_value: Value to return if CURIE not found or has no label. Defaults to the CURIE itself.
    Returns:
        The label string, or the default_value.
    """
    if default_value is None:
        default_value = curie # Fallback to CURIE if no specific default

    term_info = ontology_data.get(curie)
    if term_info and term_info.get("label"):
        return term_info["label"]
    return default_value

def get_relation_name(relation_curie: str) -> str:
    """
    Gets a human-readable name for a relation CURIE using RELATION_CONFIG.
    """
    # Exact match
    if relation_curie in config.RELATION_CONFIG:
        return config.RELATION_CONFIG[relation_curie]["label"]
    
    # Check for generic FoodON prefix if specific one not found
    generic_foodon_prefix = "obo:FOODON_"
    if relation_curie.startswith(generic_foodon_prefix) and generic_foodon_prefix in config.RELATION_CONFIG:
         # Attempt to make it slightly more readable if it's like "obo:FOODON_0000XXXX"
        relation_suffix = relation_curie.split('_')[-1]
        return f"FoodON relation {relation_suffix}" # or config.RELATION_CONFIG[generic_foodon_prefix]["label"]

    # Fallback for other OBO relations
    if relation_curie.startswith("obo:"):
        name_part = relation_curie.split(':')[-1].replace("_", " ")
        return name_part

    return relation_curie # Fallback to the CURIE itself

def create_enriched_documents(ontology_data_path: str, output_path: str) -> List[Dict[str, Any]]:
    """
    Creates enriched text documents for each ontology entry.
    Args:
        ontology_data_path: Path to the ontology_dump.json file.
        output_path: Path to save the enriched_documents.json file.
    Returns:
        A list of enriched document dictionaries.
    """
    logging.info(f"Loading ontology data from {ontology_data_path}...")
    try:
        with open(ontology_data_path, 'r', encoding='utf-8') as f:
            ontology_data = json.load(f)
    except FileNotFoundError:
        logging.error(f"Error: Ontology data file not found at {ontology_data_path}")
        return []
    except json.JSONDecodeError:
        logging.error(f"Error: Could not decode JSON from {ontology_data_path}")
        return []

    logging.info(f"Successfully loaded {len(ontology_data)} terms.")

    enriched_docs = []

    for term_curie, term_data in ontology_data.items():
        doc_parts = []

        # 1. Label
        label = term_data.get("label")
        if not label:
            logging.warning(f"Term {term_curie} has no label. Skipping for enriched document (or using CURIE as label).")
            # Consider if we want to enrich docs for terms without labels. For now, let's use CURIE if no label.
            label = term_curie 
        
        doc_parts.append(f"{label}.")

        # 2. Definition
        definition = term_data.get("definition")
        if definition:
            doc_parts.append(f"{definition}.")

        # 3. Synonyms
        synonyms = term_data.get("synonyms")
        if synonyms:
            synonyms_text = "; ".join(synonyms)
            doc_parts.append(f"Also known as: {synonyms_text}.")

        # 4. Parents (direct subclasses)
        parent_curies = term_data.get("parents", [])
        if parent_curies:
            parent_labels = [get_label_for_curie(p_curie, ontology_data) for p_curie in parent_curies]
            parent_labels_filtered = [l for l in parent_labels if l] # Filter out None if get_label_for_curie returns None
            if parent_labels_filtered:
                if len(parent_labels_filtered) == 1:
                    doc_parts.append(f"Is a type of: {parent_labels_filtered[0]}.")
                else:
                    doc_parts.append(f"Is a type of: {'; '.join(parent_labels_filtered)}.")
        
        # 5. Relations (including facets expressed as object properties)
        relations = term_data.get("relations", {})
        relation_texts = []
        for rel_curie, target_curies_list in relations.items():
            rel_name = get_relation_name(rel_curie)
            target_labels = [get_label_for_curie(t_curie, ontology_data) for t_curie in target_curies_list]
            target_labels_filtered = [l for l in target_labels if l]
            if target_labels_filtered:
                relation_texts.append(f"{rel_name}: {', '.join(target_labels_filtered)}")
        
        if relation_texts:
            doc_parts.append("Key characteristics include: " + "; ".join(relation_texts) + ".")

        # Combine all parts into a single text
        enriched_text = " ".join(doc_parts).replace("..", ".").strip() # Clean up potential double periods

        enriched_docs.append({
            "id": term_curie,
            "label": label if label != term_curie else term_data.get("label", term_curie), # Store original label if available
            "text": enriched_text
        })

    logging.info(f"Created {len(enriched_docs)} enriched documents.")

    logging.info(f"Saving enriched documents to {output_path}...")
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(enriched_docs, f, indent=2)
        logging.info("Successfully saved enriched documents.")
    except IOError:
        logging.error(f"Error: Could not write enriched documents to {output_path}")

    return enriched_docs

def main():
    """Main function to create enriched documents."""
    # Ensure data directory exists (though config.py should handle it)
    os.makedirs(config.DATA_DIR, exist_ok=True)

    enriched_documents = create_enriched_documents(
        ontology_data_path=config.ONTOLOGY_DUMP_JSON,
        output_path=config.ENRICHED_DOCUMENTS_FILE
    )

    if enriched_documents:
        logging.info(f"Processed {len(enriched_documents)} documents.")
        # Optionally print a sample
        if len(enriched_documents) > 0:
            logging.info("Sample enriched document:")
            logging.info(json.dumps(enriched_documents[0], indent=2))
    else:
        logging.warning("No enriched documents were created.")

if __name__ == "__main__":
    main()
--- END File: src/ingestion/enrich_documents.py ---

--- File: src/embeddings/embed_docs.py ---
import json
import os
import logging
import time
from typing import List, Dict, Any

# Ensure src is in path for imports if run directly
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))) # To import from src

try:
    from sentence_transformers import SentenceTransformer
    import torch 
except ModuleNotFoundError:
    logging.error("Modules sentence_transformers or torch not found. Please install them: pip install sentence-transformers torch")
    sys.exit(1)

try:
    from src import config # Use 'from src import config'
except ModuleNotFoundError:
    # Fallback for running script directly from src/embeddings where src might not be seen as a package root
    # This is less ideal but can help during direct script execution
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    try:
        import config
    except ModuleNotFoundError:
        logging.error("Error: Could not import 'config'. "
                    "Ensure the script is run from the project root or 'src' is in PYTHONPATH.")
        sys.exit(1)


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def load_enriched_documents(file_path: str) -> List[Dict[str, Any]]:
    """Loads enriched documents from a JSON file."""
    logging.info(f"Loading enriched documents from {file_path}...")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            documents = json.load(f)
        logging.info(f"Successfully loaded {len(documents)} documents.")
        return documents
    except FileNotFoundError:
        logging.error(f"Error: Enriched documents file not found at {file_path}")
        return []
    except json.JSONDecodeError:
        logging.error(f"Error: Could not decode JSON from {file_path}")
        return []

def batch_embed_documents(
    documents: List[Dict[str, Any]], 
    model_name: str, 
    batch_size: int = 32,
    device: str = None
) -> List[Dict[str, Any]]:
    """
    Generates embeddings for a list of documents using a SentenceTransformer model.

    Args:
        documents: A list of dictionaries, each with at least "id", "label", and "text" keys.
        model_name: The name of the SentenceTransformer model to use.
        batch_size: The number of documents to process in each batch.
        device: The device to use for computation (e.g., "cuda", "cpu"). Auto-detects if None.

    Returns:
        A list of dictionaries, each containing "id", "label", and "embedding" (list of floats).
    """
    if not documents:
        logging.warning("No documents provided for embedding.")
        return []

    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    logging.info(f"Using device: {device}")

    logging.info(f"Loading SentenceTransformer model: {model_name}")
    try:
        model = SentenceTransformer(model_name, device=device, trust_remote_code=True)
    except Exception as e:
        logging.error(f"Failed to load model {model_name}: {e}")
        # Fallback to CPU if CUDA error during model loading (e.g. out of memory)
        if "cuda" in str(e).lower() and device == "cuda":
            logging.warning("CUDA error during model load. Attempting to load on CPU.")
            device = "cpu"
            model = SentenceTransformer(model_name, device=device, trust_remote_code=True)
        else:
            raise

    logging.info("Model loaded. Starting embedding process...")

    texts_to_embed = [doc.get("text", "") for doc in documents] # Get text, default to empty string if missing
    ids = [doc.get("id") for doc in documents]
    labels = [doc.get("label") for doc in documents]

    all_embeddings_data = []
    start_time = time.time()

    for i in range(0, len(texts_to_embed), batch_size):
        batch_texts = texts_to_embed[i:i+batch_size]
        batch_ids = ids[i:i+batch_size]
        batch_labels = labels[i:i+batch_size]

        logging.info(f"Processing batch {i//batch_size + 1}/{(len(texts_to_embed) -1)//batch_size + 1} (size: {len(batch_texts)})")
        
        try:
            embeddings = model.encode(batch_texts, convert_to_tensor=False, show_progress_bar=False) # Returns numpy array
            
            for idx, embedding in enumerate(embeddings):
                all_embeddings_data.append({
                    "id": batch_ids[idx],
                    "label": batch_labels[idx],
                    "embedding": embedding.tolist() # Convert numpy array to list for JSON serialization
                })
        except Exception as e:
            logging.error(f"Error embedding batch starting at index {i}: {e}")
            # Optionally, decide how to handle batch errors (e.g., skip batch, add placeholders)
            # For now, we'll just log and continue, resulting in missing embeddings for that batch.
            # To add placeholders:
            # for j in range(len(batch_texts)):
            #     all_embeddings_data.append({
            #         "id": batch_ids[j],
            #         "label": batch_labels[j],
            #         "embedding": None # Or a zero vector of correct dimensionality
            #     })


    end_time = time.time()
    logging.info(f"Embedding process completed for {len(all_embeddings_data)} documents in {end_time - start_time:.2f} seconds.")
    
    if hasattr(model, 'tokenizer') and hasattr(model.tokenizer, 'model_max_length'):
        logging.info(f"Model max sequence length: {model.tokenizer.model_max_length}")
    elif hasattr(model, 'max_seq_length'):
         logging.info(f"Model max sequence length: {model.max_seq_length}")


    return all_embeddings_data

def save_embeddings(embeddings_data: List[Dict[str, Any]], output_path: str):
    """Saves the embeddings data to a JSON file."""
    if not embeddings_data:
        logging.warning("No embeddings data to save.")
        return

    logging.info(f"Saving {len(embeddings_data)} embeddings to {output_path}...")
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(embeddings_data, f, indent=2) # Use indent for readability, can remove for smaller file size
        logging.info("Successfully saved embeddings.")
    except IOError:
        logging.error(f"Error: Could not write embeddings to {output_path}")
    except TypeError as e:
        logging.error(f"TypeError during JSON serialization: {e}. Check embedding data format.")


def main():
    """Main function to generate and save document embeddings."""
    # Ensure data directory exists
    os.makedirs(config.DATA_DIR, exist_ok=True)

    enriched_docs = load_enriched_documents(config.ENRICHED_DOCUMENTS_FILE)
    if not enriched_docs:
        logging.error("Failed to load enriched documents. Exiting.")
        return

    embeddings_data = batch_embed_documents(
        documents=enriched_docs,
        model_name=config.EMBEDDING_MODEL_NAME,
        batch_size=64 # Adjust batch size based on available VRAM/RAM
    )

    if embeddings_data:
        save_embeddings(embeddings_data, config.EMBEDDINGS_FILE)
        logging.info(f"Generated and saved {len(embeddings_data)} embeddings.")
        if embeddings_data:
            sample_embedding = embeddings_data[0]['embedding']
            logging.info(f"Sample embedding vector dimension: {len(sample_embedding) if sample_embedding else 'N/A'}")
            logging.info(f"Sample embedding data point: {json.dumps(embeddings_data[0], indent=2, default=lambda x: str(x)[:100])}") # Truncate long embedding
    else:
        logging.warning("No embeddings were generated.")

if __name__ == "__main__":
    main()
--- END File: src/embeddings/embed_docs.py ---

--- File: src/tests/test_lexical_index.py ---
import pytest
import os
import json
import tempfile
import shutil
from whoosh.index import open_dir
from whoosh.qparser import QueryParser

# Adjust path to import modules from src
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from ingestion.parse_ontology import main as parse_ontology_main # Need to run parsing first
from ingestion.build_lexical_index import build_index
import config # Need config to override paths

# Define a known CURIE and search terms from the test snippet
TEST_CURIE = "FOODON:00001100" # apple
TEST_LABEL = "apple"
TEST_SYNONYM = "eating apple"
TEST_DEFINITION_KEYWORD = "fruit" # Keyword from definition "The fruit of a Malus domestica tree."
TEST_RELATION_KEYWORD = "apple tree" # Keyword from relations_text (target label)


# Fixture to create a temporary directory for the test index
@pytest.fixture(scope="module")
def temp_index_dir():
    """Creates a temporary directory for the Whoosh index."""
    path = tempfile.mkdtemp(prefix="whoosh_test_index_")
    print(f"Created temp index dir: {path}")
    yield path
    print(f"Cleaning up temp index dir: {path}")
    shutil.rmtree(path)

# Fixture to run the full ingestion pipeline on the test snippet and build index
@pytest.fixture(scope="module")
def built_test_index(temp_index_dir):
    """Runs parse_ontology and build_index on the test snippet."""
    # Use temp files/dirs for test data and index
    with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as tmp_json:
        test_json_path = tmp_json.name

    # Temporarily override config paths for the test
    original_ontology_file = config.ONTOLOGY_FILE
    original_dump_path = config.ONTOLOGY_DUMP_PATH
    original_index_dir = config.WHOOSH_INDEX_DIR

    config.ONTOLOGY_FILE = config.TEST_ONTOLOGY_FILE # Use the small test snippet
    config.ONTOLOGY_DUMP_PATH = test_json_path
    config.WHOOSH_INDEX_DIR = temp_index_dir

    try:
        # 1. Run parsing
        print("\nRunning parse_ontology_main for test index build...")
        parse_ontology_main()
        assert os.path.exists(test_json_path), "Ontology dump JSON not created!"

        # 2. Run index building
        print("\nRunning build_index for test index build...")
        build_index(test_json_path, temp_index_dir)
        assert os.path.exists(os.path.join(temp_index_dir, 'SCHEMA')), "Whoosh index not created!"

        # Yield the index directory path
        yield temp_index_dir

    finally:
        # Restore original paths
        config.ONTOLOGY_FILE = original_ontology_file
        config.ONTOLOGY_DUMP_PATH = original_dump_path
        config.WHOOSH_INDEX_DIR = original_index_dir
        # Clean up temporary JSON file
        if os.path.exists(test_json_path):
            os.unlink(test_json_path)


def test_index_exists(built_test_index):
    """Tests if the index directory was created and contains index files."""
    assert os.path.exists(built_test_index)
    assert os.path.exists(os.path.join(built_test_index, 'SCHEMA'))
    assert os.path.exists(os.path.join(built_test_index, 'MAIN')) # Or other index files

def test_query_by_label_exact(built_test_index):
    """Tests exact search on the label field."""
    ix = open_dir(built_test_index)
    searcher = ix.searcher()
    # Query the 'label' field
    parser = QueryParser("label", ix.schema)
    query = parser.parse(TEST_LABEL) # "apple"
    results = searcher.search(query)

    print(f"\nSearch results for '{TEST_LABEL}': {results}")

    assert len(results) > 0, f"No results found for '{TEST_LABEL}'"
    # Check if the expected CURIE is among the results, preferably the top one
    assert results[0]['curie'] == TEST_CURIE
    assert results[0]['label'] == TEST_LABEL # Verify stored label

    searcher.close()
    ix.close() # Close index readers/writers

def test_query_by_label_fuzzy(built_test_index):
    """Tests fuzzy search on the label field."""
    ix = open_dir(built_test_index)
    searcher = ix.searcher()
    parser = QueryParser("label", ix.schema)
    # Fuzzy search for "appl" (e.g. "appl~")
    query = parser.parse("appl~") # Whoosh fuzzy syntax
    results = searcher.search(query)

    print(f"\nFuzzy search results for 'appl~': {results}")

    assert len(results) > 0, "No results found for 'appl~'"
    assert results[0]['curie'] == TEST_CURIE
    assert results[0]['label'] == TEST_LABEL

    searcher.close()
    ix.close()

def test_query_by_synonym(built_test_index):
    """Tests search on the synonyms field."""
    ix = open_dir(built_test_index)
    searcher = ix.searcher()
    # Query the 'synonyms' field
    parser = QueryParser("synonyms", ix.schema)
    query = parser.parse(TEST_SYNONYM) # "eating apple"
    results = searcher.search(query)

    print(f"\nSearch results for synonym '{TEST_SYNONYM}': {results}")

    assert len(results) > 0, f"No results found for synonym '{TEST_SYNONYM}'"
    assert results[0]['curie'] == TEST_CURIE

    searcher.close()
    ix.close()

def test_query_by_definition_keyword(built_test_index):
    """Tests search on the definition field."""
    ix = open_dir(built_test_index)
    searcher = ix.searcher()
    # Query the 'definition' field
    parser = QueryParser("definition", ix.schema)
    query = parser.parse(TEST_DEFINITION_KEYWORD) # "fruit"
    results = searcher.search(query)

    print(f"\nSearch results for definition keyword '{TEST_DEFINITION_KEYWORD}': {results}")

    assert len(results) > 0, f"No results found for definition keyword '{TEST_DEFINITION_KEYWORD}'"
    assert results[0]['curie'] == TEST_CURIE

    searcher.close()
    ix.close()

def test_query_by_relations_keyword(built_test_index):
    """Tests search on the flattened relations_text field."""
    ix = open_dir(built_test_index)
    searcher = ix.searcher()
    # Query the 'relations_text' field
    parser = QueryParser("relations_text", ix.schema)
    # Search for a keyword expected in the relations_text (e.g., target's label or part of target's CURIE)
    # Note: Indexing targets as just CURIEs requires searching for 'FOODON:00001101'
    # Indexing target labels requires adding that logic in build_lexical_index
    # Let's assume for now we search for the CURIE or part of it if indexed as text
    # Or if we enhance build_index to add target labels to relations_text:
    # relations_text = " ".join([f"{prop}: {' '.join(targets)} {' '.join(target_labels)}" for ...])
    # For simplicity with current build_index, let's search the target CURIE part
    query = parser.parse("00001101") # Search part of the target CURIE FOODON:00001101
    results = searcher.search(query)

    print(f"\nSearch results for relations keyword '00001101': {results}")

    assert len(results) > 0, f"No results found for relations keyword '00001101'"
    assert results[0]['curie'] == TEST_CURIE

    searcher.close()
    ix.close()
--- END File: src/tests/test_lexical_index.py ---

--- File: src/tests/test_parse_ontology.py ---
import pytest
import os
import json
import rdflib
from rdflib import Graph

# Adjust path to import modules from src
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from ingestion.parse_ontology import (
    load_ontology,
    extract_labels_and_synonyms,
    extract_definitions,
    extract_hierarchy,
    extract_relations,
)
from config import TEST_ONTOLOGY_FILE, NAMESPACE_MAP, RELATION_PROPERTIES
from utils.ontology_utils import uri_to_curie # Import if needed for assertions

# Define a fixture to load the test graph once for all tests
@pytest.fixture(scope="module")
def test_graph():
    """Loads the test ontology snippet into a graph."""
    if not os.path.exists(TEST_ONTOLOGY_FILE):
        pytest.skip(f"Test ontology snippet not found at {TEST_ONTOLOGY_FILE}")
    try:
        return load_ontology(TEST_ONTOLOGY_FILE)
    except Exception as e:
        pytest.fail(f"Failed to load test ontology: {e}")

# Define a known CURIE from the test snippet
TEST_CURIE = "FOODON:00001100" # apple
TEST_CURIE_PARENT1 = "FOODON:00001000" # plant-based food
TEST_CURIE_PARENT2 = "FOODON:00002000" # Pome fruit
TEST_CURIE_RELATION_TARGET = "FOODON:00001101" # apple tree (example target)
TEST_RELATION_NAME = "part_of" # example relation name

def test_load_ontology(test_graph):
    """Tests if the ontology loads and is an RDFLib Graph."""
    assert isinstance(test_graph, Graph)
    assert len(test_graph) > 0, "Test graph is empty!"
    print(f"Loaded test graph with {len(test_graph)} triples.")


def test_extract_labels_and_synonyms(test_graph):
    """Tests extraction of labels and synonyms."""
    labels_synonyms = extract_labels_and_synonyms(test_graph)
    print(f"Extracted labels/synonyms: {labels_synonyms}")

    assert TEST_CURIE in labels_synonyms
    apple_data = labels_synonyms[TEST_CURIE]
    assert apple_data['label'] == "apple"
    assert "eating apple" in apple_data['synonyms']
    assert "apple fruit" in apple_data['synonyms']
    assert TEST_CURIE_PARENT1 in labels_synonyms # Should also extract its label/synonyms
    assert labels_synonyms[TEST_CURIE_PARENT1]['label'] == "plant-based food"


def test_extract_definitions(test_graph):
    """Tests extraction of definitions."""
    definitions = extract_definitions(test_graph)
    print(f"Extracted definitions: {definitions}")

    assert TEST_CURIE in definitions
    assert "The fruit of a Malus domestica tree." in definitions[TEST_CURIE] # Use 'in' for substring check or exact match


def test_extract_hierarchy(test_graph):
    """Tests extraction of hierarchy (parents and ancestors)."""
    hierarchy = extract_hierarchy(test_graph)
    print(f"Extracted hierarchy: {hierarchy}")

    assert TEST_CURIE in hierarchy
    apple_hierarchy = hierarchy[TEST_CURIE]

    # Check direct parents
    assert TEST_CURIE_PARENT1 in apple_hierarchy['parents']
    assert TEST_CURIE_PARENT2 in apple_hierarchy['parents']
    assert len(apple_hierarchy['parents']) >= 2 # At least 2 parents from the snippet

    # Check ancestors (should include parents and parents' ancestors)
    assert TEST_CURIE_PARENT1 in apple_hierarchy['ancestors']
    assert TEST_CURIE_PARENT2 in apple_hierarchy['ancestors']
    # Assuming plant-based food has no ancestors in this snippet, ancestors == parents here
    # If plant-based food had parents, they should appear in apple's ancestors.
    # For this snippet, ancestors should be the same as parents.
    assert set(apple_hierarchy['ancestors']) == set(apple_hierarchy['parents'])


def test_extract_relations(test_graph):
    """Tests extraction of specific relations."""
    relations = extract_relations(test_graph, RELATION_PROPERTIES)
    print(f"Extracted relations: {relations}")

    assert TEST_CURIE in relations
    apple_relations = relations[TEST_CURIE]

    # Check if the specific relation from the snippet was found
    assert TEST_RELATION_NAME in apple_relations
    assert TEST_CURIE_RELATION_TARGET in apple_relations[TEST_RELATION_NAME]
    assert len(apple_relations[TEST_RELATION_NAME]) >= 1


# You could add a test that runs the full main parsing flow using the test snippet
# and checks the structure/content of the resulting JSON file.
# Example (requires a temporary file):
# import tempfile
# @pytest.fixture
# def temp_json_path():
#     with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as tmp:
#         yield tmp.name
#     os.unlink(tmp.name)
#
# def test_main_parsing_flow(temp_json_path):
#      # Temporarily override config paths for the test
#      original_ontology_file = config.ONTOLOGY_FILE
#      original_dump_path = config.ONTOLOGY_DUMP_PATH
#      config.ONTOLOGY_FILE = TEST_ONTOLOGY_FILE
#      config.ONTOLOGY_DUMP_PATH = temp_json_path
#
#      try:
#          parse_ontology.main()
#          assert os.path.exists(temp_json_path)
#          with open(temp_json_path, 'r', encoding='utf-8') as f:
#              data = json.load(f)
#
#          assert TEST_CURIE in data
#          apple_data = data[TEST_CURIE]
#          assert apple_data['label'] == 'apple'
#          assert 'synonyms' in apple_data and len(apple_data['synonyms']) > 0
#          assert 'definition' in apple_data and apple_data['definition'] is not None
#          assert 'parents' in apple_data and len(apple_data['parents']) > 0
#          assert 'ancestors' in apple_data and len(apple_data['ancestors']) > 0
#          assert 'relations' in apple_data and len(apple_data['relations']) > 0
#
#      finally:
#          # Restore original paths
#          config.ONTOLOGY_FILE = original_ontology_file
#          config.ONTOLOGY_DUMP_PATH = original_dump_path
--- END File: src/tests/test_parse_ontology.py ---

--- File: src/tests/__init__.py ---

--- END File: src/tests/__init__.py ---

--- File: src/utils/__init__.py ---

--- END File: src/utils/__init__.py ---

--- File: src/utils/ontology_utils.py ---
# src/utils/ontology_utils.py
import rdflib
from rdflib import URIRef, Namespace
from typing import Optional, Dict, Union

# Adjusted: Import CURIE_PREFIX_MAP and use it as the default.
# This assumes that the script/module *importing* ontology_utils
# has already ensured that the project root (e.g., 'onto_rag') is on sys.path,
# so that 'src' is resolvable as a top-level package.
from src.config import CURIE_PREFIX_MAP # Corrected import

def uri_to_curie(uri: Union[str, URIRef], namespace_map: Dict[str, str] = CURIE_PREFIX_MAP) -> str:
    """
    Converts a full URI to a CURIE (e.g., http://...#term -> PREFIX:term).
    Assumes namespace_map is structured as {base_uri_str: prefix_str}.
    """
    uri_str = str(uri) # Ensure it's a string

    # Iterate through the provided namespace_map (base_uri: prefix)
    # Sort by length of base_uri descending to match longest first (more specific)
    # This helps avoid issues where one base_uri is a prefix of another.
    # e.g., "http://purl.obolibrary.org/obo/" and "http://purl.obolibrary.org/obo/FOODON_"
    sorted_namespace_map_items = sorted(namespace_map.items(), key=lambda item: len(item[0]), reverse=True)

    for base_uri, prefix in sorted_namespace_map_items:
        if uri_str.startswith(base_uri):
            return f"{prefix}:{uri_str[len(base_uri):]}"

    # Fallback for common RDF/RDFS/OWL/XSD prefixes if not found in the main map
    # This local map is prefix: base_uri_str
    common_rdf_prefixes = {
        "rdf": str(rdflib.RDF),
        "rdfs": str(rdflib.RDFS),
        "owl": str(rdflib.OWL),
        "xsd": str(rdflib.XSD),
    }
    for prefix, base_uri in common_rdf_prefixes.items():
        if uri_str.startswith(base_uri):
            return f"{prefix}:{uri_str[len(base_uri):]}"
            
    # If rdflib can make a qname (usually for registered namespaces)
    try:
        g = rdflib.Graph()
        # Bind known namespaces to help compute_qname
        # For namespace_map (base_uri: prefix), we need to iterate as base_uri, prefix
        for ns_uri_str_from_map, prefix_from_map in namespace_map.items():
             g.bind(prefix_from_map, Namespace(ns_uri_str_from_map))

        # Bind common RDF ones too, in case they weren't in namespace_map or to ensure standard prefixes
        g.bind("owl", rdflib.OWL)
        g.bind("rdf", rdflib.RDF)
        g.bind("rdfs", rdflib.RDFS)
        g.bind("xsd", rdflib.XSD)

        # compute_qname might fail if the URI doesn't match any bound namespace's base
        # It returns (prefix, namespace_uri, local_name)
        qname_tuple = g.compute_qname(URIRef(uri_str))
        return f"{qname_tuple[0]}:{qname_tuple[2]}"
    except Exception: # Broad except as compute_qname can raise various things or return unexpected tuples
        pass # If rdflib fails, just return the original URI string

    return uri_str # If no CURIE conversion possible, return original URI string

def curie_to_uri(curie: str, namespace_map: Dict[str, str] = CURIE_PREFIX_MAP) -> Optional[URIRef]:
    """
    Converts a CURIE (e.g., PREFIX:term) to a full rdflib.URIRef.
    Assumes namespace_map is structured as {base_uri_str: prefix_str}.
    """
    if ':' not in curie:
        # Try to see if it's a default rdflib qname like "rdf:type" that rdflib can expand
        # This part might be less common if CURIEs are always expected with user-defined prefixes
        try:
            g = rdflib.Graph()
            # Bind namespaces from the map (base_uri: prefix)
            for ns_uri_str_from_map, prefix_from_map in namespace_map.items():
                g.bind(prefix_from_map, Namespace(ns_uri_str_from_map))
            # Bind common RDF ones
            g.bind("owl", rdflib.OWL)
            g.bind("rdf", rdflib.RDF)
            g.bind("rdfs", rdflib.RDFS)
            g.bind("xsd", rdflib.XSD)
            
            # If it's something like "owl:Class", g.namespace_manager.expand_curie will work
            expanded_uri = g.namespace_manager.expand_curie(curie)
            if str(expanded_uri) != curie: # Check if expansion actually happened
                 return URIRef(expanded_uri)
        except Exception:
            pass # If expansion fails, proceed to manual lookup
        return None # Not a valid CURIE format for our map, and rdflib couldn't expand

    prefix_part, local_name = curie.split(':', 1)

    # Iterate through namespace_map (base_uri: prefix) to find the matching prefix
    found_base_uri = None
    for base_uri_key, prefix_val in namespace_map.items():
        if prefix_val == prefix_part:
            found_base_uri = base_uri_key
            break
    
    if found_base_uri:
        return URIRef(found_base_uri + local_name)
    else:
        # Fallback: Try common RDF prefixes if not in the custom map
        # This local map is prefix: rdflib.Namespace object
        common_rdf_namespaces = {
            "rdf": rdflib.RDF,
            "rdfs": rdflib.RDFS,
            "owl": rdflib.OWL,
            "xsd": rdflib.XSD,
        }
        if prefix_part in common_rdf_namespaces:
            # Access items in the namespace like attributes: common_rdf_namespaces[prefix_part].type
            # or by string concatenation: URIRef(str(common_rdf_namespaces[prefix_part]) + local_name)
            return URIRef(str(common_rdf_namespaces[prefix_part]) + local_name)
        return None # Prefix not found in custom map or common RDF prefixes
--- END File: src/utils/ontology_utils.py ---

--- File: src/retriever/__init__.py ---

--- END File: src/retriever/__init__.py ---

--- File: src/retriever/hybrid_retriever.py ---
# src/retriever/hybrid_retriever.py
import json
import os
# import numpy as np # numpy is used by sentence_transformers and faiss internally
from whoosh.index import open_dir as open_whoosh_index
from whoosh.qparser import MultifieldParser, OrGroup
from sentence_transformers import SentenceTransformer

# --- Add project root to sys.path if running script directly ---
# This block is useful if you ever run this script directly (e.g., for debugging)
# and not as a module (python -m src.retriever.hybrid_retriever)
if __name__ == '__main__':
    import sys
    PROJECT_ROOT_FOR_DIRECT_RUN = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    if PROJECT_ROOT_FOR_DIRECT_RUN not in sys.path:
        sys.path.insert(0, PROJECT_ROOT_FOR_DIRECT_RUN)
# --- End sys.path modification ---


from src.vector_store.faiss_store import FAISSVectorStore
from src.config import (
    ONTOLOGY_DUMP_JSON,
    WHOOSH_INDEX_DIR,
    FAISS_INDEX_PATH,
    FAISS_METADATA_PATH,
    EMBEDDING_MODEL_NAME,
    DEFAULT_K_LEXICAL,
    DEFAULT_K_VECTOR,
)
# from src.utils.logger import get_logger # Placeholder for future logging
# logger = get_logger(__name__) # Placeholder

class HybridRetriever:
    def __init__(self,
                 ontology_data_path=ONTOLOGY_DUMP_JSON,
                 whoosh_index_dir=WHOOSH_INDEX_DIR,
                 faiss_index_path=FAISS_INDEX_PATH,
                 faiss_metadata_path=FAISS_METADATA_PATH,
                 embedding_model_name=EMBEDDING_MODEL_NAME):
        """
        Initializes the HybridRetriever.
        """
        print(f"Initializing HybridRetriever...")
        # logger.info("Initializing HybridRetriever...")

        print(f"Loading ontology data from: {ontology_data_path}")
        if not os.path.exists(ontology_data_path):
            raise FileNotFoundError(f"Ontology data file not found: {ontology_data_path}")
        with open(ontology_data_path, 'r', encoding='utf-8') as f:
            self.ontology_data = json.load(f)
        print(f"Loaded {len(self.ontology_data)} ontology entries.")

        print(f"Loading Whoosh index from: {whoosh_index_dir}")
        if not os.path.exists(whoosh_index_dir) or not os.listdir(whoosh_index_dir):
            raise FileNotFoundError(f"Whoosh index directory not found or empty: {whoosh_index_dir}. Run ingestion scripts.")
        self.whoosh_ix = open_whoosh_index(whoosh_index_dir)
        self.whoosh_searcher = self.whoosh_ix.searcher()
        
        # Fields to search in Whoosh, must match the schema in build_lexical_index.py
        # 'relations_text' is indexed (stored=False) so it can be searched.
        # 'curie' is an ID field, typically not directly searched with MultifieldParser unless intended.
        self.whoosh_fields_to_search = ["label", "synonyms", "definition", "relations_text"]
        self.whoosh_parser = MultifieldParser(self.whoosh_fields_to_search, schema=self.whoosh_ix.schema, group=OrGroup)
        print("Whoosh index loaded.")

        print(f"Loading embedding model: {embedding_model_name}")
        self.embedding_model = SentenceTransformer(embedding_model_name, trust_remote_code=True)
        print("Embedding model loaded.")

        print(f"Initializing FAISS vector store (index: {faiss_index_path}, metadata: {faiss_metadata_path})...")
        self.faiss_store = FAISSVectorStore(
            index_path=faiss_index_path,
            metadata_path=faiss_metadata_path,
            embeddings_file_path=None 
        )
        if not self.faiss_store.index or not self.faiss_store.metadata:
            raise FileNotFoundError(f"FAISS index file '{faiss_index_path}' or metadata file '{faiss_metadata_path}' not found or empty. Please build it first.")
        print("FAISS vector store initialized.")
        
        print("HybridRetriever initialized successfully.")

    def _lexical_search(self, query_string, limit=DEFAULT_K_LEXICAL):
        """
        Performs lexical search using Whoosh.
        Returns a list of dicts: {'id': str, 'label': str, 'score': float, 'source': 'lexical', 'details': dict}
        """
        results = []
        if not query_string:
            return results

        try:
            query = self.whoosh_parser.parse(query_string)
            search_results = self.whoosh_searcher.search(query, limit=limit)
            
            for hit in search_results:
                hit_fields = hit.fields()  # Get all stored fields as a dictionary
                term_curie = hit_fields.get('curie') # *** CHANGED: 'id' to 'curie' ***

                if term_curie is None:
                    print(f"Warning: Lexical search hit found without a 'curie'. Hit details: {hit}")
                    # logger.warning(f"Lexical search hit found without a 'curie'. Hit details: {hit}")
                    continue

                # 'relations_text' is not stored, so it won't be in hit_fields.
                # We retrieve label, synonyms, definition if they were stored.
                retrieved_label = hit_fields.get('label')
                retrieved_synonyms_str = hit_fields.get('synonyms') # This will be a space-separated string
                retrieved_definition = hit_fields.get('definition')

                results.append({
                    "id": term_curie, # Keep 'id' as the key in the result for consistency with vector search
                    "label": retrieved_label if retrieved_label is not None else self.ontology_data.get(term_curie, {}).get('label', 'N/A'),
                    "score": hit.score, 
                    "source": "lexical",
                    "details": {
                        # Convert synonyms string back to list if needed, or keep as string
                        "retrieved_synonyms": retrieved_synonyms_str.split() if retrieved_synonyms_str else [], 
                        "retrieved_definition": retrieved_definition
                    }
                })
        except Exception as e:
            print(f"Error during lexical search for '{query_string}': {e}")
            # logger.error(f"Error during lexical search for '{query_string}': {e}", exc_info=True)
            import traceback
            traceback.print_exc() # Print full traceback for debugging
        return results

    def _vector_search(self, query_string, k=DEFAULT_K_VECTOR):
        """
        Performs vector search using FAISS.
        Returns a list of dicts: {'id': str, 'label': str, 'score': float, 'source': 'vector', 'details': dict}
        """
        results = []
        if not query_string:
            return results

        try:
            query_vector = self.embedding_model.encode([query_string], convert_to_numpy=True)
            distances, _, metadata_items = self.faiss_store.search(query_vector, k=k)
            
            for i in range(len(metadata_items)):
                term_id = metadata_items[i]['id'] # FAISS metadata stores 'id'
                results.append({
                    "id": term_id,
                    "label": metadata_items[i]['label'],
                    "score": float(distances[i]), 
                    "source": "vector",
                    "details": {}
                })
        except Exception as e:
            print(f"Error during vector search for '{query_string}': {e}")
            # logger.error(f"Error during vector search for '{query_string}': {e}", exc_info=True)
            import traceback
            traceback.print_exc() # Print full traceback for debugging
        return results

    def search(self, query_string, lexical_limit=DEFAULT_K_LEXICAL, vector_k=DEFAULT_K_VECTOR):
        """
        Performs hybrid search.
        """
        lexical_results = self._lexical_search(query_string, limit=lexical_limit)
        vector_results = self._vector_search(query_string, k=vector_k)
        
        return {
            "query": query_string,
            "lexical_results": lexical_results,
            "vector_results": vector_results,
        }

    def get_term_details(self, term_id):
        """
        Retrieves full details for a given term ID (CURIE) from the loaded ontology data.
        """
        return self.ontology_data.get(term_id)

    def close(self):
        """
        Closes any open resources, like the Whoosh searcher.
        """
        if self.whoosh_searcher:
            self.whoosh_searcher.close()
        print("HybridRetriever resources closed.")

# Example Usage (for testing purposes)
if __name__ == '__main__':
    # This sys.path modification is now at the top of the file for when __name__ == '__main__'
    
    from src.config import PROJECT_ROOT # Import after sys.path is potentially modified
    print(f"Configured project root: {PROJECT_ROOT}")
    if not os.getcwd().startswith(PROJECT_ROOT) and os.getcwd() != PROJECT_ROOT:
         print(f"Warning: Current working directory ({os.getcwd()}) might not be the project root.")
         print("Consider running with 'python -m src.retriever.hybrid_retriever' from the project root directory.")

    print("Running HybridRetriever example...")
    retriever = None
    try:
        retriever = HybridRetriever()
        
        queries = ["GARLIC", "SALT", "GARBANZO", "TAHINI", "LEMON JUICE", "HONEY" ,"WATER", "OLIVE OIL", "ROSMARY", "HUMMUS"]
        
        for query in queries:
            print(f"\nSearching for: '{query}'")
            results = retriever.search(query, lexical_limit=3, vector_k=3)
            
            print("\n--- Lexical Results ---")
            if results["lexical_results"]:
                for res in results["lexical_results"]:
                    print(f"  ID: {res['id']}, Label: {res['label']}, Score (Whoosh): {res['score']:.4f}")
                    # print(f"    Details: {res['details']}") # Uncomment to see retrieved synonyms/def
            else:
                print("  No lexical results.")

            print("\n--- Vector Results ---")
            if results["vector_results"]:
                for res in results["vector_results"]:
                    print(f"  ID: {res['id']}, Label: {res['label']}, Score (L2 Distance): {res['score']:.4f}")
            else:
                print("  No vector results.")
            print("-" * 40)
            
    except FileNotFoundError as e:
        print(f"\nERROR: A required file was not found: {e}")
        print("Please ensure all data files (ontology_dump.json) and indices (Whoosh, FAISS) are correctly built and paths are set in src/config.py.")
        print("You might need to run the ingestion and embedding scripts first.")
    except Exception as e:
        print(f"\nAn unexpected error occurred during example run: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if retriever:
            retriever.close()
--- END File: src/retriever/hybrid_retriever.py ---

--- File: src/vector_store/__init__.py ---

--- END File: src/vector_store/__init__.py ---

--- File: src/vector_store/faiss_store.py ---
# src/vector_store/faiss_store.py
import faiss
import json
import numpy as np
import os
from src.config import EMBEDDINGS_FILE # Default embeddings file to build from

# from src.utils.logger import get_logger # Placeholder for future logging
# logger = get_logger(__name__)

class FAISSVectorStore:
    def __init__(self, index_path, metadata_path, embeddings_file_path=None, dimension=None):
        """
        Initializes the FAISSVectorStore.
        Tries to load an existing index and metadata. If not found, and an
        embeddings_file_path is provided, it will attempt to build them.

        Args:
            index_path (str): Path to save/load the FAISS index file (.bin).
            metadata_path (str): Path to save/load the metadata JSON file (.json).
            embeddings_file_path (str, optional): Path to the 'embeddings.json' file.
                                                  Used only if the index/metadata needs to be built.
                                                  Defaults to config.EMBEDDINGS_FILE.
            dimension (int, optional): The dimension of the vectors.
                                       Required if building the index and it cannot be inferred.
                                       Usually inferred from the first embedding.
        """
        self.index_path = index_path
        self.metadata_path = metadata_path
        self.embeddings_file_path = embeddings_file_path if embeddings_file_path else EMBEDDINGS_FILE
        self.dimension = dimension

        self.index = None
        self.metadata = []  # List of dicts, e.g., [{'id': 'FOODON_123', 'label': 'Apple'}, ...]

        if os.path.exists(self.index_path) and os.path.exists(self.metadata_path):
            print(f"Loading existing FAISS index from {self.index_path} and metadata from {self.metadata_path}")
            # logger.info(f"Loading existing FAISS index from {self.index_path} and metadata from {self.metadata_path}")
            try:
                self.load_store()
            except Exception as e:
                print(f"Error loading existing FAISS store: {e}. Will attempt to build if embeddings file provided.")
                # logger.error(f"Error loading existing FAISS store: {e}. Will attempt to build if embeddings file provided.", exc_info=True)
                self._try_build_store()
        elif self.embeddings_file_path and os.path.exists(self.embeddings_file_path):
            print(f"FAISS index/metadata not found. Attempting to build from {self.embeddings_file_path}")
            # logger.info(f"FAISS index/metadata not found. Attempting to build from {self.embeddings_file_path}")
            self._try_build_store()
        else:
            message = "FAISS index/metadata not found. "
            if self.embeddings_file_path:
                message += f"Embeddings file {self.embeddings_file_path} also not found or not specified for building."
            else:
                message += "No embeddings file path provided to build a new store."
            print(f"Warning: {message}")
            # logger.warning(message)

    def _try_build_store(self):
        """Helper method to attempt building the store."""
        embeddings_data = self._load_embeddings_data()
        if embeddings_data:
            self.build_index_from_embeddings(embeddings_data)
            if self.index and self.metadata: # Check if build was successful
                 self.save_store()
            else:
                print("Warning: FAISS index or metadata not built successfully from embeddings data.")
                # logger.warning("FAISS index or metadata not built successfully from embeddings data.")
        else:
            print(f"Warning: No embeddings data loaded from {self.embeddings_file_path}, FAISS index not built.")
            # logger.warning(f"No embeddings data loaded from {self.embeddings_file_path}, FAISS index not built.")


    def _load_embeddings_data(self):
        """
        Loads embeddings data from the specified JSON file.
        Expected format: list of {'id': str, 'label': str, 'embedding': list[float]}
        """
        if not self.embeddings_file_path or not os.path.exists(self.embeddings_file_path):
            print(f"Error: Embeddings file not found: {self.embeddings_file_path}")
            # logger.error(f"Embeddings file not found: {self.embeddings_file_path}")
            return None
        try:
            with open(self.embeddings_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            # Sanity check the data structure
            if not isinstance(data, list) or not data:
                print(f"Error: Embeddings file {self.embeddings_file_path} is empty or not a list.")
                # logger.error(f"Embeddings file {self.embeddings_file_path} is empty or not a list.")
                return None
            if not all('id' in item and 'label' in item and 'embedding' in item for item in data):
                print(f"Error: Embeddings data in {self.embeddings_file_path} has incorrect format.")
                # logger.error(f"Embeddings data in {self.embeddings_file_path} has incorrect format.")
                return None

            print(f"Loaded {len(data)} embeddings from {self.embeddings_file_path}")
            # logger.info(f"Loaded {len(data)} embeddings from {self.embeddings_file_path}")
            return data
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON from embeddings file {self.embeddings_file_path}: {e}")
            # logger.error(f"Error decoding JSON from embeddings file {self.embeddings_file_path}: {e}", exc_info=True)
            return None
        except Exception as e:
            print(f"An unexpected error occurred while loading embeddings file {self.embeddings_file_path}: {e}")
            # logger.error(f"An unexpected error occurred while loading embeddings file {self.embeddings_file_path}: {e}", exc_info=True)
            return None

    def build_index_from_embeddings(self, embeddings_data):
        """
        Builds the FAISS index and prepares metadata from loaded embeddings data.

        Args:
            embeddings_data (list): List of dictionaries, where each dict has
                                    'id', 'label', and 'embedding' keys.
        """
        if not embeddings_data:
            print("Warning: No embeddings data provided to build_index_from_embeddings.")
            # logger.warning("No embeddings data provided to build_index_from_embeddings.")
            return

        vectors = []
        current_metadata = [] # Use a temporary list to build metadata
        for item in embeddings_data:
            vectors.append(item['embedding'])
            current_metadata.append({'id': item['id'], 'label': item['label']})

        if not vectors:
            print("Warning: No vectors extracted from embeddings_data.")
            # logger.warning("No vectors extracted from embeddings_data.")
            return

        vectors_np = np.array(vectors).astype('float32')

        if self.dimension is None:
            self.dimension = vectors_np.shape[1]
        elif self.dimension != vectors_np.shape[1]:
            err_msg = f"Provided dimension {self.dimension} does not match embeddings dimension {vectors_np.shape[1]}"
            print(f"Error: {err_msg}")
            # logger.error(err_msg)
            raise ValueError(err_msg)

        # Using IndexFlatL2, a simple L2 distance index.
        # For larger datasets, more advanced indexes like IndexIVFFlat might be better.
        try:
            self.index = faiss.IndexFlatL2(self.dimension)
            self.index.add(vectors_np)
            self.metadata = current_metadata # Assign once build is successful
            print(f"FAISS index built successfully with {self.index.ntotal} vectors of dimension {self.dimension}.")
            # logger.info(f"FAISS index built successfully with {self.index.ntotal} vectors of dimension {self.dimension}.")
        except Exception as e:
            print(f"Error building FAISS index: {e}")
            # logger.error(f"Error building FAISS index: {e}", exc_info=True)
            self.index = None # Ensure index is None if build fails
            self.metadata = []


    def save_store(self):
        """Saves the FAISS index and metadata to their respective files."""
        if self.index is None:
            print("Warning: No FAISS index to save.")
            # logger.warning("No FAISS index to save.")
            return # Do not save metadata if index is not there or failed to build

        os.makedirs(os.path.dirname(self.index_path), exist_ok=True)
        faiss.write_index(self.index, self.index_path)
        print(f"FAISS index saved to {self.index_path}")
        # logger.info(f"FAISS index saved to {self.index_path}")

        if not self.metadata:
            print("Warning: No FAISS metadata to save (metadata list is empty).")
            # logger.warning("No FAISS metadata to save (metadata list is empty).")
            # If index exists but metadata is empty, this is an inconsistent state.
            # Depending on strictness, one might choose to not save the index either,
            # or clear the index file if it exists. For now, we save index if it exists.
            return

        os.makedirs(os.path.dirname(self.metadata_path), exist_ok=True)
        with open(self.metadata_path, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, indent=4)
        print(f"FAISS metadata saved to {self.metadata_path}")
        # logger.info(f"FAISS metadata saved to {self.metadata_path}")

    def load_store(self):
        """Loads the FAISS index and metadata from files."""
        if not os.path.exists(self.index_path):
            # logger.error(f"FAISS index file not found: {self.index_path}")
            raise FileNotFoundError(f"FAISS index file not found: {self.index_path}")
        self.index = faiss.read_index(self.index_path)
        if self.dimension is None: # Infer dimension if not set
            self.dimension = self.index.d
        elif self.dimension != self.index.d: # Check consistency
            # logger.warning(f"Stored index dimension {self.index.d} differs from configured {self.dimension}. Using stored.")
            print(f"Warning: Stored index dimension {self.index.d} differs from configured {self.dimension}. Using stored.")
            self.dimension = self.index.d

        print(f"FAISS index loaded from {self.index_path}. Index has {self.index.ntotal} vectors of dim {self.index.d}.")
        # logger.info(f"FAISS index loaded from {self.index_path}. Index has {self.index.ntotal} vectors of dim {self.index.d}.")

        if not os.path.exists(self.metadata_path):
            # logger.error(f"FAISS metadata file not found: {self.metadata_path}")
            raise FileNotFoundError(f"FAISS metadata file not found: {self.metadata_path}")
        with open(self.metadata_path, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        print(f"FAISS metadata loaded from {self.metadata_path}. {len(self.metadata)} items.")
        # logger.info(f"FAISS metadata loaded from {self.metadata_path}. {len(self.metadata)} items.")

        if self.index and self.metadata and self.index.ntotal != len(self.metadata):
            mismatch_msg = (f"Mismatch between FAISS index size ({self.index.ntotal}) "
                            f"and metadata size ({len(self.metadata)}). This may lead to errors.")
            print(f"Warning: {mismatch_msg}")
            # logger.warning(mismatch_msg)
            # Potentially raise an error or try to reconcile, for now, just warn.

    def search(self, query_vector, k=5):
        """
        Performs a K-Nearest Neighbors search on the FAISS index.

        Args:
            query_vector (np.ndarray): A 2D numpy array of shape (num_queries, dimension)
                                       or a 1D numpy array (single query).
            k (int): The number of nearest neighbors to retrieve.

        Returns:
            tuple: (distances, indices, metadata_items)
                   distances (np.ndarray): Distances to the k nearest neighbors.
                   indices (np.ndarray): FAISS internal indices of the k nearest neighbors.
                   metadata_items (list of lists or list of dicts): Corresponding metadata for the results.
                                     If single query, returns list of dicts. If multiple, list of lists of dicts.
        """
        if self.index is None:
            print("Error: FAISS index not initialized. Cannot perform search.")
            # logger.error("FAISS index not initialized. Cannot perform search.")
            return np.array([]), np.array([]), []

        if not isinstance(query_vector, np.ndarray):
            query_vector = np.array(query_vector)

        if query_vector.ndim == 1: # Single query vector
            query_vector = np.expand_dims(query_vector, axis=0)
        
        if query_vector.shape[1] != self.index.d:
            err_msg = f"Query vector dimension ({query_vector.shape[1]}) does not match index dimension ({self.index.d})."
            print(f"Error: {err_msg}")
            # logger.error(err_msg)
            raise ValueError(err_msg)

        distances, faiss_indices = self.index.search(query_vector.astype('float32'), k)
        
        # faiss_indices will be shape (num_queries, k)
        # distances will be shape (num_queries, k)
        
        all_results_metadata = []
        for i in range(faiss_indices.shape[0]): # Iterate over queries
            query_results_metadata = []
            for j in range(faiss_indices.shape[1]): # Iterate over k results for that query
                idx = faiss_indices[i][j]
                if idx != -1 and idx < len(self.metadata): # faiss_index can be -1 if k > ntotal
                    query_results_metadata.append(self.metadata[idx])
                # else: could append a placeholder, or log. For now, items are just shorter if fewer than k found.
            all_results_metadata.append(query_results_metadata)
        
        # For a single query_vector (most common use case here), return the inner list directly.
        if query_vector.shape[0] == 1:
            return distances[0], faiss_indices[0], all_results_metadata[0]
        else: # If multiple query vectors were passed
            return distances, faiss_indices, all_results_metadata

# Example usage / test function
if __name__ == '__main__':
    from src.vector_store.faiss_store import FAISSVectorStore
    from src.config import FAISS_INDEX_PATH, FAISS_METADATA_PATH

    # remove old files so the class knows it has to build
    import os, pathlib, json
    for p in (FAISS_INDEX_PATH, FAISS_METADATA_PATH):
        pathlib.Path(p).unlink(missing_ok=True)

    store = FAISSVectorStore(
        index_path     = FAISS_INDEX_PATH,
        metadata_path  = FAISS_METADATA_PATH,
        # embeddings_file_path=None  â†’ default picks data/embeddings.json
        # dimension=None            â†’ it will infer 384 automatically
    )
    print(f"Built FAISS index with {store.index.ntotal} vectors of dim {store.index.d}")
--- END File: src/vector_store/faiss_store.py ---

--- File: src/prompts/reranker.tpl ---

--- END File: src/prompts/reranker.tpl ---

--- File: src/prompts/hybrid_retriever.tpl ---

--- END File: src/prompts/hybrid_retriever.tpl ---

--- File: src/prompts/parse_ontology.tpl ---

--- END File: src/prompts/parse_ontology.tpl ---

--- File: src/evaluation/evaluate_retriever_recall.py ---
# src/evaluation/evaluate_retriever_recall.py

import xml.etree.ElementTree as ET
import os
import sys
import logging
from collections import defaultdict

# Add project root to Python path to allow direct imports from src
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(PROJECT_ROOT)

from src.retriever.hybrid_retriever import HybridRetriever
from src.config import (
    ONTOLOGY_DUMP_JSON,
    WHOOSH_INDEX_DIR,
    FAISS_INDEX_PATH,
    FAISS_METADATA_PATH,
    EMBEDDING_MODEL_NAME,
    CURIE_PREFIX_MAP,
    DEFAULT_K_LEXICAL,
    DEFAULT_K_VECTOR,
    DEFAULT_RERANK_K
)
from src.utils.ontology_utils import uri_to_curie

# --- Configuration for this specific evaluation script ---
# Path to your evaluation XML file
EVALUATION_XML_FILE = os.path.join(PROJECT_ROOT, "data", "CafeteriaFCD_foodon.xml")
# Top K results from the combined retriever output to consider for a match
RECALL_AT_K = DEFAULT_RERANK_K # How many retrieved items to check for a match

# --- Logging Setup ---
# Stays with basicConfig as src.utils.logging.get_logger is "to be developed"
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def parse_evaluation_xml(xml_file_path: str) -> list:
    """
    Parses the evaluation XML file to extract entities and their ground truth semantic tags.

    Args:
        xml_file_path (str): Path to the XML file.

    Returns:
        list: A list of dictionaries, each containing:
              {'text': str, 'true_curies': set_of_str, 'doc_id': str, 'ann_id': str}
              Returns an empty list if parsing fails.
    """
    if not os.path.exists(xml_file_path):
        logger.error(f"Evaluation XML file not found: {xml_file_path}")
        return []

    gold_standard_data = []
    try:
        tree = ET.parse(xml_file_path)
        root = tree.getroot()
        for doc_idx, document_node in enumerate(root.findall('.//document')):
            doc_id_node = document_node.find('id')
            doc_id = doc_id_node.text if doc_id_node is not None else f"doc_{doc_idx}"
            
            annotations = document_node.findall('annotation')
            for ann_idx, annotation_node in enumerate(annotations):
                entity_text_node = annotation_node.find('text')
                semantic_tags_node = annotation_node.find('infon[@key="semantic_tags"]')
                ann_id_val = annotation_node.get('id', f"ann_{doc_idx}_{ann_idx}")


                if entity_text_node is not None and semantic_tags_node is not None and entity_text_node.text is not None and semantic_tags_node.text is not None:
                    entity_text = entity_text_node.text.strip()
                    
                    raw_tags = semantic_tags_node.text.strip()
                    true_uris = {tag.strip() for tag in raw_tags.split(';') if tag.strip()}
                    
                    true_curies = set()
                    for uri in true_uris:
                        try:
                            # Adjusted: Use CURIE_PREFIX_MAP
                            curie = uri_to_curie(uri, CURIE_PREFIX_MAP)
                            if curie: 
                                true_curies.add(curie)
                            else:
                                logger.warning(f"Could not convert URI to CURIE: {uri} for entity '{entity_text}' in {doc_id} (ann: {ann_id_val})")
                        except Exception as e:
                            logger.error(f"Error converting URI {uri} to CURIE: {e}")
                    
                    if entity_text and true_curies:
                        gold_standard_data.append({
                            'text': entity_text,
                            'true_curies': true_curies,
                            'doc_id': doc_id,
                            'ann_id': ann_id_val
                        })
                    elif entity_text: # Has text but no convertible true CURIEs
                        logger.warning(f"Entity '{entity_text}' in {doc_id} (ann: {ann_id_val}) had no convertible true CURIEs from URIs: {true_uris}")

                # else: # This can be too verbose if many annotations don't have these specific fields
                #     logger.debug(f"Annotation missing text or semantic_tags in {doc_id}, ann_id={ann_id_val}")


    except ET.ParseError as e:
        logger.error(f"Error parsing XML file {xml_file_path}: {e}")
        return []
    except Exception as e:
        logger.error(f"An unexpected error occurred during XML parsing: {e}", exc_info=True)
        return []
        
    logger.info(f"Successfully parsed {len(gold_standard_data)} entities with text and true CURIEs from {xml_file_path}")
    return gold_standard_data

def evaluate_retriever(retriever: HybridRetriever, gold_standard_data: list, recall_at_k: int, lexical_k: int, vector_k: int) -> tuple[float, int, int]:
    """
    Evaluates the retriever against the gold standard data.

    Args:
        retriever (HybridRetriever): The initialized hybrid retriever.
        gold_standard_data (list): List of gold standard entities and their CURIEs.
        recall_at_k (int): The K value for Recall@K (slice of combined results).
        lexical_k (int): Number of results to fetch from lexical search.
        vector_k (int): Number of results to fetch from vector search.

    Returns:
        tuple: (recall_score, total_entities_processed, hits)
    """
    total_entities_processed = 0
    hits = 0
    
    if not gold_standard_data:
        logger.warning("No gold standard data provided for evaluation.")
        return 0.0, 0, 0

    for i, item in enumerate(gold_standard_data):
        query_text = item['text']
        true_curies = item['true_curies']
        
        if not query_text or not true_curies:
            # This should ideally be filtered by parse_evaluation_xml already
            logger.warning(f"Skipping item with empty query text or true_curies: {item}")
            continue
            
        total_entities_processed += 1
        
        logger.debug(f"({i+1}/{len(gold_standard_data)}) Querying for: '{query_text}', True CURIEs: {true_curies}")

        try:
            # Adjusted: HybridRetriever.search returns a dict: {"lexical_results": [], "vector_results": []}
            retriever_output_dict = retriever.search(
                query_string=query_text,
                lexical_limit=lexical_k,
                vector_k=vector_k
            )
            
            lexical_results = retriever_output_dict.get("lexical_results", [])
            vector_results = retriever_output_dict.get("vector_results", [])

            # Combine and deduplicate results, lexical first then vector.
            # Scores are not comparable, so this is a simple merge strategy.
            # The 'id' field from result dicts contains the CURIE.
            combined_ordered_results = []
            seen_ids = set()

            for doc in lexical_results:
                doc_id = doc.get('id') # 'id' is the CURIE
                if doc_id and doc_id not in seen_ids:
                    combined_ordered_results.append(doc) # doc contains 'id', 'label', 'score', etc.
                    seen_ids.add(doc_id)
            
            for doc in vector_results:
                doc_id = doc.get('id') # 'id' is the CURIE
                if doc_id and doc_id not in seen_ids:
                    combined_ordered_results.append(doc)
                    seen_ids.add(doc_id)
            
            # Extract the CURIEs from the top `recall_at_k` combined documents
            # Adjusted: use doc['id'] as it stores the CURIE
            retrieved_curies_set = {doc['id'] for doc in combined_ordered_results[:recall_at_k]}
            logger.debug(f"Retrieved CURIEs (top {recall_at_k} from combined): {retrieved_curies_set}")

            if not true_curies.isdisjoint(retrieved_curies_set):
                hits += 1
                logger.info(f"HIT! Query: '{query_text}'. True: {true_curies}. Found in top {recall_at_k}: {true_curies.intersection(retrieved_curies_set)}")
            else:
                logger.info(f"MISS. Query: '{query_text}'. True: {true_curies}. Top {recall_at_k} (combined) CURIEs: {retrieved_curies_set}")
                # For misses, log more details if needed, e.g. full combined_ordered_results
                # logger.debug(f"Full combined/ordered results for miss: {combined_ordered_results}")


        except Exception as e:
            logger.error(f"Error during retrieval or processing for query '{query_text}': {e}", exc_info=True)
            
    if total_entities_processed == 0:
        logger.warning("No valid entities were processed for evaluation.")
        return 0.0, 0, 0
        
    recall_score = hits / total_entities_processed
    return recall_score, total_entities_processed, hits

def main():
    logger.info("Starting Retriever Evaluation Script...")

    # 1. Parse the Gold Standard XML
    logger.info(f"Loading gold standard data from: {EVALUATION_XML_FILE}")
    gold_standard_data = parse_evaluation_xml(EVALUATION_XML_FILE)
    if not gold_standard_data:
        logger.error("Failed to load or parse gold standard data. Exiting.")
        return

    # 2. Initialize the HybridRetriever
    logger.info("Initializing HybridRetriever...")
    try:
        # Ensure all paths are valid and files exist where expected by HybridRetriever
        # Adjusted: Use ONTOLOGY_DUMP_JSON for ontology_data_path
        if not os.path.exists(ONTOLOGY_DUMP_JSON):
            logger.error(f"Ontology dump not found: {ONTOLOGY_DUMP_JSON}. Run ingestion pipeline.")
            return
        if not os.path.exists(WHOOSH_INDEX_DIR) or not os.listdir(WHOOSH_INDEX_DIR): # Check if dir exists and is not empty
             logger.error(f"Whoosh index directory is empty or not found: {WHOOSH_INDEX_DIR}. Run ingestion pipeline.")
             return
        if not os.path.exists(FAISS_INDEX_PATH):
            logger.error(f"FAISS index not found: {FAISS_INDEX_PATH}. Run ingestion pipeline.")
            return
        if not os.path.exists(FAISS_METADATA_PATH):
            logger.error(f"FAISS metadata not found: {FAISS_METADATA_PATH}. Run ingestion pipeline.")
            return

        retriever = HybridRetriever(
            ontology_data_path=ONTOLOGY_DUMP_JSON, # Adjusted
            whoosh_index_dir=WHOOSH_INDEX_DIR,
            faiss_index_path=FAISS_INDEX_PATH,
            faiss_metadata_path=FAISS_METADATA_PATH,
            embedding_model_name=EMBEDDING_MODEL_NAME
        )
        logger.info("HybridRetriever initialized successfully.")
    except Exception as e:
        logger.error(f"Failed to initialize HybridRetriever: {e}", exc_info=True)
        return

    # 3. Perform Evaluation
    logger.info(f"Starting evaluation with Recall@{RECALL_AT_K}...")
    # Adjusted: Use DEFAULT_K_LEXICAL and DEFAULT_K_VECTOR
    logger.info(f"HybridRetriever search params: Lexical K={DEFAULT_K_LEXICAL}, Vector K={DEFAULT_K_VECTOR}")
    
    recall_score, total_entities, hits = evaluate_retriever(
        retriever, 
        gold_standard_data, 
        recall_at_k=RECALL_AT_K,
        lexical_k=DEFAULT_K_LEXICAL, # Adjusted
        vector_k=DEFAULT_K_VECTOR    # Adjusted
    )

    # 4. Print Results
    logger.info("--- Evaluation Results ---")
    logger.info(f"Total entities processed: {total_entities}")
    logger.info(f"Number of hits (at least one true CURIE found in top {RECALL_AT_K} combined results): {hits}")
    if total_entities > 0:
        logger.info(f"Recall@{RECALL_AT_K}: {recall_score:.4f}")
    else:
        logger.info("Recall not calculated as no entities were processed.")

    # 5. Clean up
    try:
        if hasattr(retriever, 'close') and callable(retriever.close):
            retriever.close()
            logger.info("Retriever resources closed.")
    except Exception as e:
        logger.error(f"Error closing retriever resources: {e}")

if __name__ == "__main__":
    eval_dir = os.path.dirname(EVALUATION_XML_FILE)
    if not os.path.exists(eval_dir):
        try:
            os.makedirs(eval_dir)
            logger.info(f"Created directory: {eval_dir}")
            logger.info(f"Please place '{os.path.basename(EVALUATION_XML_FILE)}' in {eval_dir} to run the evaluation.")
        except OSError as e:
            logger.error(f"Failed to create directory {eval_dir}: {e}")
            sys.exit(1) # Exit if cannot create data directory for eval file
    
    if not os.path.exists(EVALUATION_XML_FILE):
        logger.error(f"Evaluation XML file '{EVALUATION_XML_FILE}' not found. Please place it in the correct directory.")
    else:
        main()
--- END File: src/evaluation/evaluate_retriever_recall.py ---

