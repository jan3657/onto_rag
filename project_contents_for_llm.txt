--- File: requirements.txt ---
# onto_rag/requirements.txt

# Core Ontology Processing
rdflib>=6.0.0

# Lexical Indexing
whoosh>=2.7.0

# Embeddings & Semantic Search
sentence-transformers>=2.2.0
faiss-cpu>=1.7.0   # Or faiss-gpu if you have a CUDA-enabled GPU and want to use it

# Utilities
numpy>=1.21.0     # Often a dependency, good to specify
pandas>=1.3.0     # Dependency for libraries like 'datasets' used by sentence-transformers
python-dotenv>=0.19.0 # For loading .env files
tqdm>=4.60.0      # For progress bars in long operations

# Testing (optional, but good for development)
pytest>=6.2.0
--- END File: requirements.txt ---

--- File: review_app.py ---
import streamlit as st
import json
from pathlib import Path

# --- Configuration ---
# Set the path to the human-readable evaluation results file.
# This script assumes the file is in the 'data' subdirectory.
EVALUATION_FILE_PATH = Path("data") / "readable_evaluation_results.json"

# --- Helper Functions ---

@st.cache_data
def load_data(file_path: Path) -> list:
    """
    Loads the evaluation data from the specified JSON file.
    The @st.cache_data decorator ensures the data is loaded only once.
    """
    if not file_path.exists():
        st.error(f"Error: Evaluation file not found at '{file_path}'.")
        st.info("Please run the `scripts/format_evaluation_results.py` script first to generate this file.")
        return None
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def display_term_details(term_data: dict):
    """Renders the details of a single ontology term without a main title."""
    if not term_data or not term_data.get("curie"):
        st.warning("No data available for this term.")
        return

    # Display the label and CURIE
    label = term_data.get('label', 'N/A')
    curie = term_data.get('curie', 'N/A')
    st.markdown(f"**{label}** (`{curie}`)")

    # Display the definition in an info box
    definition = term_data.get('definition')
    if definition:
        st.info(f"**Definition:** {definition}")
    else:
        st.info("No definition provided.")

    # Display synonyms if they exist
    synonyms = term_data.get('synonyms', [])
    if synonyms:
        st.markdown(f"**Synonyms:** *{', '.join(synonyms)}*")

def display_term(term_data: dict, title: str):
    """Renders a single ontology term's details in a structured format."""
    st.subheader(title)
    display_term_details(term_data)

# --- Main Application Logic ---

# Set the page configuration (title, icon, layout)
st.set_page_config(
    page_title="Ontology Linking Review",
    page_icon="ğŸ§ª",
    layout="wide"
)

st.title("ğŸ§ª Ontology Linking Evaluation Review")
st.markdown("An interface for experts to review the performance of the entity linking model.")

# Load the data using the cached function
data = load_data(EVALUATION_FILE_PATH)

if data:
    # --- Session State for Navigation ---
    # Initialize the session state to keep track of the current item index
    if 'current_index' not in st.session_state:
        st.session_state.current_index = 0

    # --- Navigation Controls ---
    st.sidebar.header("Navigation")
    # Allow selection by query text for easy lookup
    query_list = [f"{i+1}. {item['query']}" for i, item in enumerate(data)]
    selected_query = st.sidebar.selectbox("Select a Query to Review:", query_list, index=st.session_state.current_index)

    # Update index based on selection
    st.session_state.current_index = query_list.index(selected_query)

    col1, col2 = st.sidebar.columns(2)
    # "Previous" button
    if col1.button("â¬…ï¸ Previous", use_container_width=True):
        if st.session_state.current_index > 0:
            st.session_state.current_index -= 1
            st.rerun()
    # "Next" button
    if col2.button("Next â¡ï¸", use_container_width=True):
        if st.session_state.current_index < len(data) - 1:
            st.session_state.current_index += 1
            st.rerun()

    # --- Display the selected item ---
    item = data[st.session_state.current_index]
    query = item.get("query")

    st.header(f"Reviewing Query: \"{query}\"", divider="rainbow")

    # --- CHANGE 1: Removed the Correct/Incorrect status message ---
    # The block checking item.get("is_correct") was removed from here.

    # Use columns for a side-by-side comparison
    left_col, right_col = st.columns(2)

    with left_col:
        # Display the model's chosen term
        display_term(item.get("chosen_term"), "ğŸ¤– Model's Choice")
        # Display the model's reasoning in an expandable section
        with st.expander("Show Model's Explanation"):
            st.info(item.get("explanation", "No explanation provided."))

    with right_col:
        # --- CHANGE 2: Display ALL ground truth terms with full details ---
        st.subheader("ğŸ¯ Ground Truth(s)")
        ground_truth_terms = item.get("ground_truth_terms", [])
        
        if not ground_truth_terms:
             st.warning("No ground truth terms provided for this query.")
        else:
            for i, term in enumerate(ground_truth_terms):
                # Add a separator between terms for clarity, but not before the first one
                if i > 0:
                    st.markdown("---")
                display_term_details(term)


    # --- Display the list of all candidates provided to the model ---
    st.markdown("---")
    with st.expander("ğŸ•µï¸â€â™€ï¸ View All Candidate Terms Provided to Model"):
        st.markdown("This is the full list of options the model had to choose from.")

        candidates = item.get("candidate_terms_provided", [])
        chosen_curie = item.get("chosen_term", {}).get("curie")
        ground_truth_curies = [gt.get("curie") for gt in item.get("ground_truth_terms", [])]

        if not candidates:
            st.info("No candidate terms were provided to the model for this query.")
        else:
            for candidate in candidates:
                label = candidate.get('label', 'N/A')
                curie = candidate.get('curie', 'N/A')

                # Highlight chosen and ground truth terms
                marker = ""
                if curie == chosen_curie:
                    marker += "ğŸ¤–"
                if curie in ground_truth_curies:
                    marker += "ğŸ¯"

                st.markdown(f"**{marker} {label}** (`{curie}`)")
                definition = candidate.get('definition')
                if definition:
                    st.text(f"  - {definition[:200]}...") # Truncate long definitions
                else:
                    st.text("  - No definition.")
--- END File: review_app.py ---

--- File: results.txt ---
llama:
2025-06-11 16:06:24,274 - INFO - --- Evaluation Complete ---
2025-06-11 16:06:24,274 - INFO - Total entities evaluated: 948
2025-06-11 16:06:24,274 - INFO - Retrieval Failures (no candidates): 0
2025-06-11 16:06:24,274 - INFO - Selection Failures (LLM error): 0
2025-06-11 16:06:24,274 - INFO - ---------------------------
2025-06-11 16:06:24,274 - INFO - Valid attempts for selector: 948
2025-06-11 16:06:24,274 - INFO - Correct selections (Hits): 386
2025-06-11 16:06:24,275 - INFO - Accuracy: 0.4072 (386/948)
2025-06-11 16:06:24,275 - INFO - Saving 562 incorrect selections to /Users/jandrole/projects/onto_rag/evaluation_results.json
2025-06-11 16:06:24,301 - INFO - Evaluation finished.

gemini:
2025-06-16 10:43:04,962 - INFO - --- Evaluation Complete ---
2025-06-16 10:43:04,962 - INFO - Total entities evaluated: 948
2025-06-16 10:43:04,962 - INFO - Retrieval Failures (no candidates): 0
2025-06-16 10:43:04,962 - INFO - Selection Failures (LLM error): 0
2025-06-16 10:43:04,962 - INFO - ---------------------------
2025-06-16 10:43:04,962 - INFO - Valid attempts for selector: 948
2025-06-16 10:43:04,962 - INFO - Correct selections (Hits): 600
2025-06-16 10:43:04,962 - INFO - Accuracy: 0.6329 (600/948)
2025-06-16 10:43:04,962 - INFO - Saving 348 incorrect selections to /Users/jandrole/projects/onto_rag/evaluation_results.json
2025-06-16 10:43:04,975 - INFO - Evaluation finished.
--- END File: results.txt ---

--- File: .gitignore ---
# Byte-compiled / cache
__pycache__/
*.py[cod]
.pytest_cache/

# Environment & secrets
.env
api_key.json

# VSCode, macOS
.vscode/
.DS_Store

# Large generated data and ontology files
/data/
/ontologies/
/models/
--- END File: .gitignore ---

--- File: .env.example ---
# OpenAI & vector DB credentials
OPENAI_API_KEY=your_openai_api_key_here
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_ENV=your_pinecone_env

--- END File: .env.example ---

--- File: tools/robot ---
#!/bin/sh

## Check for Cygwin, use grep for a case-insensitive search
IS_CYGWIN="FALSE"
if uname | grep -iq cygwin; then
    IS_CYGWIN="TRUE"
fi

# Variable to hold path to this script
# Start by assuming it was the path invoked.
ROBOT_SCRIPT="$0"

# Handle resolving symlinks to this script.
# Using ls instead of readlink, because bsd and gnu flavors
# have different behavior.
while [ -h "$ROBOT_SCRIPT" ] ; do
  ls=`ls -ld "$ROBOT_SCRIPT"`
  # Drop everything prior to ->
  link=`expr "$ls" : '.*-> \(.*\)$'`
  if expr "$link" : '/.*' > /dev/null; then
    ROBOT_SCRIPT="$link"
  else
    ROBOT_SCRIPT=`dirname "$ROBOT_SCRIPT"`/"$link"
  fi
done

# Directory that contains the this script
DIR=$(dirname "$ROBOT_SCRIPT")

if [ $IS_CYGWIN = "TRUE" ]
then
    exec java $ROBOT_JAVA_ARGS -jar "$(cygpath -w $DIR/robot.jar)" "$@"
else
    exec java $ROBOT_JAVA_ARGS -jar "$DIR/robot.jar" "$@"
fi

--- END File: tools/robot ---

--- File: .pytest_cache/CACHEDIR.TAG ---
Signature: 8a477f597d28d172789f06886806bc55
# This file is a cache directory tag created by pytest.
# For information about cache directory tags, see:
#	https://bford.info/cachedir/spec.html

--- END File: .pytest_cache/CACHEDIR.TAG ---

--- File: .pytest_cache/README.md ---
# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.

--- END File: .pytest_cache/README.md ---

--- File: .pytest_cache/.gitignore ---
# Created by pytest automatically.
*

--- END File: .pytest_cache/.gitignore ---

--- File: .pytest_cache/v/cache/nodeids ---
[
  "src/tests/test_lexical_index.py::test_index_exists",
  "src/tests/test_lexical_index.py::test_query_by_definition_keyword",
  "src/tests/test_lexical_index.py::test_query_by_label_exact",
  "src/tests/test_lexical_index.py::test_query_by_label_fuzzy",
  "src/tests/test_lexical_index.py::test_query_by_relations_keyword",
  "src/tests/test_lexical_index.py::test_query_by_synonym",
  "src/tests/test_parse_ontology.py::test_extract_definitions",
  "src/tests/test_parse_ontology.py::test_extract_hierarchy",
  "src/tests/test_parse_ontology.py::test_extract_labels_and_synonyms",
  "src/tests/test_parse_ontology.py::test_extract_relations",
  "src/tests/test_parse_ontology.py::test_load_ontology"
]
--- END File: .pytest_cache/v/cache/nodeids ---

--- File: .pytest_cache/v/cache/lastfailed ---
{
  "src/tests/test_lexical_index.py::test_index_exists": true,
  "src/tests/test_lexical_index.py::test_query_by_label_exact": true,
  "src/tests/test_lexical_index.py::test_query_by_label_fuzzy": true,
  "src/tests/test_lexical_index.py::test_query_by_synonym": true,
  "src/tests/test_lexical_index.py::test_query_by_definition_keyword": true,
  "src/tests/test_lexical_index.py::test_query_by_relations_keyword": true,
  "src/tests/test_parse_ontology.py::test_extract_labels_and_synonyms": true,
  "src/tests/test_parse_ontology.py::test_extract_definitions": true,
  "src/tests/test_parse_ontology.py::test_extract_hierarchy": true,
  "src/tests/test_parse_ontology.py::test_extract_relations": true
}
--- END File: .pytest_cache/v/cache/lastfailed ---

--- File: .pytest_cache/v/cache/stepwise ---
[]
--- END File: .pytest_cache/v/cache/stepwise ---

--- File: .devcontainer/devcontainer.json ---
{
  "name": "Python 3",
  // Or use a Dockerfile or Docker Compose file. More info: https://containers.dev/guide/dockerfile
  "image": "mcr.microsoft.com/devcontainers/python:1-3.11-bullseye",
  "customizations": {
    "codespaces": {
      "openFiles": [
        "README.md",
        "review_app.py"
      ]
    },
    "vscode": {
      "settings": {},
      "extensions": [
        "ms-python.python",
        "ms-python.vscode-pylance"
      ]
    }
  },
  "updateContentCommand": "[ -f packages.txt ] && sudo apt update && sudo apt upgrade -y && sudo xargs apt install -y <packages.txt; [ -f requirements.txt ] && pip3 install --user -r requirements.txt; pip3 install --user streamlit; echo 'âœ… Packages installed and Requirements met'",
  "postAttachCommand": {
    "server": "streamlit run review_app.py --server.enableCORS false --server.enableXsrfProtection false"
  },
  "portsAttributes": {
    "8501": {
      "label": "Application",
      "onAutoForward": "openPreview"
    }
  },
  "forwardPorts": [
    8501
  ]
}
--- END File: .devcontainer/devcontainer.json ---

--- File: scripts/find_namespaces.py ---
# scripts/find_namespaces.py
import rdflib
from rdflib import URIRef
from collections import Counter
import re
import os
import sys

# Add project root to allow importing src.config
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.append(PROJECT_ROOT)
from src.config import FOODON_PATH # Or any other OWL file you want to inspect

def get_base_uri(uri_str):
    """
    Tries to extract a base URI from a full URI.
    e.g., http://purl.obolibrary.org/obo/FOODON_0000123 -> http://purl.obolibrary.org/obo/FOODON_
    e.g., http://www.w3.org/2000/01/rdf-schema#label -> http://www.w3.org/2000/01/rdf-schema#
    """
    if '#' in uri_str:
        return uri_str.rsplit('#', 1)[0] + '#'
    # Common OBO pattern: ends with an underscore followed by digits
    match_obo = re.match(r'(.+[_A-Z]+_)(\d+)$', uri_str)
    if match_obo:
        return match_obo.group(1)
    # General pattern: up to the last slash
    if '/' in uri_str:
        return uri_str.rsplit('/', 1)[0] + '/'
    return None

def main(ontology_path):
    print(f"Loading ontology from: {ontology_path}...")
    g = rdflib.Graph()
    try:
        g.parse(ontology_path) # format will be auto-detected
        print(f"Successfully parsed. Found {len(g)} triples.")
    except Exception as e:
        print(f"Error parsing ontology: {e}")
        return

    uris = set()
    for s, p, o in g:
        if isinstance(s, URIRef):
            uris.add(str(s))
        if isinstance(p, URIRef):
            uris.add(str(p))
        if isinstance(o, URIRef):
            uris.add(str(o))

    print(f"\nFound {len(uris)} unique URIs.")

    base_uri_counts = Counter()
    for uri in uris:
        base = get_base_uri(uri)
        if base:
            base_uri_counts[base] += 1

    print("\nPotential Base URIs (with counts of distinct full URIs using them):")
    # Sort by count descending
    for base, count in base_uri_counts.most_common():
        print(f"- \"{base}\": (used in {count} URIs)")

    print("\n--- rdflib's known namespaces (Prefix: Namespace) ---")
    for prefix, namespace in g.namespaces():
        print(f"  {prefix}: {namespace}")


if __name__ == "__main__":
    # You can make the ontology path an argument if you like
    # For now, using FOODON_PATH from config
    if not os.path.exists(FOODON_PATH):
        print(f"ERROR: FoodON ontology file not found at {FOODON_PATH}")
    else:
        main(FOODON_PATH)
--- END File: scripts/find_namespaces.py ---

--- File: scripts/format_evaluation_results.py ---
# scripts/format_evaluation_results.py

import json
import sys
import argparse
from pathlib import Path
from typing import Dict, Any, List, Optional

# --- Add project root to sys.path to allow imports from src ---
# This assumes the script is in project_root/scripts/
PROJECT_ROOT = Path(__file__).resolve().parents[1]
sys.path.append(str(PROJECT_ROOT))
# ---

try:
    from src import config
except ImportError:
    print("Error: Could not import from 'src'. Make sure the script is run from the project root"
          " or the project structure is correct.")
    # Provide fallback paths if config fails to import
    class config:
        ONTOLOGY_DUMP_JSON = PROJECT_ROOT / "data" / "ontology_dump.json"

def load_json_data(file_path: Path) -> Any:
    """Loads data from a JSON file."""
    if not file_path.exists():
        print(f"Error: File not found at {file_path}")
        sys.exit(1)
    print(f"Loading data from {file_path}...")
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def get_enriched_details(curie: str, ontology_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Retrieves detailed information for a given CURIE from the ontology dump.
    
    Args:
        curie: The CURIE to look up (e.g., "FOODON:03316347").
        ontology_data: The loaded ontology_dump.json data.

    Returns:
        A dictionary with enriched details.
    """
    term_data = ontology_data.get(curie)
    if not term_data:
        return {
            "curie": curie,
            "label": "--- CURIE NOT FOUND ---",
            "definition": "N/A",
            "synonyms": []
        }
    
    return {
        "curie": curie,
        "label": term_data.get("label"),
        "definition": term_data.get("definition"),
        "synonyms": term_data.get("synonyms", [])
    }

def process_evaluation_file(
    eval_results: List[Dict[str, Any]],
    ontology_data: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    Processes the evaluation results to create a human-readable version.
    """
    print("Enriching evaluation results...")
    enriched_output = []

    for item in eval_results:
        query = item.get("query")
        chosen_curie = item.get("chosen_curie")
        true_curies = item.get("true_curies", [])
        candidates_provided = item.get("candidates_provided", [])
        
        # Check if the chosen CURIE is in the list of true CURIEs
        is_correct = chosen_curie in true_curies

        # Enrich all relevant CURIEs
        chosen_details = get_enriched_details(chosen_curie, ontology_data)
        true_details = [get_enriched_details(tc, ontology_data) for tc in true_curies]
        candidates_details = [get_enriched_details(cc, ontology_data) for cc in candidates_provided]

        enriched_item = {
            "query": query,
            "is_correct": is_correct,
            "explanation": item.get("explanation"),
            "chosen_term": chosen_details,
            "ground_truth_terms": true_details,
            "candidate_terms_provided": candidates_details
        }
        enriched_output.append(enriched_item)
        
    return enriched_output

def main():
    """Main function to run the script."""
    parser = argparse.ArgumentParser(
        description="Enrich LLM evaluation results with ontology details for human review.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        "--input-file",
        type=Path,
        default=PROJECT_ROOT / "evaluation_results_gemini_1.5-flash.json",
        help="Path to the raw evaluation results JSON file."
    )
    parser.add_argument(
        "--output-file",
        type=Path,
        default=PROJECT_ROOT / "data" / "readable_evaluation_results.json",
        help="Path to save the enriched, human-readable JSON file."
    )
    parser.add_argument(
        "--ontology-dump",
        type=Path,
        default=Path(config.ONTOLOGY_DUMP_JSON),
        help="Path to the ontology_dump.json file."
    )
    args = parser.parse_args()

    # Create output directory if it doesn't exist
    args.output_file.parent.mkdir(parents=True, exist_ok=True)

    # Load data
    ontology_data = load_json_data(args.ontology_dump)
    eval_results = load_json_data(args.input_file)

    # Process and enrich the data
    readable_results = process_evaluation_file(eval_results, ontology_data)

    # Save the new file
    print(f"Saving enriched results to {args.output_file}...")
    with open(args.output_file, 'w', encoding='utf-8') as f:
        json.dump(readable_results, f, indent=2, ensure_ascii=False)
    
    print("Done!")

if __name__ == "__main__":
    main()
--- END File: scripts/format_evaluation_results.py ---

--- File: scripts/rebuild_base.bash ---
#!/usr/bin/env bash
#
# Rebuild the whole Onto-RAG pipeline from raw ontology â†’ evaluation
# Usage: ./scripts/rebuild_all.sh            # runs with defaults
#        ./scripts/rebuild_all.sh --skip-eval   # skip final recall test
#
set -euo pipefail

ROOT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )/.." && pwd )"
cd "${ROOT_DIR}"

DATA_DIR="data"

echo "ğŸ§¹  Cleaning old artefacts â€¦"
rm -f "${DATA_DIR}"/faiss_index.bin  "${DATA_DIR}"/faiss_metadata.json
rm -rf "${DATA_DIR}/whoosh_index"

echo "â‘   Parsing ontology â€¦"
python -m src.ingestion.parse_ontology

echo "â‘¡  Creating enriched documents â€¦"
python -m src.ingestion.enrich_documents

echo "â‘¢  Embedding documents â€¦"
python -m src.embeddings.embed_docs

echo "â‘£  Building FAISS index â€¦"
python -m src.vector_store.faiss_store

echo "â‘¤  Building Whoosh index â€¦"
python -m src.ingestion.build_lexical_index

if [[ "${1-}" != "--skip-eval" ]]; then
  echo "â‘¥  Running evaluator â€¦"
  python -m src.evaluation.evaluate_retriever_recall
fi

echo "âœ…  Pipeline finished."

--- END File: scripts/rebuild_base.bash ---

--- File: scripts/reduce_evaluation_set.py ---
# scripts/reduce_evaluation_set.py

import xml.etree.ElementTree as ET
import os
import sys
from collections import defaultdict

# Add project root to Python path to allow direct imports if needed in the future
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.append(PROJECT_ROOT)

# --- Configuration ---
# Assumes the data folder is at the project root
INPUT_XML_PATH = os.path.join(PROJECT_ROOT, "data", "CafeteriaFCD_foodon.xml")
OUTPUT_XML_PATH = os.path.join(PROJECT_ROOT, "data", "CafeteriaFCD_foodon_unique.xml")

def create_unique_dataset(input_file: str, output_file: str):
    """
    Parses an XML annotation file and creates a new, smaller XML file
    containing only one instance of each unique (text, semantic_tags) pair.
    """
    if not os.path.exists(input_file):
        print(f"Error: Input file not found at {input_file}")
        return

    try:
        tree = ET.parse(input_file)
        root = tree.getroot()
    except ET.ParseError as e:
        print(f"Error parsing XML file {input_file}: {e}")
        return

    # A set to store the unique identifiers of annotations we've already added.
    # The identifier will be a tuple: (UPPERCASE_TEXT, sorted_tuple_of_tags)
    seen_annotations = set()
    
    # These will be the root and single document of our new XML file
    new_root = ET.Element("collection")
    new_doc = ET.SubElement(new_root, "document")
    new_doc.set("id", "unique_annotations_document")

    total_annotations_processed = 0
    unique_annotations_found = 0

    # Find all annotation tags anywhere in the document
    for annotation_node in root.findall('.//annotation'):
        total_annotations_processed += 1
        
        text_node = annotation_node.find('text')
        tags_node = annotation_node.find('infon[@key="semantic_tags"]')

        # Ensure both text and tags exist to form a valid entry
        if (text_node is not None and text_node.text and
                tags_node is not None and tags_node.text):
            
            # 1. Normalize the text to be case-insensitive
            text = text_node.text.strip().upper()

            # 2. Normalize the tags to be order-insensitive
            raw_tags = tags_node.text.strip()
            # Split by ';', strip whitespace, filter out any empty strings, and sort
            sorted_tags = sorted([tag.strip() for tag in raw_tags.split(';') if tag.strip()])

            # Create a unique, hashable key for this annotation
            # We convert the list of tags to a tuple to make it hashable for the set
            annotation_key = (text, tuple(sorted_tags))

            # 3. If we haven't seen this unique combination before, add it
            if annotation_key not in seen_annotations:
                seen_annotations.add(annotation_key)
                # Append the original annotation_node to our new document
                new_doc.append(annotation_node)
                unique_annotations_found += 1

    print(f"Processed {total_annotations_processed} total annotations.")
    print(f"Found {unique_annotations_found} unique (text, tags) pairs.")

    # Write the new, smaller XML tree to the output file
    new_tree = ET.ElementTree(new_root)
    # The indent function (Python 3.9+) makes the XML output readable
    if sys.version_info >= (3, 9):
        ET.indent(new_tree)
        
    new_tree.write(output_file, encoding='utf-8', xml_declaration=True)
    print(f"Successfully saved unique dataset to: {output_file}")


if __name__ == "__main__":
    create_unique_dataset(INPUT_XML_PATH, OUTPUT_XML_PATH)
--- END File: scripts/reduce_evaluation_set.py ---

--- File: prompts/final_selection_ingredients.tpl ---
You are an expert ontologist specializing in food, ingredient, and chemical substance classification. Your task is to analyze a user-provided entity and select the single most appropriate term from a list of candidate ontology terms, based on a strict set of rules.

You will be given a user entity and a numbered list of candidate terms. Each candidate in the list includes an ID, Label, Definition, and a list of Synonyms.

**User Entity:**
[USER_ENTITY]

**Candidate List:**
[CANDIDATE_LIST]

**Instructions:**
1.  Carefully evaluate the User Entity against the Label, Synonyms, and Definition of each candidate in the Candidate List.
2.  Select the single best match. An exact match between the User Entity and a candidate's Label or one of its Synonyms is the strongest signal for selection.
3.  **Specificity Rule:** If multiple candidates are good matches, you must choose the most specific term over the more general one.
4.  Your response must be a single, valid JSON object only. Do not add any text, explanations, or comments before or after the JSON block.

**Output Format:**
The JSON object you return must contain two keys:
* `"chosen_id"`: The ID of the single best matching term.
* `"explanation"`: A brief justification for your choice. This explanation must clarify why the chosen term is the best fit and, if relevant, why it was chosen over other plausible candidates by applying the specificity rule.

**Example:**
---
**User Entity:**
citric acid

**Candidate List:**
1. ID: FOODON:03301503
   Label: acidulant
   Definition: A food additive which increases the acidity or enhances the sour taste of a food.
   Synonyms: food acid

2. ID: CHEBI:30769
   Label: citric acid
   Definition: A tricarboxylic acid that is propane-1,2,3-tricarboxylic acid bearing a hydroxy substituent at position 2.
   Synonyms: 2-hydroxypropane-1,2,3-tricarboxylic acid

3. ID: FOODON:03301072
   Label: lemon juice
   Definition: The juice obtained from lemons, a common source of citric acid.
   Synonyms: None
---

Your Response:
```json
{
  "chosen_id": "CHEBI:30769",
  "explanation": "I chose 'citric acid' because its label is an exact match for the user entity. While 'acidulant' describes its function, 'citric acid' is the specific chemical entity and therefore the most precise match, adhering to the specificity rule. 'lemon juice' is a product that contains the entity, not the entity itself."
}
--- END File: prompts/final_selection_ingredients.tpl ---

--- File: prompts/reranker.tpl ---

--- END File: prompts/reranker.tpl ---

--- File: prompts/final_selection2.tpl ---
SYSTEM:
You are an expert ontologist specializing in food science.  
Your objective is to map a given *user_entity* to oneâ€”and only oneâ€”candidate term drawn from FoodOn.  
Operate deterministically: set **temperature = 0** (greedy decoding).  
Return **only** a valid JSON object as outputâ€”no prose before or after.

INPUT (exact JSON structure):
{
  "user_entity": "<string>",
  "candidate_terms": [
    {
      "id": "<CURIE>",
      "label": "<string>",
      "definition": "<string>",
      "synonyms": ["<string>", ...]
    }
    // â€¦ additional candidates in the same shape
  ]
}

TASK INSTRUCTIONS
1. Compare *user_entity* against each candidate termâ€™s label, definition, and synonyms.
2. Identify the single best match.  
   â€¢ Exact lexical matches in *label* or *synonyms* are strong signals.  
   â€¢ If multiple candidates are plausible, **prefer the most specific term** over broader ones.
3. Produce a JSON object with exactly two keys:
   â€¢ **"chosen_id"** â€“ the CURIE of the selected term.  
   â€¢ **"explanation"** â€“ a concise, step-by-step justification of why this term outranks the others (implicit chain-of-thought).

OUTPUT FORMAT (nothing else):
{
  "chosen_id": "<CURIE>",
  "explanation": "<brief but complete reasoning>"
}

EDGE-CASE EXAMPLE (for the modelâ€™s patterning only):
Input:
{
  "user_entity": "apple",
  "candidate_terms": [
    {"id":"FOODON:00002403","label":"apple (fruit)","definition":"The edible fruit of Malus domestica.","synonyms":["apple fruit","fresh apple"]},
    {"id":"FOODON:03311015","label":"fruit (plant product)","definition":"A botanical fruit.","synonyms":["plant fruit"]},
    {"id":"FOODON:00002405","label":"apple pie","definition":"A pie made with apples.","synonyms":["apple tart"]}
  ]
}

Expected Output:
{
  "chosen_id": "FOODON:00002403",
  "explanation": "Exact label match with 'apple (fruit)'. While 'fruit (plant product)' matches only generically and 'apple pie' is a derivative product, 'apple (fruit)' is the most specific, conceptually precise fit."
}






You are an expert ontologist specializing in food science.
Respond deterministically (the caller will invoke the model with temperature 0).

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
USER ENTITY
[USER_ENTITY]

CANDIDATE ONTOLOGY TERMS
[As numbered list â€” each item has â€œIDâ€, â€œLabelâ€, â€œDefinitionâ€, â€œSynonymsâ€.]
[CANDIDATE_LIST]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TASK
1. Examine the user entity against each candidateâ€™s **Label, Definition, and Synonyms**.
2. Pick the **single** best-matching term.
   â€¢ Exact lexical matches in Label or Synonyms are strong signals.  
   â€¢ If several terms could work, choose the **most specific** one.
3. Output **only** a valid JSON object â€“ nothing before or after it.

OUTPUT SHAPE
{
  "chosen_id": "<CURIE>",
  "explanation": "<concise step-by-step rationale comparing the chosen term to close alternatives>"
}

ğŸ“Œ Do not wrap the JSON in markdown fences. Do not emit any other text.

EXAMPLE (shows specificity rule; do NOT repeat in your answer)
Input block (abbreviated):
  USER_ENTITY: apple
  CANDIDATE_LIST:
    1. ID: FOODON:00002403 â€¦ Label: apple (fruit) â€¦
    2. ID: FOODON:03311015 â€¦ Label: fruit (plant product) â€¦
    3. ID: FOODON:00002405 â€¦ Label: apple pie â€¦
Expected JSON:
{
  "chosen_id": "FOODON:00002403",
  "explanation": "Exact label match with 'apple (fruit)'. 'Fruit (plant product)' is generic and 'apple pie' is a derivative food; therefore 'apple (fruit)' is the most specific fit."
}

--- END File: prompts/final_selection2.tpl ---

--- File: prompts/final_selection.tpl ---
You are an expert ontologist specializing in food science. Your task is to analyze a list of candidate ontology terms and select the single most appropriate term that matches the user's provided entity.

**User Entity:**
[USER_ENTITY]

**Candidate Ontology Terms:**
[CANDIDATE_LIST]

**Instructions:**
1.  Carefully review the user's entity and each candidate's details (ID, Label, Definition, Synonyms).
2.  Select the single best match. Consider exact matches of labels or synonyms as strong signals. If there are multiple good matches, prefer the more specific term over a general one.
3.  Provide your response in a valid JSON format only. Do not add any text before or after the JSON block.
4.  The JSON object must contain two keys:
    - "chosen_id": The CURIE (ID) of the single best matching term (e.g., "FOODON:00001290").
    - "explanation": A brief, clear explanation for your choice, justifying why it is the best fit compared to other options.

**Example Response Format:**
{
  "chosen_id": "FOODON:00001290",
  "explanation": "I chose 'garlic' because its label is an exact match for the user entity. Candidate 'allium sativum' is the scientific name but 'garlic' is the common term and therefore a better fit."
}
--- END File: prompts/final_selection.tpl ---

--- File: prompts/parse_ontology.tpl ---

--- END File: prompts/parse_ontology.tpl ---

--- File: src/run_on_off.py ---
# scripts/map_ingredients.py
"""
This script loads extracted ingredient entities from a JSON file,
runs them through the RAG pipeline to link them to ontology terms,
and saves the structured results to a new JSON file.
"""

import os
import sys
import json
import logging
from tqdm import tqdm

# --- Add project root to sys.path ---
# This allows the script to import modules from the 'src' directory
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from src.pipeline.gemini_pipeline import RAGPipeline
from src import config

# --- Configuration ---
LOGGING_LEVEL = logging.INFO
# Limit the number of products to process. Set to None to process all.
PRODUCT_LIMIT = 5 

INPUT_FILE = os.path.join(PROJECT_ROOT, 'data', 'outputs', 'parsed_ingredients_output.json')
OUTPUT_FILE = os.path.join(PROJECT_ROOT, 'data', 'outputs', 'mapped_ingredients_output.json')


# --- Setup Logging ---
logging.basicConfig(level=LOGGING_LEVEL, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')
logger = logging.getLogger(__name__)


def main():
    """
    Main function to run the ingredient mapping process.
    """
    logger.info("Starting ingredient to ontology mapping process...")
    
    # Ensure the output directory exists
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

    pipeline = None
    try:
        # --- 1. Load Input Data ---
        logger.info(f"Loading ingredients from: {INPUT_FILE}")
        if not os.path.exists(INPUT_FILE):
            logger.error(f"Input file not found: {INPUT_FILE}")
            logger.error("Please ensure you have run the parsing script first.")
            return

        with open(INPUT_FILE, 'r', encoding='utf-8') as f:
            all_ingredients_data = json.load(f)

        # --- 2. Initialize RAG Pipeline ---
        logger.info("Initializing RAG pipeline...")
        pipeline = RAGPipeline()
        logger.info("RAG pipeline initialized successfully.")

        # --- 3. Process Ingredients ---
        all_mappings = {}
        
        # Get items to process, applying the limit if specified
        items_to_process = list(all_ingredients_data.items())
        if PRODUCT_LIMIT is not None:
            items_to_process = items_to_process[:PRODUCT_LIMIT]
            logger.warning(f"Processing a limited set of {PRODUCT_LIMIT} products.")

        # Use tqdm for a progress bar
        for product_id, ingredients in tqdm(items_to_process, desc="Processing Products"):
            logger.info(f"--- Processing Product ID: {product_id} ---")
            
            product_mappings = []
            unique_ingredients = sorted(list(set(ingredients))) # Process unique ingredients to avoid duplicate work

            for ingredient_query in unique_ingredients:
                logger.info(f"Querying for: '{ingredient_query}'")
                
                # Run the pipeline for the ingredient string
                mapping_result = pipeline.run(query=ingredient_query)
                
                if mapping_result:
                    logger.info(f"  -> Found mapping: '{mapping_result.get('label')}' (ID: {mapping_result.get('id')})")
                else:
                    logger.warning(f"  -> No mapping found for '{ingredient_query}'")
                
                # Store the result in a structured format
                product_mappings.append({
                    "original_ingredient": ingredient_query,
                    "mapping_result": mapping_result if mapping_result else "No mapping found"
                })
            
            all_mappings[product_id] = product_mappings

        # --- 4. Save Results ---
        logger.info(f"Saving mapped results to: {OUTPUT_FILE}")
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(all_mappings, f, indent=4)
            
        logger.info("Mapping process completed successfully!")

    except Exception as e:
        logger.error(f"An unexpected error occurred during the mapping process: {e}", exc_info=True)
    finally:
        # --- 5. Clean up ---
        if pipeline:
            logger.info("Closing pipeline resources.")
            pipeline.close()

if __name__ == "__main__":
    main()
--- END File: src/run_on_off.py ---

--- File: src/config.py ---
# src/config.py
import os
from dotenv import load_dotenv
from rdflib import Namespace # This is fine, though rdflib.Namespace is not directly used for string constants below.

# Project Root Directory
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "."))
# print(f"Project root directory: {PROJECT_ROOT}") # Keep for debugging if you like

# Load environment variables from .env file
load_dotenv(os.path.join(PROJECT_ROOT, ".env"))

# Data Directory (for ontology dump, indexes, etc.)
DATA_DIR = os.path.join(PROJECT_ROOT, "data")
os.makedirs(DATA_DIR, exist_ok=True)

# Ontologies Directory
ONTOLOGIES_DIR = os.path.join(PROJECT_ROOT, "ontologies")
FOODON_PATH = os.path.join(ONTOLOGIES_DIR, "foodon-with-syns.owl")
TEST_FOODON_SNIPPET_PATH = os.path.join(ONTOLOGIES_DIR, "test_foodon_snippet.owl")


# Output file from parse_ontology.py
ONTOLOGY_DUMP_JSON = os.path.join(DATA_DIR, "ontology_syns_dump.json")

# Output file for enriched documents
ENRICHED_DOCUMENTS_FILE = os.path.join(DATA_DIR, "enriched_documents.json")

# Output file for embeddings (used for building FAISS index)
EMBEDDINGS_FILE = os.path.join(DATA_DIR, "embeddings.json")

# Whoosh Index Directory
WHOOSH_INDEX_DIR = os.path.join(DATA_DIR, "whoosh_index")
os.makedirs(WHOOSH_INDEX_DIR, exist_ok=True)

# FAISS Index Paths
FAISS_INDEX_PATH = os.path.join(DATA_DIR, "faiss_index.bin")
FAISS_METADATA_PATH = os.path.join(DATA_DIR, "faiss_metadata.json")

# Embedding Model Configuration
# EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
EMBEDDING_MODEL_NAME = "Lajavaness/bilingual-embedding-large" 
# RERANKER_MODEL_NAME = "intfloat/e5-mistral-7b-instruct"
RERANKER_MODEL_NAME = "cross-encoder/ms-marco-MiniLM-L-6-v2"
EMBEDDING_BATCH_SIZE = 32 # For batch embedding script
EMBEDDING_DEVICE = 'cpu'  # or 'cuda' if available, for embedding script

# Default K values for retrieval
DEFAULT_K_LEXICAL = 20
DEFAULT_K_VECTOR = 20
DEFAULT_RERANK_K = DEFAULT_K_LEXICAL + DEFAULT_K_VECTOR

# Namespaces (using string constants for broader compatibility if rdflib not always imported)
RDFS_NS_STR = "http://www.w3.org/2000/01/rdf-schema#"
RDF_NS_STR = "http://www.w3.org/1999/02/22-rdf-syntax-ns#"
OWL_NS_STR = "http://www.w3.org/2002/07/owl#"
SKOS_NS_STR = "http://www.w3.org/2004/02/skos/core#"
OBO_NS_STR = "http://purl.obolibrary.org/obo/"
OBOINOWL_NS_STR = "http://www.geneontology.org/formats/oboInOwl#"
IAO_NS_STR = "http://purl.obolibrary.org/obo/IAO_"

# For rdflib usage where Namespace objects are preferred:
RDFS_NS = Namespace(RDFS_NS_STR)
RDF_NS = Namespace(RDF_NS_STR)
OWL_NS = Namespace(OWL_NS_STR)
SKOS_NS = Namespace(SKOS_NS_STR)
OBO_NS = Namespace(OBO_NS_STR)
OBOINOWL_NS = Namespace(OBOINOWL_NS_STR)
IAO_NS = Namespace(IAO_NS_STR)


# Mapping of common relation URIs/CURIEs to human-readable names and default prefixes
# Used by parse_ontology.py and potentially enrich_documents.py
RELATION_CONFIG = {
    "obo:BFO_0000050": {"label": "part of", "prefix": "obo"},
    "obo:RO_0001000": {"label": "derives from", "prefix": "obo"},
    "obo:RO_0002200": {"label": "has phenotype", "prefix": "obo"},
    "obo:RO_0002451": {"label": "has part", "prefix": "obo"},
    "obo:FOODON_0000246": {"label": "has ingredient", "prefix": "obo"},
    "obo:RO_0000056": {"label": "participates in", "prefix": "obo"},
    "obo:RO_0000057": {"label": "has participant", "prefix": "obo"},
    "obo:RO_0000085": {"label": "functionally related to", "prefix": "obo"},
    "obo:RO_0002090": {"label": "adjacent to", "prefix": "obo"},
    "obo:RO_0002131": {"label": "overlaps", "prefix": "obo"},
    "obo:RO_0002150": {"label": "connected to", "prefix": "obo"},
    "obo:RO_0002202": {"label": "develops from", "prefix": "obo"},
    "obo:RO_0002350": {"label": "member of", "prefix": "obo"},
    "obo:RO_0002351": {"label": "has member", "prefix": "obo"},
    "obo:RO_0002353": {"label": "output of", "prefix": "obo"},
    "obo:RO_0002440": {"label": "causally related to", "prefix": "obo"},
    "obo:RO_0002450": {"label": "contains", "prefix": "obo"},
    "obo:RO_0002500": {"label": "surrounds", "prefix": "obo"},
    "obo:RO_0002573": {"label": "has quality", "prefix": "obo"},
    "obo:RO_0002606": {"label": "is transformation of", "prefix": "obo"},
    "obo:RO_0002607": {"label": "has input", "prefix": "obo"},
    "obo:RO_0003000": {"label": "used in", "prefix": "obo"},
    "obo:FOODON_0000380": {"label": "has preparation method", "prefix": "obo"},
    "obo:FOODON_0000226": {"label": "has food source", "prefix": "obo"},
    "obo:FOODON_0000388": {"label": "has maturity state", "prefix": "obo"},
    "obo:FOODON_0000286": {"label": "has packaging", "prefix": "obo"},
    "obo:FOODON_0000240": {"label": "has preservation method", "prefix": "obo"},
    "obo:FOODON_0000440": {"label": "has physical state", "prefix": "obo"},
    # "obo:FOODON_": {"label": "FoodON specific relation", "prefix": "obo"}, # This generic one might be too broad
    "obo:ERO_0000039": {"label": "has nutrient", "prefix": "obo"},
    "obo:ERO_0000589": {"label": "dietary context of", "prefix": "obo"},
    "obo:NCIT_C25277": {"label": "is allergen of", "prefix": "obo"},
    "obo:NCIT_C48670": {"label": "has active ingredient", "prefix": "obo"},
    "obo:NCIT_C64548": {"label": "part of human diet", "prefix": "obo"},
    "obo:PATO_0000001": {"label": "has quality PATO", "prefix": "obo"},
}

TARGET_RELATIONS_CURIES = list(RELATION_CONFIG.keys())

# CURIE Prefix Map for uri_to_curie and curie_to_uri conversions
# Ensure the keys are the *base URIs* that prefixes are expected for.
CURIE_PREFIX_MAP = {
    "http://purl.obolibrary.org/obo/FOODON_": "FOODON",
    "http://purl.obolibrary.org/obo/BFO_": "BFO",
    "http://purl.obolibrary.org/obo/RO_": "RO",
    "http://purl.obolibrary.org/obo/ERO_": "ERO",
    "http://purl.obolibrary.org/obo/NCIT_": "NCIT",
    "http://purl.obolibrary.org/obo/PATO_": "PATO",
    "http://purl.obolibrary.org/obo/IAO_": "IAO",
    "http://www.w3.org/2000/01/rdf-schema#": "rdfs",
    "http://www.w3.org/1999/02/22-rdf-syntax-ns#": "rdf",
    "http://www.w3.org/2002/07/owl#": "owl",
    "http://www.w3.org/2004/02/skos/core#": "skos",
    "http://www.geneontology.org/formats/oboInOwl#": "oboInOwl",
    "http://purl.obolibrary.org/obo/HANCESTRO_": "HANCESTRO",  
    "http://purl.obolibrary.org/obo/GAZ_": "GAZ",
    "http://purl.obolibrary.org/obo/CHEBI_": "CHEBI",
    "http://purl.obolibrary.org/obo/NCBITaxon_": "NCBITaxon",
    "http://purl.obolibrary.org/obo/UBERON_": "UBERON",
    "http://purl.obolibrary.org/obo/ENVO_": "ENVO",
    "http://purl.obolibrary.org/obo/HP_": "HP",
    "http://purl.obolibrary.org/obo/GO_": "GO",
    # General OBO prefix - should be last or handled carefully to avoid overly broad matches
    # if specific OBO sub-ontologies are listed above.
    "http://purl.obolibrary.org/obo/": "obo",
}

# LLM API Key (placeholders)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
LLM_SELECTOR_MODEL_NAME = "gemini-1.5-flash-latest"
OLLAMA_SELECTOR_MODEL_NAME = 'llama3.1:8b'
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Logging configuration
LOG_LEVEL = "INFO"
LOG_FILE = os.path.join(PROJECT_ROOT, "app.log") # Example log file in project root
--- END File: src/config.py ---

--- File: src/__init__.py ---

--- END File: src/__init__.py ---

--- File: src/main.py ---
# src/main.py
import argparse
import json
import os
import sys

# --- Add project root to sys.path ---
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from src.pipeline.ollama_pipeline import RAGPipeline
from src.config import DEFAULT_K_LEXICAL, DEFAULT_K_VECTOR

def main():
    parser = argparse.ArgumentParser(description="Run the Onto-RAG pipeline with LLM selection.")
    parser.add_argument("query", type=str, help="The entity or text to search for (e.g., 'raw milk').")
    parser.add_argument("--lexical_k", type=int, default=DEFAULT_K_LEXICAL, help=f"Initial candidates from lexical search (default: {DEFAULT_K_LEXICAL}).")
    parser.add_argument("--vector_k", type=int, default=DEFAULT_K_VECTOR, help=f"Initial candidates from vector search (default: {DEFAULT_K_VECTOR}).")
    parser.add_argument("--top_n_rerank", type=int, default=10, help="Number of candidates to rerank and pass to the LLM (default: 10).")
    args = parser.parse_args()

    pipeline = None
    try:
        pipeline = RAGPipeline()
        final_result = pipeline.run(
            query=args.query,
            lexical_k=args.lexical_k,
            vector_k=args.vector_k,
            rerank_top_n=args.top_n_rerank
        )

        print("\n--- Final LLM Selection ---")
        if not final_result:
            print("Could not determine a matching ontology term.")
        else:
            print(f"Query: '{args.query}'")
            print("---------------------------")
            print(f"Chosen Term ID: {final_result.get('id', 'N/A')}")
            print(f"Label:          {final_result.get('label', 'N/A')}")
            print(f"Definition:     {final_result.get('definition', 'N/A')}")
            print(f"Synonyms:       {'; '.join(final_result.get('synonyms', [])) or 'None'}")
            print("\nLLM Explanation:")
            print(f"  > {final_result.get('explanation', 'No explanation provided.')}")
        print("---------------------------\n")

    except Exception as e:
        print(f"\nAn error occurred during the pipeline execution: {e}", file=sys.stderr)
    finally:
        if pipeline:
            pipeline.close()

if __name__ == "__main__":
    main()
--- END File: src/main.py ---

--- File: src/reranker/__init__.py ---
# src/reranker/__init__.py
# This file makes Python treat the directory 'reranker' as a package.

from .llm_reranker import LLMReranker

__all__ = ["LLMReranker"]
--- END File: src/reranker/__init__.py ---

--- File: src/reranker/llm_reranker.py ---
# src/reranker/llm_reranker.py

import json
import logging
from typing import List, Dict, Optional, Tuple, Any

from sentence_transformers import CrossEncoder
import torch

# Configure logging for the module
logger = logging.getLogger(__name__)
# Basic configuration for the logger if no handlers are configured by the calling application
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')


class LLMReranker:
    """
    Reranks documents using a CrossEncoder model.
    It takes a query and a list of candidate documents, computes a relevance score
    for each query-document pair, and returns the documents sorted by these scores.
    """

    def __init__(self,
                 model_name: str,
                 enriched_docs_path: str,
                 device: Optional[str] = None):
        """
        Initializes the LLMReranker.

        Args:
            model_name (str): The name of the CrossEncoder model to use
                              (e.g., "cross-encoder/ms-marco-MiniLM-L-6-v2").
            enriched_docs_path (str): Path to the JSON file containing enriched documents,
                                      expected to be a list of dicts with "id" and "text" keys.
            device (Optional[str]): The device to run the model on ("cuda", "cpu").
                                    If None, tries to use CUDA, otherwise CPU.
        """
        self.model_name = model_name
        self.enriched_docs_path = enriched_docs_path

        if device:
            self.device = device
        elif torch.cuda.is_available():
            self.device = "cuda"
            logger.info("CUDA available. Using CUDA for reranker.")
        else:
            self.device = "cpu"
            logger.info("CUDA not available. Using CPU for reranker.")
        
        try:
            self.model = CrossEncoder(self.model_name, device=self.device, trust_remote_code=True) # Added trust_remote_code for models like e5
            logger.info(f"Successfully loaded CrossEncoder model: {self.model_name} on {self.device}")
            model_max_length = self.model.tokenizer.model_max_length
            if model_max_length > 10000: # often indicates not properly set, e.g. 1e30
                model_max_length = 512 # Fallback to a common default
                logger.warning(f"Model tokenizer returned a very large max_length. Using fallback: {model_max_length}")
            self.model.max_length = model_max_length
            logger.info(f"Set CrossEncoder max_length to: {self.model.max_length}")

        except Exception as e:
            logger.error(f"Failed to load CrossEncoder model {self.model_name}: {e}")
            if "intfloat/e5-mistral-7b-instruct" in self.model_name:
                 logger.warning(
                    f"Note: '{self.model_name}' is primarily an encoder model. "
                    "Using it directly with `CrossEncoder` might not yield optimal "
                    "results unless it's a variant fine-tuned for sequence-pair classification "
                    "or requires `trust_remote_code=True`. "
                    "Consider a bi-encoder approach (separate embeddings + cosine similarity) "
                    "or a prompt-based LLM reranking for this model type if standard CrossEncoder fails."
                )
            raise

        self.doc_texts = self._load_enriched_documents()
        if not self.doc_texts:
            logger.error(f"Enriched documents could not be loaded from {self.enriched_docs_path}. Reranker might not function correctly.")


    def _load_enriched_documents(self) -> Dict[str, str]:
        """
        Loads enriched documents and creates a mapping from document ID to text.

        Returns:
            Dict[str, str]: A dictionary mapping document CURIEs to their enriched text.
        """
        try:
            with open(self.enriched_docs_path, 'r', encoding='utf-8') as f:
                enriched_data = json.load(f)
            
            doc_map = {}
            for item in enriched_data:
                if "id" in item and "text" in item:
                    doc_map[item["id"]] = item["text"]
                else:
                    logger.warning(f"Skipping item due to missing 'id' or 'text' in {self.enriched_docs_path}: {item}")
            logger.info(f"Loaded {len(doc_map)} enriched documents for reranking from {self.enriched_docs_path}.")
            return doc_map
        except FileNotFoundError:
            logger.error(f"Enriched documents file not found: {self.enriched_docs_path}")
            return {}
        except json.JSONDecodeError:
            logger.error(f"Error decoding JSON from {self.enriched_docs_path}")
            return {}
        except Exception as e:
            logger.error(f"An unexpected error occurred while loading enriched documents: {e}")
            return {}

    def _get_document_text_for_reranking(self, doc_id: str) -> Optional[str]:
        """
        Retrieves the pre-loaded enriched text for a given document ID.

        Args:
            doc_id (str): The CURIE of the document.

        Returns:
            Optional[str]: The enriched text of the document, or None if not found.
        """
        return self.doc_texts.get(doc_id)

    def rerank(self, query: str, documents: List[Dict[str, Any]], top_n: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        Reranks a list of documents based on their relevance to a query.

        Args:
            query (str): The search query.
            documents (List[Dict[str, Any]]): A list of document dictionaries retrieved
                                              from a previous stage (e.g., HybridRetriever).
                                              Each dict must contain an 'id' key.
            top_n (Optional[int]): The maximum number of documents to return after reranking.
                                   If None, all reranked documents are returned.

        Returns:
            List[Dict[str, Any]]: The list of documents, sorted by reranked scores,
                                  with a new 'rerank_score' key added to each document.
                                  Documents for which text could not be found are excluded.
        """
        if not query:
            logger.warning("Rerank called with an empty query. Returning original documents.")
            return documents[:top_n] if top_n is not None else documents
        
        if not documents:
            logger.info("Rerank called with no documents. Returning empty list.")
            return []

        if not self.doc_texts:
            logger.error("No enriched document texts loaded. Cannot perform reranking. Returning original documents.")
            return documents[:top_n] if top_n is not None else documents

        sentence_pairs: List[Tuple[str, str]] = []
        valid_documents_for_reranking: List[Dict[str, Any]] = []

        for doc in documents:
            doc_id = doc.get("id")
            if not doc_id:
                logger.warning(f"Document missing 'id' field, cannot rerank: {doc}")
                continue

            doc_text = self._get_document_text_for_reranking(doc_id)
            if doc_text:
                sentence_pairs.append((query, doc_text))
                valid_documents_for_reranking.append(doc)
            else:
                logger.warning(f"Could not find enriched text for document ID '{doc_id}'. Skipping for reranking.")
        
        if not sentence_pairs:
            logger.warning("No valid documents found to create sentence pairs for reranking. Returning original documents that were passed in.")
            return documents[:top_n] if top_n is not None else documents

        logger.info(f"Reranking {len(sentence_pairs)} document(s) for query: '{query}'")
        
        try:
            scores = self.model.predict(sentence_pairs, 
                                        show_progress_bar=False, # Set to True for verbose progress
                                        batch_size=32) # Adjust batch_size based on VRAM
        except Exception as e:
            logger.error(f"Error during CrossEncoder prediction: {e}")
            # Fallback: return original documents without reranking scores
            return documents[:top_n] if top_n is not None else documents

        # Add scores to documents and sort
        for i, doc in enumerate(valid_documents_for_reranking):
            doc["rerank_score"] = float(scores[i]) # Ensure score is float

        # Sort documents by rerank_score in descending order
        reranked_documents = sorted(valid_documents_for_reranking, key=lambda x: x.get("rerank_score", -float('inf')), reverse=True)

        if top_n is not None:
            reranked_documents = reranked_documents[:top_n]
            logger.info(f"Returning top {len(reranked_documents)} reranked documents.")
        else:
            logger.info(f"Returning all {len(reranked_documents)} reranked documents.")
            
        return reranked_documents


if __name__ == '__main__':
    import sys
    from pathlib import Path
    import random # For dummy retriever scores

    # --- Path Setup & Config Import ---
    # Add project root to sys.path to allow imports like src.config
    # Assumes this script is in onto_rag/src/reranker/
    PROJECT_ROOT_FOR_MAIN = Path(__file__).resolve().parent.parent.parent
    if str(PROJECT_ROOT_FOR_MAIN) not in sys.path:
        sys.path.append(str(PROJECT_ROOT_FOR_MAIN))

    try:
        from src.config import (
            ENRICHED_DOCUMENTS_FILE,
            RERANKER_MODEL_NAME,
            EMBEDDING_DEVICE,
            DEFAULT_RERANK_K,
            LOG_LEVEL, # Optional: if you want to use config's log level
            LOG_FILE   # Optional: if you want to use config's log file
        )
    except ImportError as e:
        print(f"Error importing from src.config: {e}")
        print("Please ensure that src/config.py exists and the script is run from the project root,")
        print("or that the PROJECT_ROOT_FOR_MAIN path is correctly set for your structure.")
        sys.exit(1)

    # --- Basic Logging Setup for the Example ---
    # Uses the main logger configured at the top of the file.
    # You can customize this further if needed, e.g., by setting level from config.LOG_LEVEL
    logger.setLevel(LOG_LEVEL if 'LOG_LEVEL' in locals() else "INFO")
    # Example of adding a file handler if LOG_FILE is configured:
    # if 'LOG_FILE' in locals() and LOG_FILE:
    #     fh = logging.FileHandler(LOG_FILE)
    #     fh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    #     logger.addHandler(fh)
    #     logging.getLogger().addHandler(fh) # also add to root logger if basicConfig was called

    logger.info("--- Running LLMReranker Example with Real Data ---")
    logger.info(f"Using ENRICHED_DOCUMENTS_FILE: {ENRICHED_DOCUMENTS_FILE}")
    logger.info(f"Using RERANKER_MODEL_NAME: {RERANKER_MODEL_NAME}")
    logger.info(f"Using EMBEDDING_DEVICE: {EMBEDDING_DEVICE}")
    logger.info(f"Using DEFAULT_RERANK_K: {DEFAULT_RERANK_K}")


    # --- Load Sample Documents from Real Enriched Data ---
    sample_documents_from_retriever: List[Dict[str, Any]] = []
    try:
        with open(ENRICHED_DOCUMENTS_FILE, 'r', encoding='utf-8') as f:
            all_enriched_docs = json.load(f)
        
        if not all_enriched_docs:
            logger.error(f"No documents found in {ENRICHED_DOCUMENTS_FILE}. Cannot proceed with the example.")
            sys.exit(1)

        # Select a few documents to simulate retriever output
        # Take up to 5, or fewer if the file has less
        num_docs_to_sample = min(len(all_enriched_docs), 5)
        if num_docs_to_sample == 0 :
             logger.error(f"The file {ENRICHED_DOCUMENTS_FILE} is empty. Cannot create sample documents.")
             sys.exit(1)
             
        for i in range(num_docs_to_sample):
            doc = all_enriched_docs[i]
            sample_documents_from_retriever.append({
                "id": doc.get("id"),
                "label": doc.get("label", "N/A"), # Get label if available
                "retriever_score": random.uniform(0.5, 1.0) # Dummy score
            })
        logger.info(f"Loaded {len(sample_documents_from_retriever)} sample documents for reranking.")

    except FileNotFoundError:
        logger.error(f"ERROR: The enriched documents file was not found: {ENRICHED_DOCUMENTS_FILE}")
        logger.error("Please ensure you have run the data ingestion and enrichment scripts first (e.g., `src.ingestion.enrich_documents.py`).")
        sys.exit(1)
    except json.JSONDecodeError:
        logger.error(f"ERROR: Could not decode JSON from {ENRICHED_DOCUMENTS_FILE}. The file might be corrupted.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"An unexpected error occurred while loading sample documents: {e}", exc_info=True)
        sys.exit(1)

    if not sample_documents_from_retriever:
        logger.error("No sample documents could be prepared. Exiting example.")
        sys.exit(1)
        
    # --- Reranker Initialization and Usage ---
    try:
        reranker = LLMReranker(
            model_name=RERANKER_MODEL_NAME,
            enriched_docs_path=str(ENRICHED_DOCUMENTS_FILE), # Ensure path is a string
            device=EMBEDDING_DEVICE
        )

        query = "Garlic" # Generic example, adjust to your data

        logger.info(f"\nOriginal sample documents (simulated retriever output) for query '{query}':")
        for doc in sample_documents_from_retriever:
            logger.info(f"  ID: {doc.get('id')}, Label: {doc.get('label')}, Retriever Score: {doc.get('retriever_score', 0.0):.4f}")

        reranked_results = reranker.rerank(query, sample_documents_from_retriever, top_n=DEFAULT_RERANK_K)

        logger.info(f"\nReranked documents (top {DEFAULT_RERANK_K} for query '{query}'):")
        if reranked_results:
            for doc in reranked_results:
                logger.info(f"  ID: {doc.get('id')}, Label: {doc.get('label')}, Rerank Score: {doc.get('rerank_score', 'N/A'):.4f}, Original Retriever Score: {doc.get('retriever_score', 'N/A')}")
        else:
            logger.info("  No results after reranking.")
        
        # Example of how you might switch to "intfloat/e5-mistral-7b-instruct"
        # Ensure it's set in your .env or config.py as RERANKER_MODEL_NAME
        # and that you have the resources for it.
        if RERANKER_MODEL_NAME != "intfloat/e5-mistral-7b-instruct" and False: # Set to True to test e5
            logger.info("\n--- Conceptual Test: Switching to intfloat/e5-mistral-7b-instruct ---")
            logger.warning("This is a large model and may require significant resources and download time.")
            logger.warning("Ensure 'intfloat/e5-mistral-7b-instruct' is configured as RERANKER_MODEL_NAME and 'trust_remote_code=True' might be needed.")
            
            try:
                e5_model_name = "intfloat/e5-mistral-7b-instruct"
                e5_reranker = LLMReranker(
                    model_name=e5_model_name,
                    enriched_docs_path=str(ENRICHED_DOCUMENTS_FILE),
                    device=EMBEDDING_DEVICE # "cuda" recommended for this model
                )
                e5_reranked_results = e5_reranker.rerank(query, sample_documents_from_retriever, top_n=DEFAULT_RERANK_K)
                logger.info(f"\nReranked documents with {e5_model_name} (top {DEFAULT_RERANK_K} for query '{query}'):")
                if e5_reranked_results:
                    for doc in e5_reranked_results:
                        logger.info(f"  ID: {doc.get('id')}, Label: {doc.get('label')}, Rerank Score: {doc.get('rerank_score', 'N/A'):.4f}")
                else:
                    logger.info("  No results after reranking with E5.")
            except Exception as e_e5:
                logger.error(f"Could not initialize or use {e5_model_name} reranker: {e_e5}", exc_info=True)
                logger.warning(f"Skipping {e5_model_name} reranking part of the example.")


    except Exception as e:
        logger.error(f"An error occurred in the main example: {e}", exc_info=True)

    logger.info("--- LLMReranker Example Finished ---")
--- END File: src/reranker/llm_reranker.py ---

--- File: src/pipeline/ollama_pipeline.py ---
# src/pipeline/pipeline.py
import os
import sys
import logging
from typing import List, Dict, Any, Optional

# --- Add project root to sys.path ---
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from src.retriever.hybrid_retriever import HybridRetriever
from src.reranker.llm_reranker import LLMReranker
from src.rag_selectors.ollama_selector import OllamaSelector
from src import config

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')
logger = logging.getLogger(__name__)

class RAGPipeline:
    def __init__(self):
        logger.info("Initializing RAG Pipeline...")
        try:
            self.retriever = HybridRetriever()
            self.reranker = LLMReranker(
                model_name=config.RERANKER_MODEL_NAME,
                enriched_docs_path=config.ENRICHED_DOCUMENTS_FILE,
                device=config.EMBEDDING_DEVICE
            )
            self.selector = OllamaSelector(retriever=self.retriever) # <--- UPDATED INSTANTIATION
            logger.info("RAG Pipeline initialized successfully.")
        except (FileNotFoundError, ValueError) as e:
            logger.error(f"Failed to initialize pipeline: {e}")
            logger.error("Please run 'scripts/rebuild_base.bash' and ensure GEMINI_API_KEY is set in your .env file.")
            raise
        except Exception as e:
            logger.error(f"An unexpected error occurred during pipeline initialization: {e}", exc_info=True)
            raise

    def run(self, 
            query: str, 
            lexical_k: int = config.DEFAULT_K_LEXICAL, 
            vector_k: int = config.DEFAULT_K_VECTOR, 
            rerank_top_n: int = 10
            ) -> Optional[Dict[str, Any]]:
        """
        Executes the full pipeline for a given query.

        Returns:
            A dictionary of the selected term with its details and the LLM's explanation, or None.
        """
        logger.info(f"Running pipeline for query: '{query}'")

        # 1. Retrieve
        retriever_output = self.retriever.search(query, lexical_limit=lexical_k, vector_k=vector_k)
        lexical_results = retriever_output.get("lexical_results", [])
        vector_results = retriever_output.get("vector_results", [])
        
        # 2. Merge
        combined_candidates = []
        seen_ids = set()
        for doc in lexical_results + vector_results:
            doc_id = doc.get('id')
            if doc_id and doc_id not in seen_ids:
                combined_candidates.append(doc)
                seen_ids.add(doc_id)
        
        if not combined_candidates:
            logger.warning("No candidates found.")
            return None

        # 3. Rerank
        reranked_candidates = self.reranker.rerank(query, combined_candidates, top_n=rerank_top_n)

        if not reranked_candidates:
            logger.warning("No candidates left after reranking.")
            return None
        
        logger.info(f"Top {len(reranked_candidates)} candidates after reranking passed to LLM selector.")

        # 4. Select with LLM
        selection = self.selector.select_best_term(query, reranked_candidates)

        if not selection:
            logger.error("LLM selection failed. Returning the top reranked result as a fallback.")
            top_fallback = reranked_candidates[0]
            chosen_term_details = self.retriever.get_term_details(top_fallback['id'])
            chosen_term_details['explanation'] = "FALLBACK: LLM selection failed. This is the top result from the reranker."
            return chosen_term_details

        # 5. Get final details and return
        chosen_id = selection['chosen_id']
        chosen_term_details = self.retriever.get_term_details(chosen_id)
        if not chosen_term_details:
            logger.error(f"LLM chose ID '{chosen_id}', but its details could not be retrieved.")
            return None
        
        chosen_term_details['explanation'] = selection['explanation']
        return chosen_term_details

    def close(self):
        if hasattr(self.retriever, 'close'):
            self.retriever.close()
        logger.info("Pipeline resources closed.")
--- END File: src/pipeline/ollama_pipeline.py ---

--- File: src/pipeline/gemini_pipeline.py ---
# src/pipeline/pipeline.py
import os
import sys
import logging
from typing import List, Dict, Any, Optional

# --- Add project root to sys.path ---
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from src.retriever.hybrid_retriever import HybridRetriever
from src.reranker.llm_reranker import LLMReranker
from src.rag_selectors.gemini_selector import GeminiSelector
from src import config

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')
logger = logging.getLogger(__name__)

class RAGPipeline:
    def __init__(self):
        logger.info("Initializing RAG Pipeline...")
        try:
            self.retriever = HybridRetriever()
            self.reranker = LLMReranker(
                model_name=config.RERANKER_MODEL_NAME,
                enriched_docs_path=config.ENRICHED_DOCUMENTS_FILE,
                device=config.EMBEDDING_DEVICE
            )
            self.selector = GeminiSelector(retriever=self.retriever) # <--- UPDATED INSTANTIATION
            logger.info("RAG Pipeline initialized successfully.")
        except (FileNotFoundError, ValueError) as e:
            logger.error(f"Failed to initialize pipeline: {e}")
            logger.error("Please run 'scripts/rebuild_base.bash' and ensure GEMINI_API_KEY is set in your .env file.")
            raise
        except Exception as e:
            logger.error(f"An unexpected error occurred during pipeline initialization: {e}", exc_info=True)
            raise

    def run(self, 
            query: str, 
            lexical_k: int = config.DEFAULT_K_LEXICAL, 
            vector_k: int = config.DEFAULT_K_VECTOR, 
            rerank_top_n: int = 10
            ) -> Optional[Dict[str, Any]]:
        """
        Executes the full pipeline for a given query.

        Returns:
            A dictionary of the selected term with its details and the LLM's explanation, or None.
        """
        logger.info(f"Running pipeline for query: '{query}'")

        # 1. Retrieve
        retriever_output = self.retriever.search(query, lexical_limit=lexical_k, vector_k=vector_k)
        lexical_results = retriever_output.get("lexical_results", [])
        vector_results = retriever_output.get("vector_results", [])
        
        # 2. Merge
        combined_candidates = []
        seen_ids = set()
        for doc in lexical_results + vector_results:
            doc_id = doc.get('id')
            if doc_id and doc_id not in seen_ids:
                combined_candidates.append(doc)
                seen_ids.add(doc_id)
        
        if not combined_candidates:
            logger.warning("No candidates found.")
            return None

        # 3. Rerank
        reranked_candidates = self.reranker.rerank(query, combined_candidates, top_n=rerank_top_n)

        if not reranked_candidates:
            logger.warning("No candidates left after reranking.")
            return None
        
        logger.info(f"Top {len(reranked_candidates)} candidates after reranking passed to LLM selector.")

        # 4. Select with LLM
        selection = self.selector.select_best_term(query, reranked_candidates)

        if not selection:
            logger.error("LLM selection failed. Returning the top reranked result as a fallback.")
            top_fallback = reranked_candidates[0]
            chosen_term_details = self.retriever.get_term_details(top_fallback['id'])
            chosen_term_details['explanation'] = "FALLBACK: LLM selection failed. This is the top result from the reranker."
            return chosen_term_details

        # 5. Get final details and return
        chosen_id = selection['chosen_id']
        chosen_term_details = self.retriever.get_term_details(chosen_id)
        logger.debug(f"Chosen term details: {chosen_term_details}")
        if not chosen_term_details:
            logger.error(f"LLM chose ID '{chosen_id}', but its details could not be retrieved.")
            return None
        
        chosen_term_details['explanation'] = selection['explanation']
        return chosen_term_details

    def close(self):
        if hasattr(self.retriever, 'close'):
            self.retriever.close()
        logger.info("Pipeline resources closed.")
--- END File: src/pipeline/gemini_pipeline.py ---

--- File: src/ingestion/build_lexical_index.py ---
# src/ingestion/build_lexical_index.py
import sys
import os

# --- Add project root to sys.path ---
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
# --- End sys.path modification ---

import json
from whoosh.index import create_in, open_dir
from whoosh.fields import Schema, ID, TEXT
from whoosh.qparser import QueryParser
import traceback

# Now import using the 'src' package prefix
from src.config import ONTOLOGY_DUMP_JSON, WHOOSH_INDEX_DIR

# ... (rest of the build_lexical_index.py code, ensuring imports like `from src.config` are used)
def build_index(json_path: str, index_dir: str):
    print(f"Building lexical index from {json_path} into {index_dir}")

    schema = Schema(
        curie=ID(stored=True, unique=True),
        label=TEXT(stored=True, analyzer=None), # Keep None analyzer for exact matches if needed, or use default
        synonyms=TEXT(stored=True),
        definition=TEXT(stored=True),
        relations_text=TEXT(stored=False)
    )

    os.makedirs(index_dir, exist_ok=True)
    
    try:
        # create_in will overwrite if index exists. If you want to open, use open_dir.
        ix = create_in(index_dir, schema)
        print(f"Whoosh index schema created in {index_dir}")
    except Exception as e:
         print(f"Error creating Whoosh index directory or schema: {e}")
         traceback.print_exc()
         return

    writer = ix.writer()
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            ontology_data = json.load(f)

        print(f"Indexing {len(ontology_data)} entities...")
        indexed_count = 0
        for curie, data in ontology_data.items():
            label = data.get('label', '') or '' # Ensure string
            synonyms_list = data.get('synonyms', [])
            synonyms = " ".join(s for s in synonyms_list if s) # Join non-empty synonyms

            definition = data.get('definition', '') or ''

            relations_text = ""
            relations_dict = data.get('relations', {})
            if relations_dict:
                 relations_text = " ".join([
                     f"{prop.replace('_', ' ')} {' '.join(targets)}" for prop, targets in relations_dict.items()
                 ])
            
            # Skip adding document if all text fields are empty (optional)
            # if not label and not synonyms and not definition and not relations_text:
            #     print(f"Skipping empty document for CURIE: {curie}")
            #     continue

            writer.add_document(
                curie=curie,
                label=label,
                synonyms=synonyms,
                definition=definition,
                relations_text=relations_text
            )
            indexed_count += 1
            if indexed_count % 1000 == 0:
                 print(f"Indexed {indexed_count} entities...")
        
        print(f"Committing index with {indexed_count} documents.")
        writer.commit()
        print("Lexical index built successfully.")

    except FileNotFoundError:
        print(f"Error: Ontology dump file not found at {json_path}")
        traceback.print_exc()
        writer.cancel() 
    except Exception as e:
        print(f"An error occurred during index building: {e}")
        traceback.print_exc()
        writer.cancel()

if __name__ == "__main__":
    if not os.path.exists(ONTOLOGY_DUMP_JSON):
        print(f"Error: Ontology dump file {ONTOLOGY_DUMP_JSON} not found.")
        print("Please run src/ingestion/parse_ontology.py first.")
    else:
        build_index(ONTOLOGY_DUMP_JSON, WHOOSH_INDEX_DIR)
--- END File: src/ingestion/build_lexical_index.py ---

--- File: src/ingestion/__init__.py ---

--- END File: src/ingestion/__init__.py ---

--- File: src/ingestion/parse_ontology.py ---
# src/ingestion/parse_ontology.py
import sys
import os
import logging # Import logging

# --- Add project root to sys.path ---
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
# --- End sys.path modification ---

import rdflib
from rdflib import Graph, Namespace, URIRef, RDFS, OWL, RDF
from typing import Dict, List, Any
import json
import traceback

# Now import using the 'src' package prefix
from src.config import (
    FOODON_PATH,                # Adjusted: Was ONTOLOGY_FILE
    ONTOLOGY_DUMP_JSON,         # Adjusted: Was ONTOLOGY_DUMP_PATH
    CURIE_PREFIX_MAP,           # Adjusted: Was NAMESPACE_MAP
    RELATION_CONFIG,            # New: For relation names
    TARGET_RELATIONS_CURIES,    # New: For relation URIs
    IAO_NS_STR,                 # Using string constants for Namespace definitions
    OBOINOWL_NS_STR,
    # Add other NS_STR if needed for local Namespace objects
)
# Assuming src.utils.ontology_utils is already correct and uses CURIE_PREFIX_MAP
from src.utils.ontology_utils import uri_to_curie, curie_to_uri

# --- Logging Setup ---
# Using basicConfig as src.utils.logging.get_logger is "to be developed"
# You can customize this further if needed.
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s')
logger = logging.getLogger(__name__)


# Define commonly used namespaces (can still use these locally for convenience)
# It's safer to use the full URI string from config if available, or construct it.
IAO = Namespace(IAO_NS_STR)
OBOINOWL = Namespace(OBOINOWL_NS_STR)
# FOODON_BASE_URI = None # Find FOODON base URI from CURIE_PREFIX_MAP
# for base, prefix in CURIE_PREFIX_MAP.items():
#     if prefix == "FOODON":
#         FOODON_BASE_URI = base
#         break
# FOODON = Namespace(FOODON_BASE_URI) if FOODON_BASE_URI else None
# If FOODON Namespace object is not strictly needed for queries, direct URI construction is fine.

def load_ontology(path: str) -> rdflib.Graph:
    g = Graph()
    try:
        logger.info(f"Loading ontology from: {path}")
        try:
            g.parse(path, format="application/rdf+xml")
        except Exception as e_xml:
            logger.warning(f"Failed to parse as RDF/XML: {e_xml}. Trying Turtle...")
            try:
                g.parse(path, format="turtle")
            except Exception as e_ttl:
                logger.warning(f"Failed to parse as Turtle: {e_ttl}. Trying auto-detection...")
                g.parse(path)

        logger.info(f"Ontology loaded successfully. Contains {len(g)} triples.")
        return g
    except FileNotFoundError:
        logger.error(f"Error: Ontology file not found at {path}")
        raise
    except Exception as e:
        logger.error(f"Error parsing ontology file {path}: {e}")
        traceback.print_exc()
        raise

def get_ancestors(g: Graph, term_uri: URIRef, prefix_map: Dict[str, str], visited_uris: set = None) -> List[str]:
    if visited_uris is None:
        visited_uris = set()

    ancestor_curies = set()
    for parent_uri in g.objects(subject=term_uri, predicate=RDFS.subClassOf):
        if isinstance(parent_uri, URIRef) and parent_uri != OWL.Thing:
            if parent_uri not in visited_uris:
                visited_uris.add(parent_uri)
                # Pass the prefix_map explicitly
                parent_curie = uri_to_curie(parent_uri, prefix_map)
                if parent_curie and parent_curie != str(parent_uri):
                    ancestor_curies.add(parent_curie)
                    ancestor_curies.update(get_ancestors(g, parent_uri, prefix_map, visited_uris))
    return list(ancestor_curies)


def extract_labels_and_synonyms(g: Graph, prefix_map: Dict[str, str]) -> Dict[str, Dict[str, Any]]:
    data = {}
    relevant_predicates = [
        RDFS.label,
        OBOINOWL.hasExactSynonym, OBOINOWL.hasRelatedSynonym,
        OBOINOWL.hasNarrowSynonym, OBOINOWL.hasBroadSynonym
    ]
    
    processed_subjects = set()

    for pred in relevant_predicates:
        for s_uri in g.subjects(predicate=pred):
            if not isinstance(s_uri, URIRef) or s_uri in processed_subjects:
                continue
            
            # Pass the prefix_map explicitly
            curie = uri_to_curie(s_uri, prefix_map)
            # Optional: Filter for specific prefixes if desired, e.g., only FOODON, IAO, OBO terms.
            # This was present in your original script; keeping it commented for now for broader extraction.
            # if not any(curie.startswith(p) for p in ["FOODON:", "IAO:", "RO:", "BFO:", "obo:", "CHEBI:"]): # Example
            #     # logger.debug(f"Skipping non-ontology CURIE for labels/syns: {curie} from {s_uri}")
            #     continue
            if not curie or curie == str(s_uri): # Skip if not converted to a CURIE effectively
                continue

            if curie not in data:
                data[curie] = {"label": None, "synonyms": []}

            # Label
            label_val = g.value(subject=s_uri, predicate=RDFS.label)
            if label_val and isinstance(label_val, rdflib.Literal):
                data[curie]["label"] = str(label_val)

            # Synonyms
            current_synonyms = []
            for syn_prop in [OBOINOWL.hasExactSynonym, OBOINOWL.hasRelatedSynonym,
                             OBOINOWL.hasNarrowSynonym, OBOINOWL.hasBroadSynonym]:
                for syn_obj in g.objects(subject=s_uri, predicate=syn_prop):
                    if isinstance(syn_obj, rdflib.Literal):
                        current_synonyms.append(str(syn_obj))
            
            # Ensure synonyms list exists and extend uniquely
            if "synonyms" not in data[curie] or data[curie]["synonyms"] is None: # Handle if somehow becomes None
                data[curie]["synonyms"] = []
            for s in current_synonyms:
                if s not in data[curie]["synonyms"]:
                    data[curie]["synonyms"].append(s)

            processed_subjects.add(s_uri)
            
    final_data = {k: v for k, v in data.items() if v.get("label") or v.get("synonyms")}
    logger.info(f"Extracted labels and synonyms for {len(final_data)} terms.")
    return final_data


def extract_definitions(g: Graph, prefix_map: Dict[str, str]) -> Dict[str, str]:
    definitions = {}
    definition_prop_uri = IAO['0000115'] # IAO:0000115 is 'definition'
    for s_uri in g.subjects(predicate=definition_prop_uri):
        if not isinstance(s_uri, URIRef):
            continue
        
        # Pass the prefix_map explicitly
        curie = uri_to_curie(s_uri, prefix_map)
        # Optional: Filter for specific prefixes
        # if not any(curie.startswith(p) for p in ["FOODON:", "IAO:", "RO:", "BFO:", "obo:", "CHEBI:"]):
        #     # logger.debug(f"Skipping non-ontology CURIE for definitions: {curie} from {s_uri}")
        #     continue
        if not curie or curie == str(s_uri):
            continue
        
        def_obj = g.value(subject=s_uri, predicate=definition_prop_uri)
        if def_obj and isinstance(def_obj, rdflib.Literal):
            definitions[curie] = str(def_obj)
            
    logger.info(f"Extracted definitions for {len(definitions)} terms.")
    return definitions

def extract_hierarchy(g: Graph, prefix_map: Dict[str, str]) -> Dict[str, Dict[str, List[str]]]:
    hierarchy_data = {}
    all_terms_in_hierarchy = set()
    for s, p, o in g.triples((None, RDFS.subClassOf, None)):
        if isinstance(s, URIRef): all_terms_in_hierarchy.add(s)
        if isinstance(o, URIRef): all_terms_in_hierarchy.add(o)
    
    for term_uri in all_terms_in_hierarchy:
        if term_uri == OWL.Thing:
            continue

        # Pass the prefix_map explicitly
        curie = uri_to_curie(term_uri, prefix_map)
        # Optional: Filter
        # if not any(curie.startswith(p) for p in ["FOODON:", "IAO:", "RO:", "BFO:", "obo:", "CHEBI:"]):
        #     # logger.debug(f"Skipping non-ontology CURIE for hierarchy: {curie} from {term_uri}")
        #     continue
        if not curie or curie == str(term_uri):
            continue

        direct_parent_curies = []
        for parent_uri in g.objects(subject=term_uri, predicate=RDFS.subClassOf):
            if isinstance(parent_uri, URIRef) and parent_uri != OWL.Thing:
                # Pass the prefix_map explicitly
                parent_curie = uri_to_curie(parent_uri, prefix_map)
                if parent_curie and parent_curie != str(parent_uri):
                    direct_parent_curies.append(parent_curie)
        
        ancestor_curies = get_ancestors(g, term_uri, prefix_map, visited_uris=set())
        
        if direct_parent_curies or ancestor_curies:
            hierarchy_data[curie] = {
                "parents": list(set(direct_parent_curies)),
                "ancestors": list(set(ancestor_curies))
            }
            
    logger.info(f"Extracted hierarchy data for {len(hierarchy_data)} terms.")
    return hierarchy_data


# Adjusted to match outline: props_to_extract: Dict[str, str] (readable_name -> URI_string)
def extract_relations(g: Graph, props_to_extract: Dict[str, str], prefix_map: Dict[str, str]) -> Dict[str, Dict[str, List[str]]]:
    relations_data = {}
    
    for term_uri in g.subjects(unique=True):
        if not isinstance(term_uri, URIRef):
            continue

        # Pass the prefix_map explicitly
        curie = uri_to_curie(term_uri, prefix_map)
        # Optional: Filter
        # if not any(curie.startswith(p) for p in ["FOODON:", "IAO:", "RO:", "BFO:", "obo:", "CHEBI:"]):
        #     # logger.debug(f"Skipping non-ontology CURIE for relations: {curie} from {term_uri}")
        #     continue
        if not curie or curie == str(term_uri):
            continue

        term_specific_relations = {}
        for rel_readable_name, rel_uri_str in props_to_extract.items(): # rel_name is readable_name
            rel_uri = URIRef(rel_uri_str)
            target_curies = []
            for target_obj in g.objects(subject=term_uri, predicate=rel_uri):
                if isinstance(target_obj, URIRef):
                    # Pass the prefix_map explicitly
                    target_curie = uri_to_curie(target_obj, prefix_map)
                    if target_curie and target_curie != str(target_obj):
                        target_curies.append(target_curie)
            
            if target_curies:
                term_specific_relations[rel_readable_name] = list(set(target_curies))

        if term_specific_relations:
            relations_data[curie] = term_specific_relations
            
    logger.info(f"Extracted relations for {len(relations_data)} terms based on {len(props_to_extract)} specified properties.")
    return relations_data


def main():
    logger.info("Starting ontology parsing...")
    # Adjusted: Use ONTOLOGY_DUMP_JSON for output path
    data_dir = os.path.dirname(ONTOLOGY_DUMP_JSON)
    os.makedirs(data_dir, exist_ok=True)

    try:
        # Adjusted: Use FOODON_PATH as input ontology file
        g = load_ontology(FOODON_PATH)

        # Prepare relation properties for extract_relations
        # extract_relations expects: Dict[readable_name, full_uri_string]
        # RELATION_CONFIG has: {curie_str: {"label": readable_name, "prefix": prefix_str}}
        # TARGET_RELATIONS_CURIES is List[curie_str]
        
        relation_properties_for_extraction = {}
        for rel_curie_str in TARGET_RELATIONS_CURIES:
            if rel_curie_str in RELATION_CONFIG:
                config_entry = RELATION_CONFIG[rel_curie_str]
                readable_name = config_entry.get("label", rel_curie_str) # Fallback to CURIE if no label
                
                # Convert relation CURIE to full URI using curie_to_uri and CURIE_PREFIX_MAP
                full_rel_uri = curie_to_uri(rel_curie_str, CURIE_PREFIX_MAP)
                if full_rel_uri:
                    relation_properties_for_extraction[readable_name] = str(full_rel_uri)
                else:
                    logger.warning(f"Could not convert relation CURIE {rel_curie_str} to URI. Skipping this relation.")
            else:
                logger.warning(f"Relation CURIE {rel_curie_str} from TARGET_RELATIONS_CURIES not found in RELATION_CONFIG.")
        
        logger.info(f"Prepared {len(relation_properties_for_extraction)} relation properties for extraction.")


        logger.info("\nExtracting data...")
        # Pass CURIE_PREFIX_MAP to all extraction functions
        labels_synonyms = extract_labels_and_synonyms(g, CURIE_PREFIX_MAP)
        definitions = extract_definitions(g, CURIE_PREFIX_MAP)
        hierarchy = extract_hierarchy(g, CURIE_PREFIX_MAP)
        relations = extract_relations(g, relation_properties_for_extraction, CURIE_PREFIX_MAP)

        logger.info("\nMerging extracted data...")
        merged_data = {}
        all_curies = set(labels_synonyms.keys()) | \
                     set(definitions.keys()) | \
                     set(hierarchy.keys()) | \
                     set(relations.keys())

        for curie_key in all_curies:
            # No need to filter again here if extraction functions already did or if we want all extracted CURIEs
            merged_data[curie_key] = {
                "label": labels_synonyms.get(curie_key, {}).get("label"),
                "synonyms": labels_synonyms.get(curie_key, {}).get("synonyms", []),
                "definition": definitions.get(curie_key),
                "parents": hierarchy.get(curie_key, {}).get("parents", []),
                "ancestors": hierarchy.get(curie_key, {}).get("ancestors", []),
                "relations": relations.get(curie_key, {})
            }
        
        final_merged_data = {}
        for curie_key, data_dict in merged_data.items():
            if any(data_dict.values()): # Check if any value in the dict is non-empty/non-None
                final_merged_data[curie_key] = data_dict

        logger.info(f"\nTotal merged entities with some data: {len(final_merged_data)}")

        # Adjusted: Use ONTOLOGY_DUMP_JSON for output
        logger.info(f"Writing merged data to {ONTOLOGY_DUMP_JSON}")
        with open(ONTOLOGY_DUMP_JSON, 'w', encoding='utf-8') as f:
            json.dump(final_merged_data, f, indent=4, ensure_ascii=False)

        logger.info("Ontology parsing and data dump complete.")

    except FileNotFoundError:
        # Adjusted: Use FOODON_PATH in error message
        logger.error(f"Parsing aborted: Ontology file not found at {FOODON_PATH}")
        # traceback.print_exc() # Already handled by load_ontology
    except Exception as e:
        logger.error(f"An error occurred during parsing: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()
--- END File: src/ingestion/parse_ontology.py ---

--- File: src/ingestion/enrich_documents.py ---
import json
import os
import logging
from typing import Dict, Any, List, Optional

# --- Start of corrected import block ---
import sys
# Calculate the project root directory based on the script's location
# For .../onto_rag/src/ingestion/enrich_documents.py, _PROJECT_ROOT becomes .../onto_rag
_PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))

# Add the project root to sys.path if it's not already there
if _PROJECT_ROOT not in sys.path:
    sys.path.insert(0, _PROJECT_ROOT) # Insert at the beginning

try:
    from src import config
    # from src.utils.ontology_utils import curie_to_uri # This specific utility is not used in this script,
                                                      # but this is how you'd import it if needed.
except ModuleNotFoundError as e:
    print(f"CRITICAL ERROR: Could not import project modules. Exception: {e}")
    print(f"This script expects to be run in a way that the 'src' package is discoverable.")
    print(f"Attempted to add project root '{_PROJECT_ROOT}' to sys.path.")
    print(f"Current sys.path: {sys.path}")
    print("Please ensure you are running this script from the project's root directory ('onto_rag/'), for example:")
    print("  python src/ingestion/enrich_documents.py")
    print("Also ensure that 'src/__init__.py' and 'src/utils/__init__.py' (if using utils) exist.")
    sys.exit(1)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def get_label_for_curie(curie: str, ontology_data: Dict[str, Dict[str, Any]], default_value: Optional[str] = None) -> Optional[str]:
    """
    Retrieves the label for a given CURIE from the ontology data.
    Args:
        curie: The CURIE string (e.g., "FOODON:00001234").
        ontology_data: The loaded ontology data dictionary.
        default_value: Value to return if CURIE not found or has no label. Defaults to the CURIE itself.
    Returns:
        The label string, or the default_value.
    """
    if default_value is None:
        default_value = curie # Fallback to CURIE if no specific default

    term_info = ontology_data.get(curie)
    if term_info and term_info.get("label"):
        return term_info["label"]
    return default_value

def get_relation_name(relation_curie: str) -> str:
    """
    Gets a human-readable name for a relation CURIE using RELATION_CONFIG.
    """
    # Exact match
    if relation_curie in config.RELATION_CONFIG:
        return config.RELATION_CONFIG[relation_curie]["label"]
    
    # Check for generic FoodON prefix if specific one not found
    generic_foodon_prefix = "obo:FOODON_"
    if relation_curie.startswith(generic_foodon_prefix) and generic_foodon_prefix in config.RELATION_CONFIG:
         # Attempt to make it slightly more readable if it's like "obo:FOODON_0000XXXX"
        relation_suffix = relation_curie.split('_')[-1]
        return f"FoodON relation {relation_suffix}" # or config.RELATION_CONFIG[generic_foodon_prefix]["label"]

    # Fallback for other OBO relations
    if relation_curie.startswith("obo:"):
        name_part = relation_curie.split(':')[-1].replace("_", " ")
        return name_part

    return relation_curie # Fallback to the CURIE itself

def create_enriched_documents(ontology_data_path: str, output_path: str) -> List[Dict[str, Any]]:
    """
    Creates enriched text documents for each ontology entry.
    Args:
        ontology_data_path: Path to the ontology_dump.json file.
        output_path: Path to save the enriched_documents.json file.
    Returns:
        A list of enriched document dictionaries.
    """
    logging.info(f"Loading ontology data from {ontology_data_path}...")
    try:
        with open(ontology_data_path, 'r', encoding='utf-8') as f:
            ontology_data = json.load(f)
    except FileNotFoundError:
        logging.error(f"Error: Ontology data file not found at {ontology_data_path}")
        return []
    except json.JSONDecodeError:
        logging.error(f"Error: Could not decode JSON from {ontology_data_path}")
        return []

    logging.info(f"Successfully loaded {len(ontology_data)} terms.")

    enriched_docs = []

    for term_curie, term_data in ontology_data.items():
        doc_parts = []

        # 1. Label
        label = term_data.get("label")
        if not label:
            logging.warning(f"Term {term_curie} has no label. Skipping for enriched document (or using CURIE as label).")
            # Consider if we want to enrich docs for terms without labels. For now, let's use CURIE if no label.
            label = term_curie 
        
        doc_parts.append(f"{label}.")

        # 2. Definition
        definition = term_data.get("definition")
        if definition:
            doc_parts.append(f"{definition}.")

        # 3. Synonyms
        synonyms = term_data.get("synonyms")
        if synonyms:
            synonyms_text = "; ".join(synonyms)
            doc_parts.append(f"Also known as: {synonyms_text}.")

        # 4. Parents (direct subclasses)
        parent_curies = term_data.get("parents", [])
        if parent_curies:
            parent_labels = [get_label_for_curie(p_curie, ontology_data) for p_curie in parent_curies]
            parent_labels_filtered = [l for l in parent_labels if l] # Filter out None if get_label_for_curie returns None
            if parent_labels_filtered:
                if len(parent_labels_filtered) == 1:
                    doc_parts.append(f"Is a type of: {parent_labels_filtered[0]}.")
                else:
                    doc_parts.append(f"Is a type of: {'; '.join(parent_labels_filtered)}.")
        
        # 5. Relations (including facets expressed as object properties)
        relations = term_data.get("relations", {})
        relation_texts = []
        for rel_curie, target_curies_list in relations.items():
            rel_name = get_relation_name(rel_curie)
            target_labels = [get_label_for_curie(t_curie, ontology_data) for t_curie in target_curies_list]
            target_labels_filtered = [l for l in target_labels if l]
            if target_labels_filtered:
                relation_texts.append(f"{rel_name}: {', '.join(target_labels_filtered)}")
        
        if relation_texts:
            doc_parts.append("Key characteristics include: " + "; ".join(relation_texts) + ".")

        # Combine all parts into a single text
        enriched_text = " ".join(doc_parts).replace("..", ".").strip() # Clean up potential double periods

        enriched_docs.append({
            "id": term_curie,
            "label": label if label != term_curie else term_data.get("label", term_curie), # Store original label if available
            "text": enriched_text
        })

    logging.info(f"Created {len(enriched_docs)} enriched documents.")

    logging.info(f"Saving enriched documents to {output_path}...")
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(enriched_docs, f, indent=2)
        logging.info("Successfully saved enriched documents.")
    except IOError:
        logging.error(f"Error: Could not write enriched documents to {output_path}")

    return enriched_docs

def main():
    """Main function to create enriched documents."""
    # Ensure data directory exists (though config.py should handle it)
    os.makedirs(config.DATA_DIR, exist_ok=True)

    enriched_documents = create_enriched_documents(
        ontology_data_path=config.ONTOLOGY_DUMP_JSON,
        output_path=config.ENRICHED_DOCUMENTS_FILE
    )

    if enriched_documents:
        logging.info(f"Processed {len(enriched_documents)} documents.")
        # Optionally print a sample
        if len(enriched_documents) > 0:
            logging.info("Sample enriched document:")
            logging.info(json.dumps(enriched_documents[0], indent=2))
    else:
        logging.warning("No enriched documents were created.")

if __name__ == "__main__":
    main()
--- END File: src/ingestion/enrich_documents.py ---

--- File: src/embeddings/embed_docs.py ---
import json
import os
import logging
import time
from typing import List, Dict, Any

# Ensure src is in path for imports if run directly
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))) # To import from src

try:
    from sentence_transformers import SentenceTransformer
    import torch 
except ModuleNotFoundError:
    logging.error("Modules sentence_transformers or torch not found. Please install them: pip install sentence-transformers torch")
    sys.exit(1)

try:
    from src import config # Use 'from src import config'
except ModuleNotFoundError:
    # Fallback for running script directly from src/embeddings where src might not be seen as a package root
    # This is less ideal but can help during direct script execution
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    try:
        import config
    except ModuleNotFoundError:
        logging.error("Error: Could not import 'config'. "
                    "Ensure the script is run from the project root or 'src' is in PYTHONPATH.")
        sys.exit(1)


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def load_enriched_documents(file_path: str) -> List[Dict[str, Any]]:
    """Loads enriched documents from a JSON file."""
    logging.info(f"Loading enriched documents from {file_path}...")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            documents = json.load(f)
        logging.info(f"Successfully loaded {len(documents)} documents.")
        return documents
    except FileNotFoundError:
        logging.error(f"Error: Enriched documents file not found at {file_path}")
        return []
    except json.JSONDecodeError:
        logging.error(f"Error: Could not decode JSON from {file_path}")
        return []

def batch_embed_documents(
    documents: List[Dict[str, Any]], 
    model_name: str, 
    batch_size: int = 32,
    device: str = None
) -> List[Dict[str, Any]]:
    """
    Generates embeddings for a list of documents using a SentenceTransformer model.

    Args:
        documents: A list of dictionaries, each with at least "id", "label", and "text" keys.
        model_name: The name of the SentenceTransformer model to use.
        batch_size: The number of documents to process in each batch.
        device: The device to use for computation (e.g., "cuda", "cpu"). Auto-detects if None.

    Returns:
        A list of dictionaries, each containing "id", "label", and "embedding" (list of floats).
    """
    if not documents:
        logging.warning("No documents provided for embedding.")
        return []

    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    logging.info(f"Using device: {device}")

    logging.info(f"Loading SentenceTransformer model: {model_name}")
    try:
        model = SentenceTransformer(model_name, device=device, trust_remote_code=True)
    except Exception as e:
        logging.error(f"Failed to load model {model_name}: {e}")
        # Fallback to CPU if CUDA error during model loading (e.g. out of memory)
        if "cuda" in str(e).lower() and device == "cuda":
            logging.warning("CUDA error during model load. Attempting to load on CPU.")
            device = "cpu"
            model = SentenceTransformer(model_name, device=device, trust_remote_code=True)
        else:
            raise

    logging.info("Model loaded. Starting embedding process...")

    texts_to_embed = [doc.get("text", "") for doc in documents] # Get text, default to empty string if missing
    ids = [doc.get("id") for doc in documents]
    labels = [doc.get("label") for doc in documents]

    all_embeddings_data = []
    start_time = time.time()

    for i in range(0, len(texts_to_embed), batch_size):
        batch_texts = texts_to_embed[i:i+batch_size]
        batch_ids = ids[i:i+batch_size]
        batch_labels = labels[i:i+batch_size]

        logging.info(f"Processing batch {i//batch_size + 1}/{(len(texts_to_embed) -1)//batch_size + 1} (size: {len(batch_texts)})")
        
        try:
            embeddings = model.encode(batch_texts, convert_to_tensor=False, show_progress_bar=False) # Returns numpy array
            
            for idx, embedding in enumerate(embeddings):
                all_embeddings_data.append({
                    "id": batch_ids[idx],
                    "label": batch_labels[idx],
                    "embedding": embedding.tolist() # Convert numpy array to list for JSON serialization
                })
        except Exception as e:
            logging.error(f"Error embedding batch starting at index {i}: {e}")
            # Optionally, decide how to handle batch errors (e.g., skip batch, add placeholders)
            # For now, we'll just log and continue, resulting in missing embeddings for that batch.
            # To add placeholders:
            # for j in range(len(batch_texts)):
            #     all_embeddings_data.append({
            #         "id": batch_ids[j],
            #         "label": batch_labels[j],
            #         "embedding": None # Or a zero vector of correct dimensionality
            #     })


    end_time = time.time()
    logging.info(f"Embedding process completed for {len(all_embeddings_data)} documents in {end_time - start_time:.2f} seconds.")
    
    if hasattr(model, 'tokenizer') and hasattr(model.tokenizer, 'model_max_length'):
        logging.info(f"Model max sequence length: {model.tokenizer.model_max_length}")
    elif hasattr(model, 'max_seq_length'):
         logging.info(f"Model max sequence length: {model.max_seq_length}")


    return all_embeddings_data

def save_embeddings(embeddings_data: List[Dict[str, Any]], output_path: str):
    """Saves the embeddings data to a JSON file."""
    if not embeddings_data:
        logging.warning("No embeddings data to save.")
        return

    logging.info(f"Saving {len(embeddings_data)} embeddings to {output_path}...")
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(embeddings_data, f, indent=2) # Use indent for readability, can remove for smaller file size
        logging.info("Successfully saved embeddings.")
    except IOError:
        logging.error(f"Error: Could not write embeddings to {output_path}")
    except TypeError as e:
        logging.error(f"TypeError during JSON serialization: {e}. Check embedding data format.")


def main():
    """Main function to generate and save document embeddings."""
    # Ensure data directory exists
    os.makedirs(config.DATA_DIR, exist_ok=True)

    enriched_docs = load_enriched_documents(config.ENRICHED_DOCUMENTS_FILE)
    if not enriched_docs:
        logging.error("Failed to load enriched documents. Exiting.")
        return

    embeddings_data = batch_embed_documents(
        documents=enriched_docs,
        model_name=config.EMBEDDING_MODEL_NAME,
        batch_size=64 # Adjust batch size based on available VRAM/RAM
    )

    if embeddings_data:
        save_embeddings(embeddings_data, config.EMBEDDINGS_FILE)
        logging.info(f"Generated and saved {len(embeddings_data)} embeddings.")
        if embeddings_data:
            sample_embedding = embeddings_data[0]['embedding']
            logging.info(f"Sample embedding vector dimension: {len(sample_embedding) if sample_embedding else 'N/A'}")
            logging.info(f"Sample embedding data point: {json.dumps(embeddings_data[0], indent=2, default=lambda x: str(x)[:100])}") # Truncate long embedding
    else:
        logging.warning("No embeddings were generated.")

if __name__ == "__main__":
    main()
--- END File: src/embeddings/embed_docs.py ---

--- File: src/tests/test_lexical_index.py ---
import pytest
import os
import json
import tempfile
import shutil
from whoosh.index import open_dir
from whoosh.qparser import QueryParser

# Adjust path to import modules from src
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from ingestion.parse_ontology import main as parse_ontology_main # Need to run parsing first
from ingestion.build_lexical_index import build_index
import config # Need config to override paths

# Define a known CURIE and search terms from the test snippet
TEST_CURIE = "FOODON:00001100" # apple
TEST_LABEL = "apple"
TEST_SYNONYM = "eating apple"
TEST_DEFINITION_KEYWORD = "fruit" # Keyword from definition "The fruit of a Malus domestica tree."
TEST_RELATION_KEYWORD = "apple tree" # Keyword from relations_text (target label)


# Fixture to create a temporary directory for the test index
@pytest.fixture(scope="module")
def temp_index_dir():
    """Creates a temporary directory for the Whoosh index."""
    path = tempfile.mkdtemp(prefix="whoosh_test_index_")
    print(f"Created temp index dir: {path}")
    yield path
    print(f"Cleaning up temp index dir: {path}")
    shutil.rmtree(path)

# Fixture to run the full ingestion pipeline on the test snippet and build index
@pytest.fixture(scope="module")
def built_test_index(temp_index_dir):
    """Runs parse_ontology and build_index on the test snippet."""
    # Use temp files/dirs for test data and index
    with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as tmp_json:
        test_json_path = tmp_json.name

    # Temporarily override config paths for the test
    original_ontology_file = config.ONTOLOGY_FILE
    original_dump_path = config.ONTOLOGY_DUMP_PATH
    original_index_dir = config.WHOOSH_INDEX_DIR

    config.ONTOLOGY_FILE = config.TEST_ONTOLOGY_FILE # Use the small test snippet
    config.ONTOLOGY_DUMP_PATH = test_json_path
    config.WHOOSH_INDEX_DIR = temp_index_dir

    try:
        # 1. Run parsing
        print("\nRunning parse_ontology_main for test index build...")
        parse_ontology_main()
        assert os.path.exists(test_json_path), "Ontology dump JSON not created!"

        # 2. Run index building
        print("\nRunning build_index for test index build...")
        build_index(test_json_path, temp_index_dir)
        assert os.path.exists(os.path.join(temp_index_dir, 'SCHEMA')), "Whoosh index not created!"

        # Yield the index directory path
        yield temp_index_dir

    finally:
        # Restore original paths
        config.ONTOLOGY_FILE = original_ontology_file
        config.ONTOLOGY_DUMP_PATH = original_dump_path
        config.WHOOSH_INDEX_DIR = original_index_dir
        # Clean up temporary JSON file
        if os.path.exists(test_json_path):
            os.unlink(test_json_path)


def test_index_exists(built_test_index):
    """Tests if the index directory was created and contains index files."""
    assert os.path.exists(built_test_index)
    assert os.path.exists(os.path.join(built_test_index, 'SCHEMA'))
    assert os.path.exists(os.path.join(built_test_index, 'MAIN')) # Or other index files

def test_query_by_label_exact(built_test_index):
    """Tests exact search on the label field."""
    ix = open_dir(built_test_index)
    searcher = ix.searcher()
    # Query the 'label' field
    parser = QueryParser("label", ix.schema)
    query = parser.parse(TEST_LABEL) # "apple"
    results = searcher.search(query)

    print(f"\nSearch results for '{TEST_LABEL}': {results}")

    assert len(results) > 0, f"No results found for '{TEST_LABEL}'"
    # Check if the expected CURIE is among the results, preferably the top one
    assert results[0]['curie'] == TEST_CURIE
    assert results[0]['label'] == TEST_LABEL # Verify stored label

    searcher.close()
    ix.close() # Close index readers/writers

def test_query_by_label_fuzzy(built_test_index):
    """Tests fuzzy search on the label field."""
    ix = open_dir(built_test_index)
    searcher = ix.searcher()
    parser = QueryParser("label", ix.schema)
    # Fuzzy search for "appl" (e.g. "appl~")
    query = parser.parse("appl~") # Whoosh fuzzy syntax
    results = searcher.search(query)

    print(f"\nFuzzy search results for 'appl~': {results}")

    assert len(results) > 0, "No results found for 'appl~'"
    assert results[0]['curie'] == TEST_CURIE
    assert results[0]['label'] == TEST_LABEL

    searcher.close()
    ix.close()

def test_query_by_synonym(built_test_index):
    """Tests search on the synonyms field."""
    ix = open_dir(built_test_index)
    searcher = ix.searcher()
    # Query the 'synonyms' field
    parser = QueryParser("synonyms", ix.schema)
    query = parser.parse(TEST_SYNONYM) # "eating apple"
    results = searcher.search(query)

    print(f"\nSearch results for synonym '{TEST_SYNONYM}': {results}")

    assert len(results) > 0, f"No results found for synonym '{TEST_SYNONYM}'"
    assert results[0]['curie'] == TEST_CURIE

    searcher.close()
    ix.close()

def test_query_by_definition_keyword(built_test_index):
    """Tests search on the definition field."""
    ix = open_dir(built_test_index)
    searcher = ix.searcher()
    # Query the 'definition' field
    parser = QueryParser("definition", ix.schema)
    query = parser.parse(TEST_DEFINITION_KEYWORD) # "fruit"
    results = searcher.search(query)

    print(f"\nSearch results for definition keyword '{TEST_DEFINITION_KEYWORD}': {results}")

    assert len(results) > 0, f"No results found for definition keyword '{TEST_DEFINITION_KEYWORD}'"
    assert results[0]['curie'] == TEST_CURIE

    searcher.close()
    ix.close()

def test_query_by_relations_keyword(built_test_index):
    """Tests search on the flattened relations_text field."""
    ix = open_dir(built_test_index)
    searcher = ix.searcher()
    # Query the 'relations_text' field
    parser = QueryParser("relations_text", ix.schema)
    # Search for a keyword expected in the relations_text (e.g., target's label or part of target's CURIE)
    # Note: Indexing targets as just CURIEs requires searching for 'FOODON:00001101'
    # Indexing target labels requires adding that logic in build_lexical_index
    # Let's assume for now we search for the CURIE or part of it if indexed as text
    # Or if we enhance build_index to add target labels to relations_text:
    # relations_text = " ".join([f"{prop}: {' '.join(targets)} {' '.join(target_labels)}" for ...])
    # For simplicity with current build_index, let's search the target CURIE part
    query = parser.parse("00001101") # Search part of the target CURIE FOODON:00001101
    results = searcher.search(query)

    print(f"\nSearch results for relations keyword '00001101': {results}")

    assert len(results) > 0, f"No results found for relations keyword '00001101'"
    assert results[0]['curie'] == TEST_CURIE

    searcher.close()
    ix.close()
--- END File: src/tests/test_lexical_index.py ---

--- File: src/tests/test_parse_ontology.py ---
import pytest
import os
import json
import rdflib
from rdflib import Graph

# Adjust path to import modules from src
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from ingestion.parse_ontology import (
    load_ontology,
    extract_labels_and_synonyms,
    extract_definitions,
    extract_hierarchy,
    extract_relations,
)
from config import TEST_ONTOLOGY_FILE, NAMESPACE_MAP, RELATION_PROPERTIES
from utils.ontology_utils import uri_to_curie # Import if needed for assertions

# Define a fixture to load the test graph once for all tests
@pytest.fixture(scope="module")
def test_graph():
    """Loads the test ontology snippet into a graph."""
    if not os.path.exists(TEST_ONTOLOGY_FILE):
        pytest.skip(f"Test ontology snippet not found at {TEST_ONTOLOGY_FILE}")
    try:
        return load_ontology(TEST_ONTOLOGY_FILE)
    except Exception as e:
        pytest.fail(f"Failed to load test ontology: {e}")

# Define a known CURIE from the test snippet
TEST_CURIE = "FOODON:00001100" # apple
TEST_CURIE_PARENT1 = "FOODON:00001000" # plant-based food
TEST_CURIE_PARENT2 = "FOODON:00002000" # Pome fruit
TEST_CURIE_RELATION_TARGET = "FOODON:00001101" # apple tree (example target)
TEST_RELATION_NAME = "part_of" # example relation name

def test_load_ontology(test_graph):
    """Tests if the ontology loads and is an RDFLib Graph."""
    assert isinstance(test_graph, Graph)
    assert len(test_graph) > 0, "Test graph is empty!"
    print(f"Loaded test graph with {len(test_graph)} triples.")


def test_extract_labels_and_synonyms(test_graph):
    """Tests extraction of labels and synonyms."""
    labels_synonyms = extract_labels_and_synonyms(test_graph)
    print(f"Extracted labels/synonyms: {labels_synonyms}")

    assert TEST_CURIE in labels_synonyms
    apple_data = labels_synonyms[TEST_CURIE]
    assert apple_data['label'] == "apple"
    assert "eating apple" in apple_data['synonyms']
    assert "apple fruit" in apple_data['synonyms']
    assert TEST_CURIE_PARENT1 in labels_synonyms # Should also extract its label/synonyms
    assert labels_synonyms[TEST_CURIE_PARENT1]['label'] == "plant-based food"


def test_extract_definitions(test_graph):
    """Tests extraction of definitions."""
    definitions = extract_definitions(test_graph)
    print(f"Extracted definitions: {definitions}")

    assert TEST_CURIE in definitions
    assert "The fruit of a Malus domestica tree." in definitions[TEST_CURIE] # Use 'in' for substring check or exact match


def test_extract_hierarchy(test_graph):
    """Tests extraction of hierarchy (parents and ancestors)."""
    hierarchy = extract_hierarchy(test_graph)
    print(f"Extracted hierarchy: {hierarchy}")

    assert TEST_CURIE in hierarchy
    apple_hierarchy = hierarchy[TEST_CURIE]

    # Check direct parents
    assert TEST_CURIE_PARENT1 in apple_hierarchy['parents']
    assert TEST_CURIE_PARENT2 in apple_hierarchy['parents']
    assert len(apple_hierarchy['parents']) >= 2 # At least 2 parents from the snippet

    # Check ancestors (should include parents and parents' ancestors)
    assert TEST_CURIE_PARENT1 in apple_hierarchy['ancestors']
    assert TEST_CURIE_PARENT2 in apple_hierarchy['ancestors']
    # Assuming plant-based food has no ancestors in this snippet, ancestors == parents here
    # If plant-based food had parents, they should appear in apple's ancestors.
    # For this snippet, ancestors should be the same as parents.
    assert set(apple_hierarchy['ancestors']) == set(apple_hierarchy['parents'])


def test_extract_relations(test_graph):
    """Tests extraction of specific relations."""
    relations = extract_relations(test_graph, RELATION_PROPERTIES)
    print(f"Extracted relations: {relations}")

    assert TEST_CURIE in relations
    apple_relations = relations[TEST_CURIE]

    # Check if the specific relation from the snippet was found
    assert TEST_RELATION_NAME in apple_relations
    assert TEST_CURIE_RELATION_TARGET in apple_relations[TEST_RELATION_NAME]
    assert len(apple_relations[TEST_RELATION_NAME]) >= 1


# You could add a test that runs the full main parsing flow using the test snippet
# and checks the structure/content of the resulting JSON file.
# Example (requires a temporary file):
# import tempfile
# @pytest.fixture
# def temp_json_path():
#     with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as tmp:
#         yield tmp.name
#     os.unlink(tmp.name)
#
# def test_main_parsing_flow(temp_json_path):
#      # Temporarily override config paths for the test
#      original_ontology_file = config.ONTOLOGY_FILE
#      original_dump_path = config.ONTOLOGY_DUMP_PATH
#      config.ONTOLOGY_FILE = TEST_ONTOLOGY_FILE
#      config.ONTOLOGY_DUMP_PATH = temp_json_path
#
#      try:
#          parse_ontology.main()
#          assert os.path.exists(temp_json_path)
#          with open(temp_json_path, 'r', encoding='utf-8') as f:
#              data = json.load(f)
#
#          assert TEST_CURIE in data
#          apple_data = data[TEST_CURIE]
#          assert apple_data['label'] == 'apple'
#          assert 'synonyms' in apple_data and len(apple_data['synonyms']) > 0
#          assert 'definition' in apple_data and apple_data['definition'] is not None
#          assert 'parents' in apple_data and len(apple_data['parents']) > 0
#          assert 'ancestors' in apple_data and len(apple_data['ancestors']) > 0
#          assert 'relations' in apple_data and len(apple_data['relations']) > 0
#
#      finally:
#          # Restore original paths
#          config.ONTOLOGY_FILE = original_ontology_file
#          config.ONTOLOGY_DUMP_PATH = original_dump_path
--- END File: src/tests/test_parse_ontology.py ---

--- File: src/tests/__init__.py ---

--- END File: src/tests/__init__.py ---

--- File: src/utils/__init__.py ---

--- END File: src/utils/__init__.py ---

--- File: src/utils/ontology_utils.py ---
# src/utils/ontology_utils.py
import rdflib
from rdflib import URIRef, Namespace
from typing import Optional, Dict, Union

# Adjusted: Import CURIE_PREFIX_MAP and use it as the default.
# This assumes that the script/module *importing* ontology_utils
# has already ensured that the project root (e.g., 'onto_rag') is on sys.path,
# so that 'src' is resolvable as a top-level package.
from src.config import CURIE_PREFIX_MAP # Corrected import

def uri_to_curie(uri: Union[str, URIRef], namespace_map: Dict[str, str] = CURIE_PREFIX_MAP) -> str:
    """
    Converts a full URI to a CURIE (e.g., http://...#term -> PREFIX:term).
    Assumes namespace_map is structured as {base_uri_str: prefix_str}.
    """
    uri_str = str(uri) # Ensure it's a string

    # Iterate through the provided namespace_map (base_uri: prefix)
    # Sort by length of base_uri descending to match longest first (more specific)
    # This helps avoid issues where one base_uri is a prefix of another.
    # e.g., "http://purl.obolibrary.org/obo/" and "http://purl.obolibrary.org/obo/FOODON_"
    sorted_namespace_map_items = sorted(namespace_map.items(), key=lambda item: len(item[0]), reverse=True)

    for base_uri, prefix in sorted_namespace_map_items:
        if uri_str.startswith(base_uri):
            return f"{prefix}:{uri_str[len(base_uri):]}"

    # Fallback for common RDF/RDFS/OWL/XSD prefixes if not found in the main map
    # This local map is prefix: base_uri_str
    common_rdf_prefixes = {
        "rdf": str(rdflib.RDF),
        "rdfs": str(rdflib.RDFS),
        "owl": str(rdflib.OWL),
        "xsd": str(rdflib.XSD),
    }
    for prefix, base_uri in common_rdf_prefixes.items():
        if uri_str.startswith(base_uri):
            return f"{prefix}:{uri_str[len(base_uri):]}"
            
    # If rdflib can make a qname (usually for registered namespaces)
    try:
        g = rdflib.Graph()
        # Bind known namespaces to help compute_qname
        # For namespace_map (base_uri: prefix), we need to iterate as base_uri, prefix
        for ns_uri_str_from_map, prefix_from_map in namespace_map.items():
             g.bind(prefix_from_map, Namespace(ns_uri_str_from_map))

        # Bind common RDF ones too, in case they weren't in namespace_map or to ensure standard prefixes
        g.bind("owl", rdflib.OWL)
        g.bind("rdf", rdflib.RDF)
        g.bind("rdfs", rdflib.RDFS)
        g.bind("xsd", rdflib.XSD)

        # compute_qname might fail if the URI doesn't match any bound namespace's base
        # It returns (prefix, namespace_uri, local_name)
        qname_tuple = g.compute_qname(URIRef(uri_str))
        return f"{qname_tuple[0]}:{qname_tuple[2]}"
    except Exception: # Broad except as compute_qname can raise various things or return unexpected tuples
        pass # If rdflib fails, just return the original URI string

    return uri_str # If no CURIE conversion possible, return original URI string

def curie_to_uri(curie: str, namespace_map: Dict[str, str] = CURIE_PREFIX_MAP) -> Optional[URIRef]:
    """
    Converts a CURIE (e.g., PREFIX:term) to a full rdflib.URIRef.
    Assumes namespace_map is structured as {base_uri_str: prefix_str}.
    """
    if ':' not in curie:
        # Try to see if it's a default rdflib qname like "rdf:type" that rdflib can expand
        # This part might be less common if CURIEs are always expected with user-defined prefixes
        try:
            g = rdflib.Graph()
            # Bind namespaces from the map (base_uri: prefix)
            for ns_uri_str_from_map, prefix_from_map in namespace_map.items():
                g.bind(prefix_from_map, Namespace(ns_uri_str_from_map))
            # Bind common RDF ones
            g.bind("owl", rdflib.OWL)
            g.bind("rdf", rdflib.RDF)
            g.bind("rdfs", rdflib.RDFS)
            g.bind("xsd", rdflib.XSD)
            
            # If it's something like "owl:Class", g.namespace_manager.expand_curie will work
            expanded_uri = g.namespace_manager.expand_curie(curie)
            if str(expanded_uri) != curie: # Check if expansion actually happened
                 return URIRef(expanded_uri)
        except Exception:
            pass # If expansion fails, proceed to manual lookup
        return None # Not a valid CURIE format for our map, and rdflib couldn't expand

    prefix_part, local_name = curie.split(':', 1)

    # Iterate through namespace_map (base_uri: prefix) to find the matching prefix
    found_base_uri = None
    for base_uri_key, prefix_val in namespace_map.items():
        if prefix_val == prefix_part:
            found_base_uri = base_uri_key
            break
    
    if found_base_uri:
        return URIRef(found_base_uri + local_name)
    else:
        # Fallback: Try common RDF prefixes if not in the custom map
        # This local map is prefix: rdflib.Namespace object
        common_rdf_namespaces = {
            "rdf": rdflib.RDF,
            "rdfs": rdflib.RDFS,
            "owl": rdflib.OWL,
            "xsd": rdflib.XSD,
        }
        if prefix_part in common_rdf_namespaces:
            # Access items in the namespace like attributes: common_rdf_namespaces[prefix_part].type
            # or by string concatenation: URIRef(str(common_rdf_namespaces[prefix_part]) + local_name)
            return URIRef(str(common_rdf_namespaces[prefix_part]) + local_name)
        return None # Prefix not found in custom map or common RDF prefixes
--- END File: src/utils/ontology_utils.py ---

--- File: src/retriever/__init__.py ---

--- END File: src/retriever/__init__.py ---

--- File: src/retriever/hybrid_retriever.py ---
# src/retriever/hybrid_retriever.py
import json
import os
# import numpy as np # numpy is used by sentence_transformers and faiss internally
from whoosh.index import open_dir as open_whoosh_index
from whoosh.qparser import MultifieldParser, OrGroup
from sentence_transformers import SentenceTransformer

# --- Add project root to sys.path if running script directly ---
# This block is useful if you ever run this script directly (e.g., for debugging)
# and not as a module (python -m src.retriever.hybrid_retriever)
if __name__ == '__main__':
    import sys
    PROJECT_ROOT_FOR_DIRECT_RUN = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    if PROJECT_ROOT_FOR_DIRECT_RUN not in sys.path:
        sys.path.insert(0, PROJECT_ROOT_FOR_DIRECT_RUN)
# --- End sys.path modification ---


from src.vector_store.faiss_store import FAISSVectorStore
from src.config import (
    ONTOLOGY_DUMP_JSON,
    WHOOSH_INDEX_DIR,
    FAISS_INDEX_PATH,
    FAISS_METADATA_PATH,
    EMBEDDING_MODEL_NAME,
    DEFAULT_K_LEXICAL,
    DEFAULT_K_VECTOR,
)
# from src.utils.logger import get_logger # Placeholder for future logging
# logger = get_logger(__name__) # Placeholder

class HybridRetriever:
    def __init__(self,
                 ontology_data_path=ONTOLOGY_DUMP_JSON,
                 whoosh_index_dir=WHOOSH_INDEX_DIR,
                 faiss_index_path=FAISS_INDEX_PATH,
                 faiss_metadata_path=FAISS_METADATA_PATH,
                 embedding_model_name=EMBEDDING_MODEL_NAME):
        """
        Initializes the HybridRetriever.
        """
        print(f"Initializing HybridRetriever...")
        # logger.info("Initializing HybridRetriever...")

        print(f"Loading ontology data from: {ontology_data_path}")
        if not os.path.exists(ontology_data_path):
            raise FileNotFoundError(f"Ontology data file not found: {ontology_data_path}")
        with open(ontology_data_path, 'r', encoding='utf-8') as f:
            self.ontology_data = json.load(f)
        print(f"Loaded {len(self.ontology_data)} ontology entries.")

        print(f"Loading Whoosh index from: {whoosh_index_dir}")
        if not os.path.exists(whoosh_index_dir) or not os.listdir(whoosh_index_dir):
            raise FileNotFoundError(f"Whoosh index directory not found or empty: {whoosh_index_dir}. Run ingestion scripts.")
        self.whoosh_ix = open_whoosh_index(whoosh_index_dir)
        self.whoosh_searcher = self.whoosh_ix.searcher()
        
        # Fields to search in Whoosh, must match the schema in build_lexical_index.py
        # 'relations_text' is indexed (stored=False) so it can be searched.
        # 'curie' is an ID field, typically not directly searched with MultifieldParser unless intended.
        self.whoosh_fields_to_search = ["label", "synonyms", "definition", "relations_text"]
        self.whoosh_parser = MultifieldParser(self.whoosh_fields_to_search, schema=self.whoosh_ix.schema, group=OrGroup)
        print("Whoosh index loaded.")

        print(f"Loading embedding model: {embedding_model_name}")
        self.embedding_model = SentenceTransformer(embedding_model_name, trust_remote_code=True)
        print("Embedding model loaded.")

        print(f"Initializing FAISS vector store (index: {faiss_index_path}, metadata: {faiss_metadata_path})...")
        self.faiss_store = FAISSVectorStore(
            index_path=faiss_index_path,
            metadata_path=faiss_metadata_path,
            embeddings_file_path=None 
        )
        if not self.faiss_store.index or not self.faiss_store.metadata:
            raise FileNotFoundError(f"FAISS index file '{faiss_index_path}' or metadata file '{faiss_metadata_path}' not found or empty. Please build it first.")
        print("FAISS vector store initialized.")
        
        print("HybridRetriever initialized successfully.")

    def _lexical_search(self, query_string, limit=DEFAULT_K_LEXICAL):
        """
        Performs lexical search using Whoosh.
        Returns a list of dicts: {'id': str, 'label': str, 'score': float, 'source': 'lexical', 'details': dict}
        """
        results = []
        if not query_string:
            return results

        try:
            query = self.whoosh_parser.parse(query_string)
            search_results = self.whoosh_searcher.search(query, limit=limit)
            
            for hit in search_results:
                hit_fields = hit.fields()  # Get all stored fields as a dictionary
                term_curie = hit_fields.get('curie') # *** CHANGED: 'id' to 'curie' ***

                if term_curie is None:
                    print(f"Warning: Lexical search hit found without a 'curie'. Hit details: {hit}")
                    # logger.warning(f"Lexical search hit found without a 'curie'. Hit details: {hit}")
                    continue

                # 'relations_text' is not stored, so it won't be in hit_fields.
                # We retrieve label, synonyms, definition if they were stored.
                retrieved_label = hit_fields.get('label')
                retrieved_synonyms_str = hit_fields.get('synonyms') # This will be a space-separated string
                retrieved_definition = hit_fields.get('definition')

                results.append({
                    "id": term_curie, # Keep 'id' as the key in the result for consistency with vector search
                    "label": retrieved_label if retrieved_label is not None else self.ontology_data.get(term_curie, {}).get('label', 'N/A'),
                    "score": hit.score, 
                    "source": "lexical",
                    "details": {
                        # Convert synonyms string back to list if needed, or keep as string
                        "retrieved_synonyms": retrieved_synonyms_str.split() if retrieved_synonyms_str else [], 
                        "retrieved_definition": retrieved_definition
                    }
                })
        except Exception as e:
            print(f"Error during lexical search for '{query_string}': {e}")
            # logger.error(f"Error during lexical search for '{query_string}': {e}", exc_info=True)
            import traceback
            traceback.print_exc() # Print full traceback for debugging
        return results

    def _vector_search(self, query_string, k=DEFAULT_K_VECTOR):
        """
        Performs vector search using FAISS.
        Returns a list of dicts: {'id': str, 'label': str, 'score': float, 'source': 'vector', 'details': dict}
        """
        results = []
        if not query_string:
            return results

        try:
            query_vector = self.embedding_model.encode([query_string], convert_to_numpy=True)
            distances, _, metadata_items = self.faiss_store.search(query_vector, k=k)
            
            for i in range(len(metadata_items)):
                term_id = metadata_items[i]['id'] # FAISS metadata stores 'id'
                results.append({
                    "id": term_id,
                    "label": metadata_items[i]['label'],
                    "score": float(distances[i]), 
                    "source": "vector",
                    "details": {}
                })
        except Exception as e:
            print(f"Error during vector search for '{query_string}': {e}")
            # logger.error(f"Error during vector search for '{query_string}': {e}", exc_info=True)
            import traceback
            traceback.print_exc() # Print full traceback for debugging
        return results

    def search(self, query_string, lexical_limit=DEFAULT_K_LEXICAL, vector_k=DEFAULT_K_VECTOR):
        """
        Performs hybrid search.
        """
        lexical_results = self._lexical_search(query_string, limit=lexical_limit)
        vector_results = self._vector_search(query_string, k=vector_k)
        
        return {
            "query": query_string,
            "lexical_results": lexical_results,
            "vector_results": vector_results,
        }

    def get_term_details(self, term_id):
        """
        Retrieves full details for a given term ID (CURIE) from the loaded ontology data.
        Returns a dictionary with all term details including the ID, or None if not found.
        """
        term_data = self.ontology_data.get(term_id)
        if term_data is not None:
            # Make a copy to avoid modifying the original data
            term_data = dict(term_data)
            # Add the ID to the returned data
            term_data['id'] = term_id
        return term_data

    def close(self):
        """
        Closes any open resources, like the Whoosh searcher.
        """
        if self.whoosh_searcher:
            self.whoosh_searcher.close()
        print("HybridRetriever resources closed.")

# Example Usage (for testing purposes)
if __name__ == '__main__':
    # This sys.path modification is now at the top of the file for when __name__ == '__main__'
    
    from src.config import PROJECT_ROOT # Import after sys.path is potentially modified
    print(f"Configured project root: {PROJECT_ROOT}")
    if not os.getcwd().startswith(PROJECT_ROOT) and os.getcwd() != PROJECT_ROOT:
         print(f"Warning: Current working directory ({os.getcwd()}) might not be the project root.")
         print("Consider running with 'python -m src.retriever.hybrid_retriever' from the project root directory.")

    print("Running HybridRetriever example...")
    retriever = None
    try:
        retriever = HybridRetriever()
        
        queries = ["GARLIC", "SALT", "GARBANZO", "TAHINI", "LEMON JUICE", "HONEY" ,"WATER", "OLIVE OIL", "ROSMARY", "HUMMUS"]
        
        for query in queries:
            print(f"\nSearching for: '{query}'")
            results = retriever.search(query, lexical_limit=3, vector_k=3)
            
            print("\n--- Lexical Results ---")
            if results["lexical_results"]:
                for res in results["lexical_results"]:
                    print(f"  ID: {res['id']}, Label: {res['label']}, Score (Whoosh): {res['score']:.4f}")
                    # print(f"    Details: {res['details']}") # Uncomment to see retrieved synonyms/def
            else:
                print("  No lexical results.")

            print("\n--- Vector Results ---")
            if results["vector_results"]:
                for res in results["vector_results"]:
                    print(f"  ID: {res['id']}, Label: {res['label']}, Score (L2 Distance): {res['score']:.4f}")
            else:
                print("  No vector results.")
            print("-" * 40)
            
    except FileNotFoundError as e:
        print(f"\nERROR: A required file was not found: {e}")
        print("Please ensure all data files (ontology_dump.json) and indices (Whoosh, FAISS) are correctly built and paths are set in src/config.py.")
        print("You might need to run the ingestion and embedding scripts first.")
    except Exception as e:
        print(f"\nAn unexpected error occurred during example run: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if retriever:
            retriever.close()
--- END File: src/retriever/hybrid_retriever.py ---

--- File: src/vector_store/__init__.py ---

--- END File: src/vector_store/__init__.py ---

--- File: src/vector_store/faiss_store.py ---
# src/vector_store/faiss_store.py
import faiss
import json
import numpy as np
import os
from src.config import EMBEDDINGS_FILE # Default embeddings file to build from

# from src.utils.logger import get_logger # Placeholder for future logging
# logger = get_logger(__name__)

class FAISSVectorStore:
    def __init__(self, index_path, metadata_path, embeddings_file_path=None, dimension=None):
        """
        Initializes the FAISSVectorStore.
        Tries to load an existing index and metadata. If not found, and an
        embeddings_file_path is provided, it will attempt to build them.

        Args:
            index_path (str): Path to save/load the FAISS index file (.bin).
            metadata_path (str): Path to save/load the metadata JSON file (.json).
            embeddings_file_path (str, optional): Path to the 'embeddings.json' file.
                                                  Used only if the index/metadata needs to be built.
                                                  Defaults to config.EMBEDDINGS_FILE.
            dimension (int, optional): The dimension of the vectors.
                                       Required if building the index and it cannot be inferred.
                                       Usually inferred from the first embedding.
        """
        self.index_path = index_path
        self.metadata_path = metadata_path
        self.embeddings_file_path = embeddings_file_path if embeddings_file_path else EMBEDDINGS_FILE
        self.dimension = dimension

        self.index = None
        self.metadata = []  # List of dicts, e.g., [{'id': 'FOODON_123', 'label': 'Apple'}, ...]

        if os.path.exists(self.index_path) and os.path.exists(self.metadata_path):
            print(f"Loading existing FAISS index from {self.index_path} and metadata from {self.metadata_path}")
            # logger.info(f"Loading existing FAISS index from {self.index_path} and metadata from {self.metadata_path}")
            try:
                self.load_store()
            except Exception as e:
                print(f"Error loading existing FAISS store: {e}. Will attempt to build if embeddings file provided.")
                # logger.error(f"Error loading existing FAISS store: {e}. Will attempt to build if embeddings file provided.", exc_info=True)
                self._try_build_store()
        elif self.embeddings_file_path and os.path.exists(self.embeddings_file_path):
            print(f"FAISS index/metadata not found. Attempting to build from {self.embeddings_file_path}")
            # logger.info(f"FAISS index/metadata not found. Attempting to build from {self.embeddings_file_path}")
            self._try_build_store()
        else:
            message = "FAISS index/metadata not found. "
            if self.embeddings_file_path:
                message += f"Embeddings file {self.embeddings_file_path} also not found or not specified for building."
            else:
                message += "No embeddings file path provided to build a new store."
            print(f"Warning: {message}")
            # logger.warning(message)

    def _try_build_store(self):
        """Helper method to attempt building the store."""
        embeddings_data = self._load_embeddings_data()
        if embeddings_data:
            self.build_index_from_embeddings(embeddings_data)
            if self.index and self.metadata: # Check if build was successful
                 self.save_store()
            else:
                print("Warning: FAISS index or metadata not built successfully from embeddings data.")
                # logger.warning("FAISS index or metadata not built successfully from embeddings data.")
        else:
            print(f"Warning: No embeddings data loaded from {self.embeddings_file_path}, FAISS index not built.")
            # logger.warning(f"No embeddings data loaded from {self.embeddings_file_path}, FAISS index not built.")


    def _load_embeddings_data(self):
        """
        Loads embeddings data from the specified JSON file.
        Expected format: list of {'id': str, 'label': str, 'embedding': list[float]}
        """
        if not self.embeddings_file_path or not os.path.exists(self.embeddings_file_path):
            print(f"Error: Embeddings file not found: {self.embeddings_file_path}")
            # logger.error(f"Embeddings file not found: {self.embeddings_file_path}")
            return None
        try:
            with open(self.embeddings_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            # Sanity check the data structure
            if not isinstance(data, list) or not data:
                print(f"Error: Embeddings file {self.embeddings_file_path} is empty or not a list.")
                # logger.error(f"Embeddings file {self.embeddings_file_path} is empty or not a list.")
                return None
            if not all('id' in item and 'label' in item and 'embedding' in item for item in data):
                print(f"Error: Embeddings data in {self.embeddings_file_path} has incorrect format.")
                # logger.error(f"Embeddings data in {self.embeddings_file_path} has incorrect format.")
                return None

            print(f"Loaded {len(data)} embeddings from {self.embeddings_file_path}")
            # logger.info(f"Loaded {len(data)} embeddings from {self.embeddings_file_path}")
            return data
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON from embeddings file {self.embeddings_file_path}: {e}")
            # logger.error(f"Error decoding JSON from embeddings file {self.embeddings_file_path}: {e}", exc_info=True)
            return None
        except Exception as e:
            print(f"An unexpected error occurred while loading embeddings file {self.embeddings_file_path}: {e}")
            # logger.error(f"An unexpected error occurred while loading embeddings file {self.embeddings_file_path}: {e}", exc_info=True)
            return None

    def build_index_from_embeddings(self, embeddings_data):
        """
        Builds the FAISS index and prepares metadata from loaded embeddings data.

        Args:
            embeddings_data (list): List of dictionaries, where each dict has
                                    'id', 'label', and 'embedding' keys.
        """
        if not embeddings_data:
            print("Warning: No embeddings data provided to build_index_from_embeddings.")
            # logger.warning("No embeddings data provided to build_index_from_embeddings.")
            return

        vectors = []
        current_metadata = [] # Use a temporary list to build metadata
        for item in embeddings_data:
            vectors.append(item['embedding'])
            current_metadata.append({'id': item['id'], 'label': item['label']})

        if not vectors:
            print("Warning: No vectors extracted from embeddings_data.")
            # logger.warning("No vectors extracted from embeddings_data.")
            return

        vectors_np = np.array(vectors).astype('float32')

        if self.dimension is None:
            self.dimension = vectors_np.shape[1]
        elif self.dimension != vectors_np.shape[1]:
            err_msg = f"Provided dimension {self.dimension} does not match embeddings dimension {vectors_np.shape[1]}"
            print(f"Error: {err_msg}")
            # logger.error(err_msg)
            raise ValueError(err_msg)

        # Using IndexFlatL2, a simple L2 distance index.
        # For larger datasets, more advanced indexes like IndexIVFFlat might be better.
        try:
            self.index = faiss.IndexFlatL2(self.dimension)
            self.index.add(vectors_np)
            self.metadata = current_metadata # Assign once build is successful
            print(f"FAISS index built successfully with {self.index.ntotal} vectors of dimension {self.dimension}.")
            # logger.info(f"FAISS index built successfully with {self.index.ntotal} vectors of dimension {self.dimension}.")
        except Exception as e:
            print(f"Error building FAISS index: {e}")
            # logger.error(f"Error building FAISS index: {e}", exc_info=True)
            self.index = None # Ensure index is None if build fails
            self.metadata = []


    def save_store(self):
        """Saves the FAISS index and metadata to their respective files."""
        if self.index is None:
            print("Warning: No FAISS index to save.")
            # logger.warning("No FAISS index to save.")
            return # Do not save metadata if index is not there or failed to build

        os.makedirs(os.path.dirname(self.index_path), exist_ok=True)
        faiss.write_index(self.index, self.index_path)
        print(f"FAISS index saved to {self.index_path}")
        # logger.info(f"FAISS index saved to {self.index_path}")

        if not self.metadata:
            print("Warning: No FAISS metadata to save (metadata list is empty).")
            # logger.warning("No FAISS metadata to save (metadata list is empty).")
            # If index exists but metadata is empty, this is an inconsistent state.
            # Depending on strictness, one might choose to not save the index either,
            # or clear the index file if it exists. For now, we save index if it exists.
            return

        os.makedirs(os.path.dirname(self.metadata_path), exist_ok=True)
        with open(self.metadata_path, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, indent=4)
        print(f"FAISS metadata saved to {self.metadata_path}")
        # logger.info(f"FAISS metadata saved to {self.metadata_path}")

    def load_store(self):
        """Loads the FAISS index and metadata from files."""
        if not os.path.exists(self.index_path):
            # logger.error(f"FAISS index file not found: {self.index_path}")
            raise FileNotFoundError(f"FAISS index file not found: {self.index_path}")
        self.index = faiss.read_index(self.index_path)
        if self.dimension is None: # Infer dimension if not set
            self.dimension = self.index.d
        elif self.dimension != self.index.d: # Check consistency
            # logger.warning(f"Stored index dimension {self.index.d} differs from configured {self.dimension}. Using stored.")
            print(f"Warning: Stored index dimension {self.index.d} differs from configured {self.dimension}. Using stored.")
            self.dimension = self.index.d

        print(f"FAISS index loaded from {self.index_path}. Index has {self.index.ntotal} vectors of dim {self.index.d}.")
        # logger.info(f"FAISS index loaded from {self.index_path}. Index has {self.index.ntotal} vectors of dim {self.index.d}.")

        if not os.path.exists(self.metadata_path):
            # logger.error(f"FAISS metadata file not found: {self.metadata_path}")
            raise FileNotFoundError(f"FAISS metadata file not found: {self.metadata_path}")
        with open(self.metadata_path, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        print(f"FAISS metadata loaded from {self.metadata_path}. {len(self.metadata)} items.")
        # logger.info(f"FAISS metadata loaded from {self.metadata_path}. {len(self.metadata)} items.")

        if self.index and self.metadata and self.index.ntotal != len(self.metadata):
            mismatch_msg = (f"Mismatch between FAISS index size ({self.index.ntotal}) "
                            f"and metadata size ({len(self.metadata)}). This may lead to errors.")
            print(f"Warning: {mismatch_msg}")
            # logger.warning(mismatch_msg)
            # Potentially raise an error or try to reconcile, for now, just warn.

    def search(self, query_vector, k=5):
        """
        Performs a K-Nearest Neighbors search on the FAISS index.

        Args:
            query_vector (np.ndarray): A 2D numpy array of shape (num_queries, dimension)
                                       or a 1D numpy array (single query).
            k (int): The number of nearest neighbors to retrieve.

        Returns:
            tuple: (distances, indices, metadata_items)
                   distances (np.ndarray): Distances to the k nearest neighbors.
                   indices (np.ndarray): FAISS internal indices of the k nearest neighbors.
                   metadata_items (list of lists or list of dicts): Corresponding metadata for the results.
                                     If single query, returns list of dicts. If multiple, list of lists of dicts.
        """
        if self.index is None:
            print("Error: FAISS index not initialized. Cannot perform search.")
            # logger.error("FAISS index not initialized. Cannot perform search.")
            return np.array([]), np.array([]), []

        if not isinstance(query_vector, np.ndarray):
            query_vector = np.array(query_vector)

        if query_vector.ndim == 1: # Single query vector
            query_vector = np.expand_dims(query_vector, axis=0)
        
        if query_vector.shape[1] != self.index.d:
            err_msg = f"Query vector dimension ({query_vector.shape[1]}) does not match index dimension ({self.index.d})."
            print(f"Error: {err_msg}")
            # logger.error(err_msg)
            raise ValueError(err_msg)

        distances, faiss_indices = self.index.search(query_vector.astype('float32'), k)
        
        # faiss_indices will be shape (num_queries, k)
        # distances will be shape (num_queries, k)
        
        all_results_metadata = []
        for i in range(faiss_indices.shape[0]): # Iterate over queries
            query_results_metadata = []
            for j in range(faiss_indices.shape[1]): # Iterate over k results for that query
                idx = faiss_indices[i][j]
                if idx != -1 and idx < len(self.metadata): # faiss_index can be -1 if k > ntotal
                    query_results_metadata.append(self.metadata[idx])
                # else: could append a placeholder, or log. For now, items are just shorter if fewer than k found.
            all_results_metadata.append(query_results_metadata)
        
        # For a single query_vector (most common use case here), return the inner list directly.
        if query_vector.shape[0] == 1:
            return distances[0], faiss_indices[0], all_results_metadata[0]
        else: # If multiple query vectors were passed
            return distances, faiss_indices, all_results_metadata

# Example usage / test function
if __name__ == '__main__':
    from src.vector_store.faiss_store import FAISSVectorStore
    from src.config import FAISS_INDEX_PATH, FAISS_METADATA_PATH

    # remove old files so the class knows it has to build
    import os, pathlib, json
    for p in (FAISS_INDEX_PATH, FAISS_METADATA_PATH):
        pathlib.Path(p).unlink(missing_ok=True)

    store = FAISSVectorStore(
        index_path     = FAISS_INDEX_PATH,
        metadata_path  = FAISS_METADATA_PATH,
        # embeddings_file_path=None  â†’ default picks data/embeddings.json
        # dimension=None            â†’ it will infer 384 automatically
    )
    print(f"Built FAISS index with {store.index.ntotal} vectors of dim {store.index.d}")
--- END File: src/vector_store/faiss_store.py ---

--- File: src/evaluation/evaluate_retriever_recall.py ---
# src/evaluation/evaluate_retriever_recall.py

import xml.etree.ElementTree as ET
import os
import sys
import logging
from collections import defaultdict

# Add project root to Python path to allow direct imports from src
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(PROJECT_ROOT)

from src.retriever.hybrid_retriever import HybridRetriever
from src.config import (
    ONTOLOGY_DUMP_JSON,
    WHOOSH_INDEX_DIR,
    FAISS_INDEX_PATH,
    FAISS_METADATA_PATH,
    EMBEDDING_MODEL_NAME,
    CURIE_PREFIX_MAP,
    DEFAULT_K_LEXICAL,
    DEFAULT_K_VECTOR,
    DEFAULT_RERANK_K
)
from src.utils.ontology_utils import uri_to_curie

# --- Configuration for this specific evaluation script ---
# Path to your evaluation XML file
EVALUATION_XML_FILE = os.path.join(PROJECT_ROOT, "data", "CafeteriaFCD_foodon.xml")
# Top K results from the combined retriever output to consider for a match
RECALL_AT_K = DEFAULT_RERANK_K # How many retrieved items to check for a match

# --- Logging Setup ---
# Stays with basicConfig as src.utils.logging.get_logger is "to be developed"
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def parse_evaluation_xml(xml_file_path: str) -> list:
    """
    Parses the evaluation XML file to extract entities and their ground truth semantic tags.

    Args:
        xml_file_path (str): Path to the XML file.

    Returns:
        list: A list of dictionaries, each containing:
              {'text': str, 'true_curies': set_of_str, 'doc_id': str, 'ann_id': str}
              Returns an empty list if parsing fails.
    """
    if not os.path.exists(xml_file_path):
        logger.error(f"Evaluation XML file not found: {xml_file_path}")
        return []

    gold_standard_data = []
    try:
        tree = ET.parse(xml_file_path)
        root = tree.getroot()
        for doc_idx, document_node in enumerate(root.findall('.//document')):
            doc_id_node = document_node.find('id')
            doc_id = doc_id_node.text if doc_id_node is not None else f"doc_{doc_idx}"
            
            annotations = document_node.findall('annotation')
            for ann_idx, annotation_node in enumerate(annotations):
                entity_text_node = annotation_node.find('text')
                semantic_tags_node = annotation_node.find('infon[@key="semantic_tags"]')
                ann_id_val = annotation_node.get('id', f"ann_{doc_idx}_{ann_idx}")


                if entity_text_node is not None and semantic_tags_node is not None and entity_text_node.text is not None and semantic_tags_node.text is not None:
                    entity_text = entity_text_node.text.strip()
                    
                    raw_tags = semantic_tags_node.text.strip()
                    true_uris = {tag.strip() for tag in raw_tags.split(';') if tag.strip()}
                    
                    true_curies = set()
                    for uri in true_uris:
                        try:
                            # Adjusted: Use CURIE_PREFIX_MAP
                            curie = uri_to_curie(uri, CURIE_PREFIX_MAP)
                            if curie: 
                                true_curies.add(curie)
                            else:
                                logger.warning(f"Could not convert URI to CURIE: {uri} for entity '{entity_text}' in {doc_id} (ann: {ann_id_val})")
                        except Exception as e:
                            logger.error(f"Error converting URI {uri} to CURIE: {e}")
                    
                    if entity_text and true_curies:
                        gold_standard_data.append({
                            'text': entity_text,
                            'true_curies': true_curies,
                            'doc_id': doc_id,
                            'ann_id': ann_id_val
                        })
                    elif entity_text: # Has text but no convertible true CURIEs
                        logger.warning(f"Entity '{entity_text}' in {doc_id} (ann: {ann_id_val}) had no convertible true CURIEs from URIs: {true_uris}")

                # else: # This can be too verbose if many annotations don't have these specific fields
                #     logger.debug(f"Annotation missing text or semantic_tags in {doc_id}, ann_id={ann_id_val}")


    except ET.ParseError as e:
        logger.error(f"Error parsing XML file {xml_file_path}: {e}")
        return []
    except Exception as e:
        logger.error(f"An unexpected error occurred during XML parsing: {e}", exc_info=True)
        return []
        
    logger.info(f"Successfully parsed {len(gold_standard_data)} entities with text and true CURIEs from {xml_file_path}")
    return gold_standard_data

def evaluate_retriever(retriever: HybridRetriever, gold_standard_data: list, recall_at_k: int, lexical_k: int, vector_k: int) -> tuple[float, int, int]:
    """
    Evaluates the retriever against the gold standard data.

    Args:
        retriever (HybridRetriever): The initialized hybrid retriever.
        gold_standard_data (list): List of gold standard entities and their CURIEs.
        recall_at_k (int): The K value for Recall@K (slice of combined results).
        lexical_k (int): Number of results to fetch from lexical search.
        vector_k (int): Number of results to fetch from vector search.

    Returns:
        tuple: (recall_score, total_entities_processed, hits)
    """
    total_entities_processed = 0
    hits = 0
    
    if not gold_standard_data:
        logger.warning("No gold standard data provided for evaluation.")
        return 0.0, 0, 0

    for i, item in enumerate(gold_standard_data):
        query_text = item['text']
        true_curies = item['true_curies']
        
        if not query_text or not true_curies:
            # This should ideally be filtered by parse_evaluation_xml already
            logger.warning(f"Skipping item with empty query text or true_curies: {item}")
            continue
            
        total_entities_processed += 1
        
        logger.debug(f"({i+1}/{len(gold_standard_data)}) Querying for: '{query_text}', True CURIEs: {true_curies}")

        try:
            # Adjusted: HybridRetriever.search returns a dict: {"lexical_results": [], "vector_results": []}
            retriever_output_dict = retriever.search(
                query_string=query_text,
                lexical_limit=lexical_k,
                vector_k=vector_k
            )
            
            lexical_results = retriever_output_dict.get("lexical_results", [])
            vector_results = retriever_output_dict.get("vector_results", [])

            # Combine and deduplicate results, lexical first then vector.
            # Scores are not comparable, so this is a simple merge strategy.
            # The 'id' field from result dicts contains the CURIE.
            combined_ordered_results = []
            seen_ids = set()

            for doc in lexical_results:
                doc_id = doc.get('id') # 'id' is the CURIE
                if doc_id and doc_id not in seen_ids:
                    combined_ordered_results.append(doc) # doc contains 'id', 'label', 'score', etc.
                    seen_ids.add(doc_id)
            
            for doc in vector_results:
                doc_id = doc.get('id') # 'id' is the CURIE
                if doc_id and doc_id not in seen_ids:
                    combined_ordered_results.append(doc)
                    seen_ids.add(doc_id)
            
            # Extract the CURIEs from the top `recall_at_k` combined documents
            # Adjusted: use doc['id'] as it stores the CURIE
            retrieved_curies_set = {doc['id'] for doc in combined_ordered_results[:recall_at_k]}
            logger.debug(f"Retrieved CURIEs (top {recall_at_k} from combined): {retrieved_curies_set}")

            if not true_curies.isdisjoint(retrieved_curies_set):
                hits += 1
                logger.info(f"HIT! Query: '{query_text}'. True: {true_curies}. Found in top {recall_at_k}: {true_curies.intersection(retrieved_curies_set)}")
            else:
                logger.info(f"MISS. Query: '{query_text}'. True: {true_curies}. Top {recall_at_k} (combined) CURIEs: {retrieved_curies_set}")
                # For misses, log more details if needed, e.g. full combined_ordered_results
                # logger.debug(f"Full combined/ordered results for miss: {combined_ordered_results}")


        except Exception as e:
            logger.error(f"Error during retrieval or processing for query '{query_text}': {e}", exc_info=True)
            
    if total_entities_processed == 0:
        logger.warning("No valid entities were processed for evaluation.")
        return 0.0, 0, 0
        
    recall_score = hits / total_entities_processed
    return recall_score, total_entities_processed, hits

def main():
    logger.info("Starting Retriever Evaluation Script...")

    # 1. Parse the Gold Standard XML
    logger.info(f"Loading gold standard data from: {EVALUATION_XML_FILE}")
    gold_standard_data = parse_evaluation_xml(EVALUATION_XML_FILE)
    if not gold_standard_data:
        logger.error("Failed to load or parse gold standard data. Exiting.")
        return

    # 2. Initialize the HybridRetriever
    logger.info("Initializing HybridRetriever...")
    try:
        # Ensure all paths are valid and files exist where expected by HybridRetriever
        # Adjusted: Use ONTOLOGY_DUMP_JSON for ontology_data_path
        if not os.path.exists(ONTOLOGY_DUMP_JSON):
            logger.error(f"Ontology dump not found: {ONTOLOGY_DUMP_JSON}. Run ingestion pipeline.")
            return
        if not os.path.exists(WHOOSH_INDEX_DIR) or not os.listdir(WHOOSH_INDEX_DIR): # Check if dir exists and is not empty
             logger.error(f"Whoosh index directory is empty or not found: {WHOOSH_INDEX_DIR}. Run ingestion pipeline.")
             return
        if not os.path.exists(FAISS_INDEX_PATH):
            logger.error(f"FAISS index not found: {FAISS_INDEX_PATH}. Run ingestion pipeline.")
            return
        if not os.path.exists(FAISS_METADATA_PATH):
            logger.error(f"FAISS metadata not found: {FAISS_METADATA_PATH}. Run ingestion pipeline.")
            return

        retriever = HybridRetriever(
            ontology_data_path=ONTOLOGY_DUMP_JSON, # Adjusted
            whoosh_index_dir=WHOOSH_INDEX_DIR,
            faiss_index_path=FAISS_INDEX_PATH,
            faiss_metadata_path=FAISS_METADATA_PATH,
            embedding_model_name=EMBEDDING_MODEL_NAME
        )
        logger.info("HybridRetriever initialized successfully.")
    except Exception as e:
        logger.error(f"Failed to initialize HybridRetriever: {e}", exc_info=True)
        return

    # 3. Perform Evaluation
    logger.info(f"Starting evaluation with Recall@{RECALL_AT_K}...")
    # Adjusted: Use DEFAULT_K_LEXICAL and DEFAULT_K_VECTOR
    logger.info(f"HybridRetriever search params: Lexical K={DEFAULT_K_LEXICAL}, Vector K={DEFAULT_K_VECTOR}")
    
    recall_score, total_entities, hits = evaluate_retriever(
        retriever, 
        gold_standard_data, 
        recall_at_k=RECALL_AT_K,
        lexical_k=DEFAULT_K_LEXICAL, # Adjusted
        vector_k=DEFAULT_K_VECTOR    # Adjusted
    )

    # 4. Print Results
    logger.info("--- Evaluation Results ---")
    logger.info(f"Total entities processed: {total_entities}")
    logger.info(f"Number of hits (at least one true CURIE found in top {RECALL_AT_K} combined results): {hits}")
    if total_entities > 0:
        logger.info(f"Recall@{RECALL_AT_K}: {recall_score:.4f}")
    else:
        logger.info("Recall not calculated as no entities were processed.")

    # 5. Clean up
    try:
        if hasattr(retriever, 'close') and callable(retriever.close):
            retriever.close()
            logger.info("Retriever resources closed.")
    except Exception as e:
        logger.error(f"Error closing retriever resources: {e}")

if __name__ == "__main__":
    eval_dir = os.path.dirname(EVALUATION_XML_FILE)
    if not os.path.exists(eval_dir):
        try:
            os.makedirs(eval_dir)
            logger.info(f"Created directory: {eval_dir}")
            logger.info(f"Please place '{os.path.basename(EVALUATION_XML_FILE)}' in {eval_dir} to run the evaluation.")
        except OSError as e:
            logger.error(f"Failed to create directory {eval_dir}: {e}")
            sys.exit(1) # Exit if cannot create data directory for eval file
    
    if not os.path.exists(EVALUATION_XML_FILE):
        logger.error(f"Evaluation XML file '{EVALUATION_XML_FILE}' not found. Please place it in the correct directory.")
    else:
        main()
--- END File: src/evaluation/evaluate_retriever_recall.py ---

--- File: src/evaluation/evaluate_pipeline.py ---
# src/evaluation/evaluate_pipeline.py

import xml.etree.ElementTree as ET
import os
import sys
import logging
import json
from typing import Tuple, List, Dict

# Add project root to Python path to allow direct imports from src
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(PROJECT_ROOT)

from src.retriever.hybrid_retriever import HybridRetriever
from src.rag_selectors.ollama_selector import OllamaSelector # Import the new selector
from src.rag_selectors.gemini_selector import GeminiSelector # Import the old selector for reference

from src.config import (
    ONTOLOGY_DUMP_JSON,
    WHOOSH_INDEX_DIR,
    FAISS_INDEX_PATH,
    FAISS_METADATA_PATH,
    EMBEDDING_MODEL_NAME,
    CURIE_PREFIX_MAP,
    DEFAULT_K_LEXICAL,
    DEFAULT_K_VECTOR,
    OLLAMA_SELECTOR_MODEL_NAME # Import the model name
)
from src.utils.ontology_utils import uri_to_curie

# --- Configuration for this specific evaluation script ---
EVALUATION_XML_FILE = os.path.join(PROJECT_ROOT, "data", "CafeteriaFCD_foodon_unique.xml")
# File to save detailed results of incorrect selections
EVALUATION_OUTPUT_FILE = os.path.join(PROJECT_ROOT, "evaluation_results.json")

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Re-used from evaluate_retriever_recall.py (no changes needed) ---
def parse_evaluation_xml(xml_file_path: str) -> list:
    """
    Parses the evaluation XML file to extract entities and their ground truth semantic tags.
    (This function is identical to the one in evaluate_retriever_recall.py)
    """
    if not os.path.exists(xml_file_path):
        logger.error(f"Evaluation XML file not found: {xml_file_path}")
        return []

    gold_standard_data = []
    try:
        tree = ET.parse(xml_file_path)
        root = tree.getroot()
        for doc_idx, document_node in enumerate(root.findall('.//document')):
            doc_id_node = document_node.find('id')
            doc_id = doc_id_node.text if doc_id_node is not None else f"doc_{doc_idx}"
            
            for ann_idx, annotation_node in enumerate(document_node.findall('annotation')):
                entity_text_node = annotation_node.find('text')
                semantic_tags_node = annotation_node.find('infon[@key="semantic_tags"]')
                ann_id_val = annotation_node.get('id', f"ann_{doc_idx}_{ann_idx}")

                if entity_text_node is not None and semantic_tags_node is not None and entity_text_node.text is not None and semantic_tags_node.text is not None:
                    entity_text = entity_text_node.text.strip()
                    raw_tags = semantic_tags_node.text.strip()
                    true_uris = {tag.strip() for tag in raw_tags.split(';') if tag.strip()}
                    true_curies = {uri_to_curie(uri, CURIE_PREFIX_MAP) for uri in true_uris} - {None}
                    
                    if entity_text and true_curies:
                        gold_standard_data.append({
                            'text': entity_text,
                            'true_curies': true_curies,
                            'doc_id': doc_id,
                            'ann_id': ann_id_val
                        })
    except ET.ParseError as e:
        logger.error(f"Error parsing XML file {xml_file_path}: {e}")
        return []
    
    logger.info(f"Successfully parsed {len(gold_standard_data)} entities from {xml_file_path}")
    return gold_standard_data

# --- New Evaluation Logic for the Full Pipeline ---
def evaluate_full_pipeline(
    retriever: HybridRetriever, 
    selector: GeminiSelector,  #
    gold_standard_data: list, 
    lexical_k: int, 
    vector_k: int
) -> Tuple[float, int, int, int, int, List[Dict]]:
    """
    Evaluates the full retrieval and selection pipeline against the gold standard data.

    Returns:
        A tuple containing:
        - accuracy_score (float)
        - total_entities_processed (int)
        - correct_selections (int)
        - retrieval_failures (int): Count of queries that returned no candidates.
        - selection_failures (int): Count of queries where the selector failed.
        - incorrect_selections (list): A list of dictionaries detailing the misses.
    """
    total_entities_processed = 0
    correct_selections = 0
    retrieval_failures = 0
    selection_failures = 0
    incorrect_selections = []

    if not gold_standard_data:
        logger.warning("No gold standard data provided for evaluation.")
        return 0.0, 0, 0, 0, 0, []

    for i, item in enumerate(gold_standard_data):
        query_text = item['text']
        true_curies = item['true_curies']
        
        total_entities_processed += 1
        logger.info(f"--- Processing ({i+1}/{len(gold_standard_data)}): '{query_text}' (True: {true_curies}) ---")

        # 1. RETRIEVAL STEP: Get candidates
        try:
            # Note: The HybridRetriever's search method should return a combined, reranked list of candidates.
            # We adapt to the provided retriever's output format.
            retriever_output_dict = retriever.search(
                query_string=query_text, lexical_limit=lexical_k, vector_k=vector_k
            )
            lexical_results = retriever_output_dict.get("lexical_results", [])
            vector_results = retriever_output_dict.get("vector_results", [])

            # Combine and deduplicate
            candidates = []
            seen_ids = set()
            for doc in lexical_results + vector_results:
                doc_id = doc.get('id')
                if doc_id and doc_id not in seen_ids:
                    candidates.append(doc)
                    seen_ids.add(doc_id)
            
            if not candidates:
                logger.warning(f"Retrieval Failure: No candidates found for '{query_text}'.")
                retrieval_failures += 1
                continue

        except Exception as e:
            logger.error(f"Error during retrieval for query '{query_text}': {e}", exc_info=True)
            retrieval_failures += 1
            continue

        # 2. SELECTION STEP: Use OllamaSelector
        try:
            selection_result = selector.select_best_term(query=query_text, candidates=candidates)

            if not selection_result or "chosen_id" not in selection_result:
                logger.warning(f"Selection Failure: Selector did not return a valid choice for '{query_text}'.")
                selection_failures += 1
                continue
            
            chosen_curie = selection_result["chosen_id"]

        except Exception as e:
            logger.error(f"Error during selection for query '{query_text}': {e}", exc_info=True)
            selection_failures += 1
            continue
        
        # 3. COMPARISON STEP
        if chosen_curie in true_curies:
            correct_selections += 1
            logger.info(f"âœ… HIT! Query: '{query_text}'. Chosen: '{chosen_curie}'. Correct.")
        else:
            logger.info(f"âŒ MISS! Query: '{query_text}'. Chosen: '{chosen_curie}', Expected: {true_curies}.")
            incorrect_selections.append({
                "query": query_text,
                "chosen_curie": chosen_curie,
                "true_curies": list(true_curies),
                "explanation": selection_result.get("explanation", "N/A"),
                "candidates_provided": [cand['id'] for cand in candidates]
            })

    if (total_entities_processed - retrieval_failures - selection_failures) == 0:
        accuracy_score = 0.0
    else:
        # Accuracy is based on the number of times the selector could make a choice
        accuracy_score = correct_selections / (total_entities_processed - retrieval_failures - selection_failures)

    return accuracy_score, total_entities_processed, correct_selections, retrieval_failures, selection_failures, incorrect_selections

def main():
    logger.info("Starting Full Pipeline Evaluation Script...")
    
    # 1. Check for necessary model name in config
    if not OLLAMA_SELECTOR_MODEL_NAME:
        logger.error("OLLAMA_SELECTOR_MODEL_NAME is not set in config.py. Exiting.")
        return

    # 2. Parse Gold Standard XML
    logger.info(f"Loading gold standard data from: {EVALUATION_XML_FILE}")
    gold_standard_data = parse_evaluation_xml(EVALUATION_XML_FILE)
    if not gold_standard_data:
        logger.error("Failed to load or parse gold standard data. Exiting.")
        return

    # 3. Initialize Pipeline Components
    try:
        logger.info("Initializing HybridRetriever...")
        retriever = HybridRetriever(
            ontology_data_path=ONTOLOGY_DUMP_JSON,
            whoosh_index_dir=WHOOSH_INDEX_DIR,
            faiss_index_path=FAISS_INDEX_PATH,
            faiss_metadata_path=FAISS_METADATA_PATH,
            embedding_model_name=EMBEDDING_MODEL_NAME
        )
        logger.info("HybridRetriever initialized successfully.")

        logger.info(f"Initializing OllamaSelector with model '{OLLAMA_SELECTOR_MODEL_NAME}'...")
        selector = GeminiSelector(retriever=retriever)
        logger.info("OllamaSelector initialized successfully.")

    except Exception as e:
        logger.error(f"Failed to initialize pipeline components: {e}", exc_info=True)
        return

    # 4. Perform Evaluation
    logger.info(
        f"Starting evaluation with Retriever(lexical_k={DEFAULT_K_LEXICAL}, vector_k={DEFAULT_K_VECTOR}) "
        f"and Selector(model={OLLAMA_SELECTOR_MODEL_NAME})"
    )
    
    accuracy, total, correct, ret_fails, sel_fails, misses = evaluate_full_pipeline(
        retriever, 
        selector, 
        gold_standard_data, 
        lexical_k=DEFAULT_K_LEXICAL,
        vector_k=DEFAULT_K_VECTOR
    )

    # 5. Print and Save Results
    logger.info("--- Evaluation Complete ---")
    logger.info(f"Total entities evaluated: {total}")
    logger.info(f"Retrieval Failures (no candidates): {ret_fails}")
    logger.info(f"Selection Failures (LLM error): {sel_fails}")
    logger.info("-" * 27)
    valid_attempts = total - ret_fails - sel_fails
    logger.info(f"Valid attempts for selector: {valid_attempts}")
    logger.info(f"Correct selections (Hits): {correct}")
    if valid_attempts > 0:
        logger.info(f"Accuracy: {accuracy:.4f} ({correct}/{valid_attempts})")
    else:
        logger.info("Accuracy: N/A (no valid attempts were made)")
    
    logger.info(f"Saving {len(misses)} incorrect selections to {EVALUATION_OUTPUT_FILE}")
    with open(EVALUATION_OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(misses, f, indent=4)
        
    logger.info("Evaluation finished.")

if __name__ == "__main__":
    if not os.path.exists(EVALUATION_XML_FILE):
        logger.error(f"Evaluation XML file '{EVALUATION_XML_FILE}' not found.")
    else:
        main()
--- END File: src/evaluation/evaluate_pipeline.py ---

--- File: src/rag_selectors/__init__.py ---

--- END File: src/rag_selectors/__init__.py ---

--- File: src/rag_selectors/gemini_selector.py ---
# src/selectors/gemini_selector.py
import os
import logging
import json
from typing import List, Dict, Any, Optional

from google import genai
from google.api_core import exceptions  # <-- Import for better error handling

from src.retriever.hybrid_retriever import HybridRetriever
from src import config

logger = logging.getLogger(__name__)

class GeminiSelector:
    """
    Uses the Google Gemini model to select the best ontology term
    from a list of candidates.
    """
    def __init__(self, retriever: HybridRetriever):
        """
        Initializes the GeminiSelector.

        Args:
            retriever (HybridRetriever): An initialized retriever instance,
                                         used to fetch full details of candidate terms.
        """
        self.retriever = retriever
        self.prompt_template = self._load_prompt_template()

        if not config.GEMINI_API_KEY:
            raise ValueError("GEMINI_API_KEY not found in environment variables.")
        
        # --- CORRECTED PART 1: Client Instantiation ---
        # Instead of genai.configure(), we instantiate a client.
        self.client = genai.Client(api_key=config.GEMINI_API_KEY)
        self.model_name = config.LLM_SELECTOR_MODEL_NAME
        
        logger.info(f"GeminiSelector initialized for model: {self.model_name}")

    def _load_prompt_template(self) -> str:
        """Loads the prompt template from the file."""
        template_path = os.path.join(config.PROJECT_ROOT, "prompts", "final_selection.tpl")
        try:
            with open(template_path, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            logger.error(f"Prompt template not found at {template_path}")
            raise

    def _format_candidates_for_prompt(self, candidates: List[Dict[str, Any]]) -> str:
        """Formats the list of candidate documents into a string for the prompt."""
        formatted_list = []
        for i, candidate in enumerate(candidates):
            term_id = candidate.get("id")
            if not term_id:
                continue
            
            # Fetch full details using the retriever
            details = self.retriever.get_term_details(term_id)
            if not details:
                continue

            # Format the details for display
            label = details.get("label", "N/A")
            definition = details.get("definition", "No definition available.")
            synonyms = "; ".join(details.get("synonyms", [])) or "None"

            formatted_list.append(
                f"{i+1}. ID: {term_id}\n"
                f"   Label: {label}\n"
                f"   Definition: {definition}\n"
                f"   Synonyms: {synonyms}"
            )
        return "\n\n".join(formatted_list)

    def select_best_term(self, query: str, candidates: List[Dict[str, Any]]) -> Optional[Dict[str, str]]:
        """
        Prompts Gemini to select the best term and parses the JSON response.

        Args:
            query (str): The original user query.
            candidates (List[Dict[str, Any]]): The list of candidate documents.

        Returns:
            A dictionary with {"chosen_id": str, "explanation": str}, or None on failure.
        """
        if not candidates:
            return None

        candidate_str = self._format_candidates_for_prompt(candidates)
        prompt = self.prompt_template.replace("[USER_ENTITY]", query).replace("[CANDIDATE_LIST]", candidate_str)
        
        logger.info(f"Sending request to Gemini for query: '{query}'")
        try:
            # --- CORRECTED PART 2: The API Call ---
            # Call generate_content on the client.models service.
            # Pass the prompt string to the 'contents' parameter.
            generation_config = {'temperature': 0}
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=generation_config
            )

            # The rest of your logic is good.
            # Safety feedback check from your example code is a good practice to add here if needed.
            feedback = getattr(response, 'prompt_feedback', None)
            if feedback and any(r.blocked for r in feedback.safety_ratings or []):
                logger.warning(f"Request for query '{query}' was blocked by safety filters.")
                return None

            # Clean up the response text to extract the JSON part
            cleaned_response = response.text.strip().lstrip("```json").rstrip("```").strip()
            
            # Parse the JSON response
            result = json.loads(cleaned_response)
            if "chosen_id" in result and "explanation" in result:
                return result
            else:
                logger.error(f"LLM response is valid JSON but missing required keys: {result}")
                return None
                
        except json.JSONDecodeError:
            logger.error(f"Failed to decode JSON from LLM response: {response.text}")
            return None
        except exceptions.GoogleAPIError as e:  # <-- Specific API error handling
            logger.error(f"A Google API error occurred with the Gemini call: {e}", exc_info=True)
            return None
        except Exception as e:
            logger.error(f"An unexpected error occurred with the Gemini API call: {e}", exc_info=True)
            return None
--- END File: src/rag_selectors/gemini_selector.py ---

--- File: src/rag_selectors/ollama_selector.py ---
# src/selectors/ollama_selector.py
import os
import logging
import json
from typing import List, Dict, Any, Optional

import ollama # <-- New import

from src.retriever.hybrid_retriever import HybridRetriever
from src import config

logger = logging.getLogger(__name__)

class OllamaSelector:
    """
    Uses a local LLM via Ollama to select the best ontology term
    from a list of candidates.
    """
    def __init__(self, retriever: HybridRetriever):
        """
        Initializes the OllamaSelector.

        Args:
            retriever (HybridRetriever): An initialized retriever instance,
                                         used to fetch full details of candidate terms.
        """
        self.retriever = retriever
        self.prompt_template = self._load_prompt_template()
        
        # We get the model name from config, but no API key or client is needed for Ollama.
        self.model_name = config.OLLAMA_SELECTOR_MODEL_NAME # Assumes you have this in your config
        
        # You might want to add a check here to ensure the Ollama service is running.
        try:
            ollama.ps()
            logger.info(f"Ollama service is running. Selector initialized for model: {self.model_name}")
        except Exception:
            logger.error("Ollama service not detected. Please ensure Ollama is running.")
            raise ConnectionError("Ollama service not available.")


    def _load_prompt_template(self) -> str:
        """Loads the prompt template from the file."""
        # This method is unchanged
        template_path = os.path.join(config.PROJECT_ROOT, "prompts", "final_selection.tpl")
        try:
            with open(template_path, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            logger.error(f"Prompt template not found at {template_path}")
            raise

    def _format_candidates_for_prompt(self, candidates: List[Dict[str, Any]]) -> str:
        """Formats the list of candidate documents into a string for the prompt."""
        # This method is unchanged
        formatted_list = []
        for i, candidate in enumerate(candidates):
            term_id = candidate.get("id")
            if not term_id:
                continue
            
            details = self.retriever.get_term_details(term_id)
            if not details:
                continue

            label = details.get("label", "N/A")
            definition = details.get("definition", "No definition available.")
            synonyms = "; ".join(details.get("synonyms", [])) or "None"

            formatted_list.append(
                f"{i+1}. ID: {term_id}\n"
                f"   Label: {label}\n"
                f"   Definition: {definition}\n"
                f"   Synonyms: {synonyms}"
            )
        return "\n\n".join(formatted_list)

    def select_best_term(self, query: str, candidates: List[Dict[str, Any]]) -> Optional[Dict[str, str]]:
        """
        Prompts Ollama to select the best term and parses the JSON response.

        Args:
            query (str): The original user query.
            candidates (List[Dict[str, Any]]): The list of candidate documents.

        Returns:
            A dictionary with {"chosen_id": str, "explanation": str}, or None on failure.
        """
        if not candidates:
            return None

        candidate_str = self._format_candidates_for_prompt(candidates)
        # The full prompt is created just like before.
        full_prompt = self.prompt_template.replace("[USER_ENTITY]", query).replace("[CANDIDATE_LIST]", candidate_str)
        
        logger.info(f"Sending request to Ollama for query: '{query}' with model '{self.model_name}'")
        try:
            # --- CORRECTED OLLAMA CALL ---
            # The entire prompt goes into a single 'user' message.
            # We use format='json' to ensure the output is valid JSON.
            response = ollama.chat(
                model=self.model_name,
                messages=[
                    {
                        'role': 'user',
                        'content': full_prompt,
                    },
                ],
                format='json' # This is a key feature to get structured output!
            )

            # The response content should be a JSON string.
            response_content = response['message']['content']
            
            # Parse the JSON response
            result = json.loads(response_content)
            
            if "chosen_id" in result and "explanation" in result:
                return result
            else:
                logger.error(f"LLM response is valid JSON but missing required keys: {result}")
                return None
                
        except json.JSONDecodeError:
            # This is less likely with format='json' but is good practice to keep.
            logger.error(f"Failed to decode JSON from Ollama response: {response_content}")
            return None
        except ollama.ResponseError as e:
            logger.error(f"An error occurred with the Ollama API call: {e.status_code} - {e.error}")
            return None
        except Exception as e:
            logger.error(f"An unexpected error occurred during the Ollama call: {e}", exc_info=True)
            return None
--- END File: src/rag_selectors/ollama_selector.py ---

