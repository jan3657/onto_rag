"project_summary":
  "project_name": |-
    onto_rag
  "readme_content": |
    # onto_rag
    A Retrieval Augmented Generation system for matching extracted entities to ontologies
  "key_files":
    ".gitignore": |-
      # Byte-compiled / cache
      __pycache__/
      *.py[cod]
      .pytest_cache/

      # Environment & secrets
      .env
      api_key.json

      # VSCode, macOS
      .vscode/
      .DS_Store

      # Large generated data and ontology files
      /data/
      /ontologies/
      /models/
    "requirements.txt": |-
      # onto_rag/requirements.txt

      # Core Ontology Processing
      rdflib>=6.0.0

      # Lexical Indexing
      whoosh>=2.7.0

      # Embeddings & Semantic Search
      sentence-transformers>=2.2.0
      faiss-cpu>=1.7.0   # Or faiss-gpu if you have a CUDA-enabled GPU and want to use it

      # Utilities
      numpy>=1.21.0     # Often a dependency, good to specify
      pandas>=1.3.0     # Dependency for libraries like 'datasets' used by sentence-transformers
      python-dotenv>=0.19.0 # For loading .env files
      tqdm>=4.60.0      # For progress bars in long operations

      # Testing (optional, but good for development)
      pytest>=6.2.0
    "src/config.py": "# src/config.py\nimport os\nfrom dotenv import load_dotenv\n\
      from rdflib import Namespace # This is fine, though rdflib.Namespace is not\
      \ directly used for string constants below.\n\n# Project Root Directory\nPROJECT_ROOT\
      \ = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \".\"))\n\
      # print(f\"Project root directory: {PROJECT_ROOT}\") # Keep for debugging if\
      \ you like\n\n# Load environment variables from .env file\nload_dotenv(os.path.join(PROJECT_ROOT,\
      \ \".env\"))\n\n# Data Directory (for ontology dump, indexes, etc.)\nDATA_DIR\
      \ = os.path.join(PROJECT_ROOT, \"data\")\nos.makedirs(DATA_DIR, exist_ok=True)\n\
      \n# Ontologies Directory\nONTOLOGIES_DIR = os.path.join(PROJECT_ROOT, \"ontologies\"\
      )\nFOODON_PATH = os.path.join(ONTOLOGIES_DIR, \"foodon-with-syns.owl\")\nCHEBI_PATH\
      \ = os.path.join(ONTOLOGIES_DIR, \"chebi.owl\")\nTEST_FOODON_SNIPPET_PATH =\
      \ os.path.join(ONTOLOGIES_DIR, \"test_foodon_snippet.owl\")\n\n\n# Output file\
      \ from parse_ontology.py\nONTOLOGY_DUMP_JSON = os.path.join(DATA_DIR, \"ontology_syns_dump.json\"\
      )\n\n# Output file for enriched documents\nENRICHED_DOCUMENTS_FILE = os.path.join(DATA_DIR,\
      \ \"enriched_documents.json\")\n\n# Output file for embeddings (used for building\
      \ FAISS index)\nEMBEDDINGS_FILE = os.path.join(DATA_DIR, \"embeddings.json\"\
      )\n\n# Whoosh Index Directory\nWHOOSH_INDEX_DIR = os.path.join(DATA_DIR, \"\
      whoosh_index\")\nos.makedirs(WHOOSH_INDEX_DIR, exist_ok=True)\n\n# FAISS Index\
      \ Paths\nFAISS_INDEX_PATH = os.path.join(DATA_DIR, \"faiss_index.bin\")\nFAISS_METADATA_PATH\
      \ = os.path.join(DATA_DIR, \"faiss_metadata.json\")\n\n# Embedding Model Configuration\n\
      EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n#EMBEDDING_MODEL_NAME = \"Lajavaness/bilingual-embedding-large\"\
      \ \n# RERANKER_MODEL_NAME = \"intfloat/e5-mistral-7b-instruct\"\nRERANKER_MODEL_NAME\
      \ = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\nEMBEDDING_BATCH_SIZE = 32 # For\
      \ batch embedding script\nEMBEDDING_DEVICE = 'cpu'  # or 'cuda' if available,\
      \ for embedding script\n\n# Default K values for retrieval\nDEFAULT_K_LEXICAL\
      \ = 20\nDEFAULT_K_VECTOR = 20\nDEFAULT_RERANK_K = DEFAULT_K_LEXICAL + DEFAULT_K_VECTOR\n\
      \n# Namespaces (using string constants for broader compatibility if rdflib not\
      \ always imported)\nRDFS_NS_STR = \"http://www.w3.org/2000/01/rdf-schema#\"\n\
      RDF_NS_STR = \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\nOWL_NS_STR = \"\
      http://www.w3.org/2002/07/owl#\"\nSKOS_NS_STR = \"http://www.w3.org/2004/02/skos/core#\"\
      \nOBO_NS_STR = \"http://purl.obolibrary.org/obo/\"\nOBOINOWL_NS_STR = \"http://www.geneontology.org/formats/oboInOwl#\"\
      \nIAO_NS_STR = \"http://purl.obolibrary.org/obo/IAO_\"\n\n# For rdflib usage\
      \ where Namespace objects are preferred:\nRDFS_NS = Namespace(RDFS_NS_STR)\n\
      RDF_NS = Namespace(RDF_NS_STR)\nOWL_NS = Namespace(OWL_NS_STR)\nSKOS_NS = Namespace(SKOS_NS_STR)\n\
      OBO_NS = Namespace(OBO_NS_STR)\nOBOINOWL_NS = Namespace(OBOINOWL_NS_STR)\nIAO_NS\
      \ = Namespace(IAO_NS_STR)\n\n\n# Mapping of common relation URIs/CURIEs to human-readable\
      \ names and default prefixes\n# Used by parse_ontology.py and potentially enrich_documents.py\n\
      RELATION_CONFIG = {\n    \"obo:BFO_0000050\": {\"label\": \"part of\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0001000\": {\"label\": \"derives from\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0002200\": {\"label\": \"has phenotype\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0002451\": {\"label\": \"has part\", \"prefix\": \"\
      obo\"},\n    \"obo:FOODON_0000246\": {\"label\": \"has ingredient\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000056\": {\"label\": \"participates in\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000057\": {\"label\": \"has participant\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000085\": {\"label\": \"functionally related to\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002090\": {\"label\": \"adjacent to\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002131\": {\"label\": \"overlaps\", \"\
      prefix\": \"obo\"},\n    \"obo:RO_0002150\": {\"label\": \"connected to\", \"\
      prefix\": \"obo\"},\n    \"obo:RO_0002202\": {\"label\": \"develops from\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002350\": {\"label\": \"member of\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002351\": {\"label\": \"has member\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002353\": {\"label\": \"output of\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002440\": {\"label\": \"causally related\
      \ to\", \"prefix\": \"obo\"},\n    \"obo:RO_0002450\": {\"label\": \"contains\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002500\": {\"label\": \"surrounds\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002573\": {\"label\": \"has quality\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002606\": {\"label\": \"is transformation\
      \ of\", \"prefix\": \"obo\"},\n    \"obo:RO_0002607\": {\"label\": \"has input\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0003000\": {\"label\": \"used in\", \"\
      prefix\": \"obo\"},\n    \"obo:FOODON_0000380\": {\"label\": \"has preparation\
      \ method\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000226\": {\"label\": \"\
      has food source\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000388\": {\"label\"\
      : \"has maturity state\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000286\"\
      : {\"label\": \"has packaging\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000240\"\
      : {\"label\": \"has preservation method\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000440\"\
      : {\"label\": \"has physical state\", \"prefix\": \"obo\"},\n    # \"obo:FOODON_\"\
      : {\"label\": \"FoodON specific relation\", \"prefix\": \"obo\"}, # This generic\
      \ one might be too broad\n    \"obo:ERO_0000039\": {\"label\": \"has nutrient\"\
      , \"prefix\": \"obo\"},\n    \"obo:ERO_0000589\": {\"label\": \"dietary context\
      \ of\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C25277\": {\"label\": \"is allergen\
      \ of\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C48670\": {\"label\": \"has active\
      \ ingredient\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C64548\": {\"label\":\
      \ \"part of human diet\", \"prefix\": \"obo\"},\n    \"obo:PATO_0000001\": {\"\
      label\": \"has quality PATO\", \"prefix\": \"obo\"},\n}\n\nTARGET_RELATIONS_CURIES\
      \ = list(RELATION_CONFIG.keys())\n\n# CURIE Prefix Map for uri_to_curie and\
      \ curie_to_uri conversions\n# Ensure the keys are the *base URIs* that prefixes\
      \ are expected for.\nCURIE_PREFIX_MAP = {\n    \"http://purl.obolibrary.org/obo/FOODON_\"\
      : \"FOODON\",\n    \"http://purl.obolibrary.org/obo/BFO_\": \"BFO\",\n    \"\
      http://purl.obolibrary.org/obo/RO_\": \"RO\",\n    \"http://purl.obolibrary.org/obo/ERO_\"\
      : \"ERO\",\n    \"http://purl.obolibrary.org/obo/NCIT_\": \"NCIT\",\n    \"\
      http://purl.obolibrary.org/obo/PATO_\": \"PATO\",\n    \"http://purl.obolibrary.org/obo/IAO_\"\
      : \"IAO\",\n    \"http://www.w3.org/2000/01/rdf-schema#\": \"rdfs\",\n    \"\
      http://www.w3.org/1999/02/22-rdf-syntax-ns#\": \"rdf\",\n    \"http://www.w3.org/2002/07/owl#\"\
      : \"owl\",\n    \"http://www.w3.org/2004/02/skos/core#\": \"skos\",\n    \"\
      http://www.geneontology.org/formats/oboInOwl#\": \"oboInOwl\",\n    \"http://purl.obolibrary.org/obo/HANCESTRO_\"\
      : \"HANCESTRO\",  \n    \"http://purl.obolibrary.org/obo/GAZ_\": \"GAZ\",\n\
      \    \"http://purl.obolibrary.org/obo/CHEBI_\": \"CHEBI\",\n    \"http://purl.obolibrary.org/obo/NCBITaxon_\"\
      : \"NCBITaxon\",\n    \"http://purl.obolibrary.org/obo/UBERON_\": \"UBERON\"\
      ,\n    \"http://purl.obolibrary.org/obo/ENVO_\": \"ENVO\",\n    \"http://purl.obolibrary.org/obo/HP_\"\
      : \"HP\",\n    \"http://purl.obolibrary.org/obo/GO_\": \"GO\",\n    # General\
      \ OBO prefix - should be last or handled carefully to avoid overly broad matches\n\
      \    # if specific OBO sub-ontologies are listed above.\n    \"http://purl.obolibrary.org/obo/\"\
      : \"obo\",\n}\n\n# LLM API Key (placeholders)\nGEMINI_API_KEY = os.getenv(\"\
      GEMINI_API_KEY\")\nLLM_SELECTOR_MODEL_NAME = \"gemini-1.5-flash-latest\"\nOLLAMA_SELECTOR_MODEL_NAME\
      \ = 'llama3.1:8b'\n# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Logging\
      \ configuration\nLOG_LEVEL = \"INFO\"\nLOG_FILE = os.path.join(PROJECT_ROOT,\
      \ \"app.log\") # Example log file in project root"
    "src/main.py": |-
      # src/main.py
      import argparse
      import json
      import os
      import sys

      # --- Add project root to sys.path ---
      PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
      if PROJECT_ROOT not in sys.path:
          sys.path.insert(0, PROJECT_ROOT)

      from src.pipeline.ollama_pipeline import RAGPipeline
      from src.config import DEFAULT_K_LEXICAL, DEFAULT_K_VECTOR

      def main():
          parser = argparse.ArgumentParser(description="Run the Onto-RAG pipeline with LLM selection.")
          parser.add_argument("query", type=str, help="The entity or text to search for (e.g., 'raw milk').")
          parser.add_argument("--lexical_k", type=int, default=DEFAULT_K_LEXICAL, help=f"Initial candidates from lexical search (default: {DEFAULT_K_LEXICAL}).")
          parser.add_argument("--vector_k", type=int, default=DEFAULT_K_VECTOR, help=f"Initial candidates from vector search (default: {DEFAULT_K_VECTOR}).")
          parser.add_argument("--top_n_rerank", type=int, default=10, help="Number of candidates to rerank and pass to the LLM (default: 10).")
          args = parser.parse_args()

          pipeline = None
          try:
              pipeline = RAGPipeline()
              final_result = pipeline.run(
                  query=args.query,
                  lexical_k=args.lexical_k,
                  vector_k=args.vector_k,
                  rerank_top_n=args.top_n_rerank
              )

              print("\n--- Final LLM Selection ---")
              if not final_result:
                  print("Could not determine a matching ontology term.")
              else:
                  print(f"Query: '{args.query}'")
                  print("---------------------------")
                  print(f"Chosen Term ID: {final_result.get('id', 'N/A')}")
                  print(f"Label:          {final_result.get('label', 'N/A')}")
                  print(f"Definition:     {final_result.get('definition', 'N/A')}")
                  print(f"Synonyms:       {'; '.join(final_result.get('synonyms', [])) or 'None'}")
                  print("\nLLM Explanation:")
                  print(f"  > {final_result.get('explanation', 'No explanation provided.')}")
              print("---------------------------\n")

          except Exception as e:
              print(f"\nAn error occurred during the pipeline execution: {e}", file=sys.stderr)
          finally:
              if pipeline:
                  pipeline.close()

      if __name__ == "__main__":
          main()
  "directory_structure": |2-
        ├── .devcontainer/ (excluded)
        ├── .pytest_cache/ (excluded)
        ├── data/ (excluded)
        ├── docs/ (excluded)
        ├── models/ (excluded)
        ├── ontologies/ (excluded)
        ├── prompts/
        ├── scripts/
        ├── src/
        ├── .DS_Store (excluded)
        ├── .env (excluded)
        ├── .env.example
        ├── .gitignore
        ├── README.md (excluded)
        ├── evaluation_results_gemini_1.5-flash.json (excluded)
        ├── evaluation_results_llama.json (excluded)
        ├── export_files_contents.py (excluded)
        ├── project_contents_for_llm.txt (excluded)
        ├── requirements.txt
        ├── results.txt
        ├── review_app.py
        └── tempCodeRunnerFile.py (excluded)
        ├── .devcontainer/ (excluded)
            └── devcontainer.json (excluded)
        ├── .pytest_cache/ (excluded)
            ├── v/ (excluded)
            ├── .gitignore (excluded)
            ├── CACHEDIR.TAG (excluded)
            └── README.md (excluded)
            ├── v/ (excluded)
                └── cache/ (excluded)
                ├── cache/ (excluded)
                    ├── lastfailed (excluded)
                    ├── nodeids (excluded)
                    └── stepwise (excluded)
        ├── data/ (excluded)
            ├── outputs/ (excluded)
            ├── whoosh_index/ (excluded)
            ├── CafeteriaFCD_foodon.xml (excluded)
            ├── CafeteriaFCD_foodon_unique.xml (excluded)
            ├── CafeteriaFCD_snomedct.xml (excluded)
            ├── embeddings.json (excluded)
            ├── enriched_documents.json (excluded)
            ├── faiss_index.bin (excluded)
            ├── faiss_metadata.json (excluded)
            ├── ontology_dump.json (excluded)
            ├── ontology_syns_dump.json (excluded)
            └── readable_evaluation_results.json (excluded)
            ├── outputs/ (excluded)
                ├── mapped_ingredients_output.json (excluded)
                └── parsed_ingredients_output.json (excluded)
            ├── whoosh_index/ (excluded)
                ├── MAIN_29x4gpdinh37pl3n.seg (excluded)
                ├── MAIN_WRITELOCK (excluded)
                └── _MAIN_1.toc (excluded)
        ├── docs/ (excluded)
            ├── project_outline.txt (excluded)
            └── tasks.txt (excluded)
        ├── models/ (excluded)
            ├── food_bert_v3/ (excluded)
            └── .DS_Store (excluded)
            ├── food_bert_v3/ (excluded)
                ├── checkpoint-2000/ (excluded)
                ├── checkpoint-2280/ (excluded)
                ├── runs/ (excluded)
                ├── config.json (excluded)
                ├── model.safetensors (excluded)
                ├── special_tokens_map.json (excluded)
                ├── tokenizer_config.json (excluded)
                └── vocab.txt (excluded)
                ├── checkpoint-2000/ (excluded)
                    ├── config.json (excluded)
                    ├── model.safetensors (excluded)
                    ├── optimizer.pt (excluded)
                    ├── rng_state.pth (excluded)
                    ├── scheduler.pt (excluded)
                    ├── trainer_state.json (excluded)
                    └── training_args.bin (excluded)
                ├── checkpoint-2280/ (excluded)
                    ├── config.json (excluded)
                    ├── model.safetensors (excluded)
                    ├── optimizer.pt (excluded)
                    ├── rng_state.pth (excluded)
                    ├── scheduler.pt (excluded)
                    ├── trainer_state.json (excluded)
                    └── training_args.bin (excluded)
                ├── runs/ (excluded)
                    └── Mar18_19-24-31_Mac/ (excluded)
                    ├── Mar18_19-24-31_Mac/ (excluded)
                        └── events.out.tfevents.1742340271.Mac.12182.0 (excluded)
        ├── ontologies/ (excluded)
            ├── .DS_Store (excluded)
            ├── chebi.owl (excluded)
            ├── foodon.owl (excluded)
            ├── foodonmerged.owl (excluded)
            ├── test.sh (excluded)
            ├── test_foodon_snippet.owl (excluded)
            └── tmp.owl (excluded)
        ├── prompts/
            ├── final_selection.tpl
            ├── final_selection2.tpl
            ├── final_selection_ingredients.tpl
            ├── parse_ontology.tpl
            └── reranker.tpl
        ├── scripts/
            ├── find_namespaces.py
            ├── format_evaluation_results.py
            ├── rebuild_base.bash
            └── reduce_evaluation_set.py
        ├── src/
            ├── __pycache__/
            ├── embeddings/
            ├── evaluation/
            ├── ingestion/
            ├── pipeline/
            ├── rag_selectors/
            ├── reranker/
            ├── retriever/
            ├── tests/
            ├── utils/
            ├── vector_store/
            ├── __init__.py
            ├── config.py
            ├── main.py
            └── run_on_off.py
            ├── __pycache__/
                ├── __init__.cpython-39.pyc (excluded)
                └── config.cpython-39.pyc (excluded)
            ├── embeddings/
                ├── __pycache__/
                └── embed_docs.py
                ├── __pycache__/
                    └── embed_docs.cpython-39.pyc (excluded)
            ├── evaluation/
                ├── __pycache__/
                ├── evaluate_pipeline.py
                └── evaluate_retriever_recall.py
                ├── __pycache__/
                    └── evaluate_retriever_recall.cpython-39.pyc (excluded)
            ├── ingestion/
                ├── __pycache__/
                ├── __init__.py
                ├── build_lexical_index.py
                ├── enrich_documents.py
                └── parse_ontology.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    ├── build_lexical_index.cpython-39.pyc (excluded)
                    ├── enrich_documents.cpython-39.pyc (excluded)
                    └── parse_ontology.cpython-39.pyc (excluded)
            ├── pipeline/
                ├── __pycache__/
                ├── gemini_pipeline.py
                └── ollama_pipeline.py
                ├── __pycache__/
                    ├── gemini_pipeline.cpython-39.pyc (excluded)
                    ├── ollama_pipeline.cpython-39.pyc (excluded)
                    └── pipeline.cpython-39.pyc (excluded)
            ├── rag_selectors/
                ├── __pycache__/
                ├── __init__.py
                ├── gemini_selector.py
                └── ollama_selector.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    ├── gemini_selector.cpython-39.pyc (excluded)
                    └── ollama_selector.cpython-39.pyc (excluded)
            ├── reranker/
                ├── __pycache__/
                ├── __init__.py
                └── llm_reranker.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    └── llm_reranker.cpython-39.pyc (excluded)
            ├── retriever/
                ├── __pycache__/
                ├── __init__.py
                └── hybrid_retriever.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    └── hybrid_retriever.cpython-39.pyc (excluded)
            ├── tests/
                ├── __pycache__/
                ├── __init__.py
                ├── test_lexical_index.py
                └── test_parse_ontology.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    ├── test_lexical_index.cpython-39-pytest-8.3.5.pyc (excluded)
                    └── test_parse_ontology.cpython-39-pytest-8.3.5.pyc (excluded)
            ├── utils/
                ├── __pycache__/
                ├── __init__.py
                └── ontology_utils.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    └── ontology_utils.cpython-39.pyc (excluded)
            ├── vector_store/
                ├── __pycache__/
                ├── __init__.py
                └── faiss_store.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    └── faiss_store.cpython-39.pyc (excluded)
"file_contents":
  ".env.example": |
    # OpenAI & vector DB credentials
    OPENAI_API_KEY=your_openai_api_key_here
    PINECONE_API_KEY=your_pinecone_api_key_here
    PINECONE_ENV=your_pinecone_env
  ".gitignore": |-
    # Byte-compiled / cache
    __pycache__/
    *.py[cod]
    .pytest_cache/

    # Environment & secrets
    .env
    api_key.json

    # VSCode, macOS
    .vscode/
    .DS_Store

    # Large generated data and ontology files
    /data/
    /ontologies/
    /models/
  "requirements.txt": |-
    # onto_rag/requirements.txt

    # Core Ontology Processing
    rdflib>=6.0.0

    # Lexical Indexing
    whoosh>=2.7.0

    # Embeddings & Semantic Search
    sentence-transformers>=2.2.0
    faiss-cpu>=1.7.0   # Or faiss-gpu if you have a CUDA-enabled GPU and want to use it

    # Utilities
    numpy>=1.21.0     # Often a dependency, good to specify
    pandas>=1.3.0     # Dependency for libraries like 'datasets' used by sentence-transformers
    python-dotenv>=0.19.0 # For loading .env files
    tqdm>=4.60.0      # For progress bars in long operations

    # Testing (optional, but good for development)
    pytest>=6.2.0
  "results.txt": |-
    llama:
    2025-06-11 16:06:24,274 - INFO - --- Evaluation Complete ---
    2025-06-11 16:06:24,274 - INFO - Total entities evaluated: 948
    2025-06-11 16:06:24,274 - INFO - Retrieval Failures (no candidates): 0
    2025-06-11 16:06:24,274 - INFO - Selection Failures (LLM error): 0
    2025-06-11 16:06:24,274 - INFO - ---------------------------
    2025-06-11 16:06:24,274 - INFO - Valid attempts for selector: 948
    2025-06-11 16:06:24,274 - INFO - Correct selections (Hits): 386
    2025-06-11 16:06:24,275 - INFO - Accuracy: 0.4072 (386/948)
    2025-06-11 16:06:24,275 - INFO - Saving 562 incorrect selections to /Users/jandrole/projects/onto_rag/evaluation_results.json
    2025-06-11 16:06:24,301 - INFO - Evaluation finished.

    gemini:
    2025-06-16 10:43:04,962 - INFO - --- Evaluation Complete ---
    2025-06-16 10:43:04,962 - INFO - Total entities evaluated: 948
    2025-06-16 10:43:04,962 - INFO - Retrieval Failures (no candidates): 0
    2025-06-16 10:43:04,962 - INFO - Selection Failures (LLM error): 0
    2025-06-16 10:43:04,962 - INFO - ---------------------------
    2025-06-16 10:43:04,962 - INFO - Valid attempts for selector: 948
    2025-06-16 10:43:04,962 - INFO - Correct selections (Hits): 600
    2025-06-16 10:43:04,962 - INFO - Accuracy: 0.6329 (600/948)
    2025-06-16 10:43:04,962 - INFO - Saving 348 incorrect selections to /Users/jandrole/projects/onto_rag/evaluation_results.json
    2025-06-16 10:43:04,975 - INFO - Evaluation finished.
  "review_app.py": "import streamlit as st\nimport json\nfrom pathlib import Path\n\
    \n# --- Configuration ---\n# Set the path to the human-readable evaluation results\
    \ file.\n# This script assumes the file is in the 'data' subdirectory.\nEVALUATION_FILE_PATH\
    \ = Path(\"data\") / \"readable_evaluation_results.json\"\n\n# --- Helper Functions\
    \ ---\n\n@st.cache_data\ndef load_data(file_path: Path) -> list:\n    \"\"\"\n\
    \    Loads the evaluation data from the specified JSON file.\n    The @st.cache_data\
    \ decorator ensures the data is loaded only once.\n    \"\"\"\n    if not file_path.exists():\n\
    \        st.error(f\"Error: Evaluation file not found at '{file_path}'.\")\n \
    \       st.info(\"Please run the `scripts/format_evaluation_results.py` script\
    \ first to generate this file.\")\n        return None\n    with open(file_path,\
    \ 'r', encoding='utf-8') as f:\n        return json.load(f)\n\ndef display_term_details(term_data:\
    \ dict):\n    \"\"\"Renders the details of a single ontology term without a main\
    \ title.\"\"\"\n    if not term_data or not term_data.get(\"curie\"):\n      \
    \  st.warning(\"No data available for this term.\")\n        return\n\n    # Display\
    \ the label and CURIE\n    label = term_data.get('label', 'N/A')\n    curie =\
    \ term_data.get('curie', 'N/A')\n    st.markdown(f\"**{label}** (`{curie}`)\"\
    )\n\n    # Display the definition in an info box\n    definition = term_data.get('definition')\n\
    \    if definition:\n        st.info(f\"**Definition:** {definition}\")\n    else:\n\
    \        st.info(\"No definition provided.\")\n\n    # Display synonyms if they\
    \ exist\n    synonyms = term_data.get('synonyms', [])\n    if synonyms:\n    \
    \    st.markdown(f\"**Synonyms:** *{', '.join(synonyms)}*\")\n\ndef display_term(term_data:\
    \ dict, title: str):\n    \"\"\"Renders a single ontology term's details in a\
    \ structured format.\"\"\"\n    st.subheader(title)\n    display_term_details(term_data)\n\
    \n# --- Main Application Logic ---\n\n# Set the page configuration (title, icon,\
    \ layout)\nst.set_page_config(\n    page_title=\"Ontology Linking Review\",\n\
    \    page_icon=\"\U0001F9EA\",\n    layout=\"wide\"\n)\n\nst.title(\"\U0001F9EA\
    \ Ontology Linking Evaluation Review\")\nst.markdown(\"An interface for experts\
    \ to review the performance of the entity linking model.\")\n\n# Load the data\
    \ using the cached function\ndata = load_data(EVALUATION_FILE_PATH)\n\nif data:\n\
    \    # --- Session State for Navigation ---\n    # Initialize the session state\
    \ to keep track of the current item index\n    if 'current_index' not in st.session_state:\n\
    \        st.session_state.current_index = 0\n\n    # --- Navigation Controls ---\n\
    \    st.sidebar.header(\"Navigation\")\n    # Allow selection by query text for\
    \ easy lookup\n    query_list = [f\"{i+1}. {item['query']}\" for i, item in enumerate(data)]\n\
    \    selected_query = st.sidebar.selectbox(\"Select a Query to Review:\", query_list,\
    \ index=st.session_state.current_index)\n\n    # Update index based on selection\n\
    \    st.session_state.current_index = query_list.index(selected_query)\n\n   \
    \ col1, col2 = st.sidebar.columns(2)\n    # \"Previous\" button\n    if col1.button(\"\
    ⬅️ Previous\", use_container_width=True):\n        if st.session_state.current_index\
    \ > 0:\n            st.session_state.current_index -= 1\n            st.rerun()\n\
    \    # \"Next\" button\n    if col2.button(\"Next ➡️\", use_container_width=True):\n\
    \        if st.session_state.current_index < len(data) - 1:\n            st.session_state.current_index\
    \ += 1\n            st.rerun()\n\n    # --- Display the selected item ---\n  \
    \  item = data[st.session_state.current_index]\n    query = item.get(\"query\"\
    )\n\n    st.header(f\"Reviewing Query: \\\"{query}\\\"\", divider=\"rainbow\"\
    )\n\n    # --- CHANGE 1: Removed the Correct/Incorrect status message ---\n  \
    \  # The block checking item.get(\"is_correct\") was removed from here.\n\n  \
    \  # Use columns for a side-by-side comparison\n    left_col, right_col = st.columns(2)\n\
    \n    with left_col:\n        # Display the model's chosen term\n        display_term(item.get(\"\
    chosen_term\"), \"\U0001F916 Model's Choice\")\n        # Display the model's\
    \ reasoning in an expandable section\n        with st.expander(\"Show Model's\
    \ Explanation\"):\n            st.info(item.get(\"explanation\", \"No explanation\
    \ provided.\"))\n\n    with right_col:\n        # --- CHANGE 2: Display ALL ground\
    \ truth terms with full details ---\n        st.subheader(\"\U0001F3AF Ground\
    \ Truth(s)\")\n        ground_truth_terms = item.get(\"ground_truth_terms\", [])\n\
    \        \n        if not ground_truth_terms:\n             st.warning(\"No ground\
    \ truth terms provided for this query.\")\n        else:\n            for i, term\
    \ in enumerate(ground_truth_terms):\n                # Add a separator between\
    \ terms for clarity, but not before the first one\n                if i > 0:\n\
    \                    st.markdown(\"---\")\n                display_term_details(term)\n\
    \n\n    # --- Display the list of all candidates provided to the model ---\n \
    \   st.markdown(\"---\")\n    with st.expander(\"\U0001F575️‍♀️ View All Candidate\
    \ Terms Provided to Model\"):\n        st.markdown(\"This is the full list of\
    \ options the model had to choose from.\")\n\n        candidates = item.get(\"\
    candidate_terms_provided\", [])\n        chosen_curie = item.get(\"chosen_term\"\
    , {}).get(\"curie\")\n        ground_truth_curies = [gt.get(\"curie\") for gt\
    \ in item.get(\"ground_truth_terms\", [])]\n\n        if not candidates:\n   \
    \         st.info(\"No candidate terms were provided to the model for this query.\"\
    )\n        else:\n            for candidate in candidates:\n                label\
    \ = candidate.get('label', 'N/A')\n                curie = candidate.get('curie',\
    \ 'N/A')\n\n                # Highlight chosen and ground truth terms\n      \
    \          marker = \"\"\n                if curie == chosen_curie:\n        \
    \            marker += \"\U0001F916\"\n                if curie in ground_truth_curies:\n\
    \                    marker += \"\U0001F3AF\"\n\n                st.markdown(f\"\
    **{marker} {label}** (`{curie}`)\")\n                definition = candidate.get('definition')\n\
    \                if definition:\n                    st.text(f\"  - {definition[:200]}...\"\
    ) # Truncate long definitions\n                else:\n                    st.text(\"\
    \  - No definition.\")"
  "prompts":
    "final_selection.tpl": |-
      You are an expert ontologist specializing in food science. Your task is to analyze a list of candidate ontology terms and select the single most appropriate term that matches the user's provided entity.

      **User Entity:**
      [USER_ENTITY]

      **Candidate Ontology Terms:**
      [CANDIDATE_LIST]

      **Instructions:**
      1.  Carefully review the user's entity and each candidate's details (ID, Label, Definition, Synonyms).
      2.  Select the single best match. Consider exact matches of labels or synonyms as strong signals. If there are multiple good matches, prefer the more specific term over a general one.
      3.  Provide your response in a valid JSON format only. Do not add any text before or after the JSON block.
      4.  The JSON object must contain two keys:
          - "chosen_id": The CURIE (ID) of the single best matching term (e.g., "FOODON:00001290").
          - "explanation": A brief, clear explanation for your choice, justifying why it is the best fit compared to other options.

      **Example Response Format:**
      {
        "chosen_id": "FOODON:00001290",
        "explanation": "I chose 'garlic' because its label is an exact match for the user entity. Candidate 'allium sativum' is the scientific name but 'garlic' is the common term and therefore a better fit."
      }
    "final_selection2.tpl": "SYSTEM:\nYou are an expert ontologist specializing in\
      \ food science.  \nYour objective is to map a given *user_entity* to one—and\
      \ only one—candidate term drawn from FoodOn.  \nOperate deterministically: set\
      \ **temperature = 0** (greedy decoding).  \nReturn **only** a valid JSON object\
      \ as output—no prose before or after.\n\nINPUT (exact JSON structure):\n{\n\
      \  \"user_entity\": \"<string>\",\n  \"candidate_terms\": [\n    {\n      \"\
      id\": \"<CURIE>\",\n      \"label\": \"<string>\",\n      \"definition\": \"\
      <string>\",\n      \"synonyms\": [\"<string>\", ...]\n    }\n    // … additional\
      \ candidates in the same shape\n  ]\n}\n\nTASK INSTRUCTIONS\n1. Compare *user_entity*\
      \ against each candidate term’s label, definition, and synonyms.\n2. Identify\
      \ the single best match.  \n   • Exact lexical matches in *label* or *synonyms*\
      \ are strong signals.  \n   • If multiple candidates are plausible, **prefer\
      \ the most specific term** over broader ones.\n3. Produce a JSON object with\
      \ exactly two keys:\n   • **\"chosen_id\"** – the CURIE of the selected term.\
      \  \n   • **\"explanation\"** – a concise, step-by-step justification of why\
      \ this term outranks the others (implicit chain-of-thought).\n\nOUTPUT FORMAT\
      \ (nothing else):\n{\n  \"chosen_id\": \"<CURIE>\",\n  \"explanation\": \"<brief\
      \ but complete reasoning>\"\n}\n\nEDGE-CASE EXAMPLE (for the model’s patterning\
      \ only):\nInput:\n{\n  \"user_entity\": \"apple\",\n  \"candidate_terms\": [\n\
      \    {\"id\":\"FOODON:00002403\",\"label\":\"apple (fruit)\",\"definition\"\
      :\"The edible fruit of Malus domestica.\",\"synonyms\":[\"apple fruit\",\"fresh\
      \ apple\"]},\n    {\"id\":\"FOODON:03311015\",\"label\":\"fruit (plant product)\"\
      ,\"definition\":\"A botanical fruit.\",\"synonyms\":[\"plant fruit\"]},\n  \
      \  {\"id\":\"FOODON:00002405\",\"label\":\"apple pie\",\"definition\":\"A pie\
      \ made with apples.\",\"synonyms\":[\"apple tart\"]}\n  ]\n}\n\nExpected Output:\n\
      {\n  \"chosen_id\": \"FOODON:00002403\",\n  \"explanation\": \"Exact label match\
      \ with 'apple (fruit)'. While 'fruit (plant product)' matches only generically\
      \ and 'apple pie' is a derivative product, 'apple (fruit)' is the most specific,\
      \ conceptually precise fit.\"\n}\n\n\n\n\n\n\nYou are an expert ontologist specializing\
      \ in food science.\nRespond deterministically (the caller will invoke the model\
      \ with temperature 0).\n\n────────────────────────────────────────────────────────\n\
      USER ENTITY\n[USER_ENTITY]\n\nCANDIDATE ONTOLOGY TERMS\n[As numbered list —\
      \ each item has “ID”, “Label”, “Definition”, “Synonyms”.]\n[CANDIDATE_LIST]\n\
      ────────────────────────────────────────────────────────\n\nTASK\n1. Examine\
      \ the user entity against each candidate’s **Label, Definition, and Synonyms**.\n\
      2. Pick the **single** best-matching term.\n   • Exact lexical matches in Label\
      \ or Synonyms are strong signals.  \n   • If several terms could work, choose\
      \ the **most specific** one.\n3. Output **only** a valid JSON object – nothing\
      \ before or after it.\n\nOUTPUT SHAPE\n{\n  \"chosen_id\": \"<CURIE>\",\n  \"\
      explanation\": \"<concise step-by-step rationale comparing the chosen term to\
      \ close alternatives>\"\n}\n\n\U0001F4CC Do not wrap the JSON in markdown fences.\
      \ Do not emit any other text.\n\nEXAMPLE (shows specificity rule; do NOT repeat\
      \ in your answer)\nInput block (abbreviated):\n  USER_ENTITY: apple\n  CANDIDATE_LIST:\n\
      \    1. ID: FOODON:00002403 … Label: apple (fruit) …\n    2. ID: FOODON:03311015\
      \ … Label: fruit (plant product) …\n    3. ID: FOODON:00002405 … Label: apple\
      \ pie …\nExpected JSON:\n{\n  \"chosen_id\": \"FOODON:00002403\",\n  \"explanation\"\
      : \"Exact label match with 'apple (fruit)'. 'Fruit (plant product)' is generic\
      \ and 'apple pie' is a derivative food; therefore 'apple (fruit)' is the most\
      \ specific fit.\"\n}\n"
    "final_selection_ingredients.tpl": |-
      You are an expert ontologist specializing in food, ingredient, and chemical substance classification. Your task is to analyze a user-provided entity and select the single most appropriate term from a list of candidate ontology terms, based on a strict set of rules.

      You will be given a user entity and a numbered list of candidate terms. Each candidate in the list includes an ID, Label, Definition, and a list of Synonyms.

      **User Entity:**
      [USER_ENTITY]

      **Candidate List:**
      [CANDIDATE_LIST]

      **Instructions:**
      1.  Carefully evaluate the User Entity against the Label, Synonyms, and Definition of each candidate in the Candidate List.
      2.  Select the single best match. An exact match between the User Entity and a candidate's Label or one of its Synonyms is the strongest signal for selection.
      3.  **Specificity Rule:** If multiple candidates are good matches, you must choose the most specific term over the more general one.
      4.  Your response must be a single, valid JSON object only. Do not add any text, explanations, or comments before or after the JSON block.

      **Output Format:**
      The JSON object you return must contain two keys:
      * `"chosen_id"`: The ID of the single best matching term.
      * `"explanation"`: A brief justification for your choice. This explanation must clarify why the chosen term is the best fit and, if relevant, why it was chosen over other plausible candidates by applying the specificity rule.

      **Example:**
      ---
      **User Entity:**
      citric acid

      **Candidate List:**
      1. ID: FOODON:03301503
         Label: acidulant
         Definition: A food additive which increases the acidity or enhances the sour taste of a food.
         Synonyms: food acid

      2. ID: CHEBI:30769
         Label: citric acid
         Definition: A tricarboxylic acid that is propane-1,2,3-tricarboxylic acid bearing a hydroxy substituent at position 2.
         Synonyms: 2-hydroxypropane-1,2,3-tricarboxylic acid

      3. ID: FOODON:03301072
         Label: lemon juice
         Definition: The juice obtained from lemons, a common source of citric acid.
         Synonyms: None
      ---

      Your Response:
      ```json
      {
        "chosen_id": "CHEBI:30769",
        "explanation": "I chose 'citric acid' because its label is an exact match for the user entity. While 'acidulant' describes its function, 'citric acid' is the specific chemical entity and therefore the most precise match, adhering to the specificity rule. 'lemon juice' is a product that contains the entity, not the entity itself."
      }
    "parse_ontology.tpl": ""
    "reranker.tpl": ""
  "scripts":
    "find_namespaces.py": |-
      # scripts/find_namespaces.py
      import rdflib
      from rdflib import URIRef
      from collections import Counter
      import re
      import os
      import sys

      # Add project root to allow importing src.config
      PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
      sys.path.append(PROJECT_ROOT)
      from src.config import FOODON_PATH # Or any other OWL file you want to inspect

      def get_base_uri(uri_str):
          """
          Tries to extract a base URI from a full URI.
          e.g., http://purl.obolibrary.org/obo/FOODON_0000123 -> http://purl.obolibrary.org/obo/FOODON_
          e.g., http://www.w3.org/2000/01/rdf-schema#label -> http://www.w3.org/2000/01/rdf-schema#
          """
          if '#' in uri_str:
              return uri_str.rsplit('#', 1)[0] + '#'
          # Common OBO pattern: ends with an underscore followed by digits
          match_obo = re.match(r'(.+[_A-Z]+_)(\d+)$', uri_str)
          if match_obo:
              return match_obo.group(1)
          # General pattern: up to the last slash
          if '/' in uri_str:
              return uri_str.rsplit('/', 1)[0] + '/'
          return None

      def main(ontology_path):
          print(f"Loading ontology from: {ontology_path}...")
          g = rdflib.Graph()
          try:
              g.parse(ontology_path) # format will be auto-detected
              print(f"Successfully parsed. Found {len(g)} triples.")
          except Exception as e:
              print(f"Error parsing ontology: {e}")
              return

          uris = set()
          for s, p, o in g:
              if isinstance(s, URIRef):
                  uris.add(str(s))
              if isinstance(p, URIRef):
                  uris.add(str(p))
              if isinstance(o, URIRef):
                  uris.add(str(o))

          print(f"\nFound {len(uris)} unique URIs.")

          base_uri_counts = Counter()
          for uri in uris:
              base = get_base_uri(uri)
              if base:
                  base_uri_counts[base] += 1

          print("\nPotential Base URIs (with counts of distinct full URIs using them):")
          # Sort by count descending
          for base, count in base_uri_counts.most_common():
              print(f"- \"{base}\": (used in {count} URIs)")

          print("\n--- rdflib's known namespaces (Prefix: Namespace) ---")
          for prefix, namespace in g.namespaces():
              print(f"  {prefix}: {namespace}")


      if __name__ == "__main__":
          # You can make the ontology path an argument if you like
          # For now, using FOODON_PATH from config
          if not os.path.exists(FOODON_PATH):
              print(f"ERROR: FoodON ontology file not found at {FOODON_PATH}")
          else:
              main(FOODON_PATH)
    "format_evaluation_results.py": "# scripts/format_evaluation_results.py\n\nimport\
      \ json\nimport sys\nimport argparse\nfrom pathlib import Path\nfrom typing import\
      \ Dict, Any, List, Optional\n\n# --- Add project root to sys.path to allow imports\
      \ from src ---\n# This assumes the script is in project_root/scripts/\nPROJECT_ROOT\
      \ = Path(__file__).resolve().parents[1]\nsys.path.append(str(PROJECT_ROOT))\n\
      # ---\n\ntry:\n    from src import config\nexcept ImportError:\n    print(\"\
      Error: Could not import from 'src'. Make sure the script is run from the project\
      \ root\"\n          \" or the project structure is correct.\")\n    # Provide\
      \ fallback paths if config fails to import\n    class config:\n        ONTOLOGY_DUMP_JSON\
      \ = PROJECT_ROOT / \"data\" / \"ontology_dump.json\"\n\ndef load_json_data(file_path:\
      \ Path) -> Any:\n    \"\"\"Loads data from a JSON file.\"\"\"\n    if not file_path.exists():\n\
      \        print(f\"Error: File not found at {file_path}\")\n        sys.exit(1)\n\
      \    print(f\"Loading data from {file_path}...\")\n    with open(file_path,\
      \ 'r', encoding='utf-8') as f:\n        return json.load(f)\n\ndef get_enriched_details(curie:\
      \ str, ontology_data: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Retrieves\
      \ detailed information for a given CURIE from the ontology dump.\n    \n   \
      \ Args:\n        curie: The CURIE to look up (e.g., \"FOODON:03316347\").\n\
      \        ontology_data: The loaded ontology_dump.json data.\n\n    Returns:\n\
      \        A dictionary with enriched details.\n    \"\"\"\n    term_data = ontology_data.get(curie)\n\
      \    if not term_data:\n        return {\n            \"curie\": curie,\n  \
      \          \"label\": \"--- CURIE NOT FOUND ---\",\n            \"definition\"\
      : \"N/A\",\n            \"synonyms\": []\n        }\n    \n    return {\n  \
      \      \"curie\": curie,\n        \"label\": term_data.get(\"label\"),\n   \
      \     \"definition\": term_data.get(\"definition\"),\n        \"synonyms\":\
      \ term_data.get(\"synonyms\", [])\n    }\n\ndef process_evaluation_file(\n \
      \   eval_results: List[Dict[str, Any]],\n    ontology_data: Dict[str, Any]\n\
      ) -> List[Dict[str, Any]]:\n    \"\"\"\n    Processes the evaluation results\
      \ to create a human-readable version.\n    \"\"\"\n    print(\"Enriching evaluation\
      \ results...\")\n    enriched_output = []\n\n    for item in eval_results:\n\
      \        query = item.get(\"query\")\n        chosen_curie = item.get(\"chosen_curie\"\
      )\n        true_curies = item.get(\"true_curies\", [])\n        candidates_provided\
      \ = item.get(\"candidates_provided\", [])\n        \n        # Check if the\
      \ chosen CURIE is in the list of true CURIEs\n        is_correct = chosen_curie\
      \ in true_curies\n\n        # Enrich all relevant CURIEs\n        chosen_details\
      \ = get_enriched_details(chosen_curie, ontology_data)\n        true_details\
      \ = [get_enriched_details(tc, ontology_data) for tc in true_curies]\n      \
      \  candidates_details = [get_enriched_details(cc, ontology_data) for cc in candidates_provided]\n\
      \n        enriched_item = {\n            \"query\": query,\n            \"is_correct\"\
      : is_correct,\n            \"explanation\": item.get(\"explanation\"),\n   \
      \         \"chosen_term\": chosen_details,\n            \"ground_truth_terms\"\
      : true_details,\n            \"candidate_terms_provided\": candidates_details\n\
      \        }\n        enriched_output.append(enriched_item)\n        \n    return\
      \ enriched_output\n\ndef main():\n    \"\"\"Main function to run the script.\"\
      \"\"\n    parser = argparse.ArgumentParser(\n        description=\"Enrich LLM\
      \ evaluation results with ontology details for human review.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n\
      \    )\n    parser.add_argument(\n        \"--input-file\",\n        type=Path,\n\
      \        default=PROJECT_ROOT / \"evaluation_results_gemini_1.5-flash.json\"\
      ,\n        help=\"Path to the raw evaluation results JSON file.\"\n    )\n \
      \   parser.add_argument(\n        \"--output-file\",\n        type=Path,\n \
      \       default=PROJECT_ROOT / \"data\" / \"readable_evaluation_results.json\"\
      ,\n        help=\"Path to save the enriched, human-readable JSON file.\"\n \
      \   )\n    parser.add_argument(\n        \"--ontology-dump\",\n        type=Path,\n\
      \        default=Path(config.ONTOLOGY_DUMP_JSON),\n        help=\"Path to the\
      \ ontology_dump.json file.\"\n    )\n    args = parser.parse_args()\n\n    #\
      \ Create output directory if it doesn't exist\n    args.output_file.parent.mkdir(parents=True,\
      \ exist_ok=True)\n\n    # Load data\n    ontology_data = load_json_data(args.ontology_dump)\n\
      \    eval_results = load_json_data(args.input_file)\n\n    # Process and enrich\
      \ the data\n    readable_results = process_evaluation_file(eval_results, ontology_data)\n\
      \n    # Save the new file\n    print(f\"Saving enriched results to {args.output_file}...\"\
      )\n    with open(args.output_file, 'w', encoding='utf-8') as f:\n        json.dump(readable_results,\
      \ f, indent=2, ensure_ascii=False)\n    \n    print(\"Done!\")\n\nif __name__\
      \ == \"__main__\":\n    main()"
    "rebuild_base.bash": |
      #!/usr/bin/env bash
      #
      # Rebuild the whole Onto-RAG pipeline from raw ontology → evaluation
      # Usage: ./scripts/rebuild_all.sh            # runs with defaults
      #        ./scripts/rebuild_all.sh --skip-eval   # skip final recall test
      #
      set -euo pipefail

      ROOT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )/.." && pwd )"
      cd "${ROOT_DIR}"

      DATA_DIR="data"

      echo "🧹  Cleaning old artefacts …"
      rm -f "${DATA_DIR}"/faiss_index.bin  "${DATA_DIR}"/faiss_metadata.json
      rm -rf "${DATA_DIR}/whoosh_index"

      echo "①  Parsing ontology …"
      python -m src.ingestion.parse_ontology

      echo "②  Creating enriched documents …"
      python -m src.ingestion.enrich_documents

      echo "③  Embedding documents …"
      python -m src.embeddings.embed_docs

      echo "④  Building FAISS index …"
      python -m src.vector_store.faiss_store

      echo "⑤  Building Whoosh index …"
      python -m src.ingestion.build_lexical_index

      if [[ "${1-}" != "--skip-eval" ]]; then
        echo "⑥  Running evaluator …"
        python -m src.evaluation.evaluate_retriever_recall
      fi

      echo "✅  Pipeline finished."
    "reduce_evaluation_set.py": "# scripts/reduce_evaluation_set.py\n\nimport xml.etree.ElementTree\
      \ as ET\nimport os\nimport sys\nfrom collections import defaultdict\n\n# Add\
      \ project root to Python path to allow direct imports if needed in the future\n\
      PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n\
      sys.path.append(PROJECT_ROOT)\n\n# --- Configuration ---\n# Assumes the data\
      \ folder is at the project root\nINPUT_XML_PATH = os.path.join(PROJECT_ROOT,\
      \ \"data\", \"CafeteriaFCD_foodon.xml\")\nOUTPUT_XML_PATH = os.path.join(PROJECT_ROOT,\
      \ \"data\", \"CafeteriaFCD_foodon_unique.xml\")\n\ndef create_unique_dataset(input_file:\
      \ str, output_file: str):\n    \"\"\"\n    Parses an XML annotation file and\
      \ creates a new, smaller XML file\n    containing only one instance of each\
      \ unique (text, semantic_tags) pair.\n    \"\"\"\n    if not os.path.exists(input_file):\n\
      \        print(f\"Error: Input file not found at {input_file}\")\n        return\n\
      \n    try:\n        tree = ET.parse(input_file)\n        root = tree.getroot()\n\
      \    except ET.ParseError as e:\n        print(f\"Error parsing XML file {input_file}:\
      \ {e}\")\n        return\n\n    # A set to store the unique identifiers of annotations\
      \ we've already added.\n    # The identifier will be a tuple: (UPPERCASE_TEXT,\
      \ sorted_tuple_of_tags)\n    seen_annotations = set()\n    \n    # These will\
      \ be the root and single document of our new XML file\n    new_root = ET.Element(\"\
      collection\")\n    new_doc = ET.SubElement(new_root, \"document\")\n    new_doc.set(\"\
      id\", \"unique_annotations_document\")\n\n    total_annotations_processed =\
      \ 0\n    unique_annotations_found = 0\n\n    # Find all annotation tags anywhere\
      \ in the document\n    for annotation_node in root.findall('.//annotation'):\n\
      \        total_annotations_processed += 1\n        \n        text_node = annotation_node.find('text')\n\
      \        tags_node = annotation_node.find('infon[@key=\"semantic_tags\"]')\n\
      \n        # Ensure both text and tags exist to form a valid entry\n        if\
      \ (text_node is not None and text_node.text and\n                tags_node is\
      \ not None and tags_node.text):\n            \n            # 1. Normalize the\
      \ text to be case-insensitive\n            text = text_node.text.strip().upper()\n\
      \n            # 2. Normalize the tags to be order-insensitive\n            raw_tags\
      \ = tags_node.text.strip()\n            # Split by ';', strip whitespace, filter\
      \ out any empty strings, and sort\n            sorted_tags = sorted([tag.strip()\
      \ for tag in raw_tags.split(';') if tag.strip()])\n\n            # Create a\
      \ unique, hashable key for this annotation\n            # We convert the list\
      \ of tags to a tuple to make it hashable for the set\n            annotation_key\
      \ = (text, tuple(sorted_tags))\n\n            # 3. If we haven't seen this unique\
      \ combination before, add it\n            if annotation_key not in seen_annotations:\n\
      \                seen_annotations.add(annotation_key)\n                # Append\
      \ the original annotation_node to our new document\n                new_doc.append(annotation_node)\n\
      \                unique_annotations_found += 1\n\n    print(f\"Processed {total_annotations_processed}\
      \ total annotations.\")\n    print(f\"Found {unique_annotations_found} unique\
      \ (text, tags) pairs.\")\n\n    # Write the new, smaller XML tree to the output\
      \ file\n    new_tree = ET.ElementTree(new_root)\n    # The indent function (Python\
      \ 3.9+) makes the XML output readable\n    if sys.version_info >= (3, 9):\n\
      \        ET.indent(new_tree)\n        \n    new_tree.write(output_file, encoding='utf-8',\
      \ xml_declaration=True)\n    print(f\"Successfully saved unique dataset to:\
      \ {output_file}\")\n\n\nif __name__ == \"__main__\":\n    create_unique_dataset(INPUT_XML_PATH,\
      \ OUTPUT_XML_PATH)"
  "src":
    "__init__.py": ""
    "config.py": "# src/config.py\nimport os\nfrom dotenv import load_dotenv\nfrom\
      \ rdflib import Namespace # This is fine, though rdflib.Namespace is not directly\
      \ used for string constants below.\n\n# Project Root Directory\nPROJECT_ROOT\
      \ = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \".\"))\n\
      # print(f\"Project root directory: {PROJECT_ROOT}\") # Keep for debugging if\
      \ you like\n\n# Load environment variables from .env file\nload_dotenv(os.path.join(PROJECT_ROOT,\
      \ \".env\"))\n\n# Data Directory (for ontology dump, indexes, etc.)\nDATA_DIR\
      \ = os.path.join(PROJECT_ROOT, \"data\")\nos.makedirs(DATA_DIR, exist_ok=True)\n\
      \n# Ontologies Directory\nONTOLOGIES_DIR = os.path.join(PROJECT_ROOT, \"ontologies\"\
      )\nFOODON_PATH = os.path.join(ONTOLOGIES_DIR, \"foodon-with-syns.owl\")\nCHEBI_PATH\
      \ = os.path.join(ONTOLOGIES_DIR, \"chebi.owl\")\nTEST_FOODON_SNIPPET_PATH =\
      \ os.path.join(ONTOLOGIES_DIR, \"test_foodon_snippet.owl\")\n\n\n# Output file\
      \ from parse_ontology.py\nONTOLOGY_DUMP_JSON = os.path.join(DATA_DIR, \"ontology_syns_dump.json\"\
      )\n\n# Output file for enriched documents\nENRICHED_DOCUMENTS_FILE = os.path.join(DATA_DIR,\
      \ \"enriched_documents.json\")\n\n# Output file for embeddings (used for building\
      \ FAISS index)\nEMBEDDINGS_FILE = os.path.join(DATA_DIR, \"embeddings.json\"\
      )\n\n# Whoosh Index Directory\nWHOOSH_INDEX_DIR = os.path.join(DATA_DIR, \"\
      whoosh_index\")\nos.makedirs(WHOOSH_INDEX_DIR, exist_ok=True)\n\n# FAISS Index\
      \ Paths\nFAISS_INDEX_PATH = os.path.join(DATA_DIR, \"faiss_index.bin\")\nFAISS_METADATA_PATH\
      \ = os.path.join(DATA_DIR, \"faiss_metadata.json\")\n\n# Embedding Model Configuration\n\
      EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n#EMBEDDING_MODEL_NAME = \"Lajavaness/bilingual-embedding-large\"\
      \ \n# RERANKER_MODEL_NAME = \"intfloat/e5-mistral-7b-instruct\"\nRERANKER_MODEL_NAME\
      \ = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\nEMBEDDING_BATCH_SIZE = 32 # For\
      \ batch embedding script\nEMBEDDING_DEVICE = 'cpu'  # or 'cuda' if available,\
      \ for embedding script\n\n# Default K values for retrieval\nDEFAULT_K_LEXICAL\
      \ = 20\nDEFAULT_K_VECTOR = 20\nDEFAULT_RERANK_K = DEFAULT_K_LEXICAL + DEFAULT_K_VECTOR\n\
      \n# Namespaces (using string constants for broader compatibility if rdflib not\
      \ always imported)\nRDFS_NS_STR = \"http://www.w3.org/2000/01/rdf-schema#\"\n\
      RDF_NS_STR = \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\nOWL_NS_STR = \"\
      http://www.w3.org/2002/07/owl#\"\nSKOS_NS_STR = \"http://www.w3.org/2004/02/skos/core#\"\
      \nOBO_NS_STR = \"http://purl.obolibrary.org/obo/\"\nOBOINOWL_NS_STR = \"http://www.geneontology.org/formats/oboInOwl#\"\
      \nIAO_NS_STR = \"http://purl.obolibrary.org/obo/IAO_\"\n\n# For rdflib usage\
      \ where Namespace objects are preferred:\nRDFS_NS = Namespace(RDFS_NS_STR)\n\
      RDF_NS = Namespace(RDF_NS_STR)\nOWL_NS = Namespace(OWL_NS_STR)\nSKOS_NS = Namespace(SKOS_NS_STR)\n\
      OBO_NS = Namespace(OBO_NS_STR)\nOBOINOWL_NS = Namespace(OBOINOWL_NS_STR)\nIAO_NS\
      \ = Namespace(IAO_NS_STR)\n\n\n# Mapping of common relation URIs/CURIEs to human-readable\
      \ names and default prefixes\n# Used by parse_ontology.py and potentially enrich_documents.py\n\
      RELATION_CONFIG = {\n    \"obo:BFO_0000050\": {\"label\": \"part of\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0001000\": {\"label\": \"derives from\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0002200\": {\"label\": \"has phenotype\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0002451\": {\"label\": \"has part\", \"prefix\": \"\
      obo\"},\n    \"obo:FOODON_0000246\": {\"label\": \"has ingredient\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000056\": {\"label\": \"participates in\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000057\": {\"label\": \"has participant\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000085\": {\"label\": \"functionally related to\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002090\": {\"label\": \"adjacent to\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002131\": {\"label\": \"overlaps\", \"\
      prefix\": \"obo\"},\n    \"obo:RO_0002150\": {\"label\": \"connected to\", \"\
      prefix\": \"obo\"},\n    \"obo:RO_0002202\": {\"label\": \"develops from\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002350\": {\"label\": \"member of\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002351\": {\"label\": \"has member\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002353\": {\"label\": \"output of\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002440\": {\"label\": \"causally related\
      \ to\", \"prefix\": \"obo\"},\n    \"obo:RO_0002450\": {\"label\": \"contains\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002500\": {\"label\": \"surrounds\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002573\": {\"label\": \"has quality\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002606\": {\"label\": \"is transformation\
      \ of\", \"prefix\": \"obo\"},\n    \"obo:RO_0002607\": {\"label\": \"has input\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0003000\": {\"label\": \"used in\", \"\
      prefix\": \"obo\"},\n    \"obo:FOODON_0000380\": {\"label\": \"has preparation\
      \ method\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000226\": {\"label\": \"\
      has food source\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000388\": {\"label\"\
      : \"has maturity state\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000286\"\
      : {\"label\": \"has packaging\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000240\"\
      : {\"label\": \"has preservation method\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000440\"\
      : {\"label\": \"has physical state\", \"prefix\": \"obo\"},\n    # \"obo:FOODON_\"\
      : {\"label\": \"FoodON specific relation\", \"prefix\": \"obo\"}, # This generic\
      \ one might be too broad\n    \"obo:ERO_0000039\": {\"label\": \"has nutrient\"\
      , \"prefix\": \"obo\"},\n    \"obo:ERO_0000589\": {\"label\": \"dietary context\
      \ of\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C25277\": {\"label\": \"is allergen\
      \ of\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C48670\": {\"label\": \"has active\
      \ ingredient\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C64548\": {\"label\":\
      \ \"part of human diet\", \"prefix\": \"obo\"},\n    \"obo:PATO_0000001\": {\"\
      label\": \"has quality PATO\", \"prefix\": \"obo\"},\n}\n\nTARGET_RELATIONS_CURIES\
      \ = list(RELATION_CONFIG.keys())\n\n# CURIE Prefix Map for uri_to_curie and\
      \ curie_to_uri conversions\n# Ensure the keys are the *base URIs* that prefixes\
      \ are expected for.\nCURIE_PREFIX_MAP = {\n    \"http://purl.obolibrary.org/obo/FOODON_\"\
      : \"FOODON\",\n    \"http://purl.obolibrary.org/obo/BFO_\": \"BFO\",\n    \"\
      http://purl.obolibrary.org/obo/RO_\": \"RO\",\n    \"http://purl.obolibrary.org/obo/ERO_\"\
      : \"ERO\",\n    \"http://purl.obolibrary.org/obo/NCIT_\": \"NCIT\",\n    \"\
      http://purl.obolibrary.org/obo/PATO_\": \"PATO\",\n    \"http://purl.obolibrary.org/obo/IAO_\"\
      : \"IAO\",\n    \"http://www.w3.org/2000/01/rdf-schema#\": \"rdfs\",\n    \"\
      http://www.w3.org/1999/02/22-rdf-syntax-ns#\": \"rdf\",\n    \"http://www.w3.org/2002/07/owl#\"\
      : \"owl\",\n    \"http://www.w3.org/2004/02/skos/core#\": \"skos\",\n    \"\
      http://www.geneontology.org/formats/oboInOwl#\": \"oboInOwl\",\n    \"http://purl.obolibrary.org/obo/HANCESTRO_\"\
      : \"HANCESTRO\",  \n    \"http://purl.obolibrary.org/obo/GAZ_\": \"GAZ\",\n\
      \    \"http://purl.obolibrary.org/obo/CHEBI_\": \"CHEBI\",\n    \"http://purl.obolibrary.org/obo/NCBITaxon_\"\
      : \"NCBITaxon\",\n    \"http://purl.obolibrary.org/obo/UBERON_\": \"UBERON\"\
      ,\n    \"http://purl.obolibrary.org/obo/ENVO_\": \"ENVO\",\n    \"http://purl.obolibrary.org/obo/HP_\"\
      : \"HP\",\n    \"http://purl.obolibrary.org/obo/GO_\": \"GO\",\n    # General\
      \ OBO prefix - should be last or handled carefully to avoid overly broad matches\n\
      \    # if specific OBO sub-ontologies are listed above.\n    \"http://purl.obolibrary.org/obo/\"\
      : \"obo\",\n}\n\n# LLM API Key (placeholders)\nGEMINI_API_KEY = os.getenv(\"\
      GEMINI_API_KEY\")\nLLM_SELECTOR_MODEL_NAME = \"gemini-1.5-flash-latest\"\nOLLAMA_SELECTOR_MODEL_NAME\
      \ = 'llama3.1:8b'\n# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Logging\
      \ configuration\nLOG_LEVEL = \"INFO\"\nLOG_FILE = os.path.join(PROJECT_ROOT,\
      \ \"app.log\") # Example log file in project root"
    "main.py": |-
      # src/main.py
      import argparse
      import json
      import os
      import sys

      # --- Add project root to sys.path ---
      PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
      if PROJECT_ROOT not in sys.path:
          sys.path.insert(0, PROJECT_ROOT)

      from src.pipeline.ollama_pipeline import RAGPipeline
      from src.config import DEFAULT_K_LEXICAL, DEFAULT_K_VECTOR

      def main():
          parser = argparse.ArgumentParser(description="Run the Onto-RAG pipeline with LLM selection.")
          parser.add_argument("query", type=str, help="The entity or text to search for (e.g., 'raw milk').")
          parser.add_argument("--lexical_k", type=int, default=DEFAULT_K_LEXICAL, help=f"Initial candidates from lexical search (default: {DEFAULT_K_LEXICAL}).")
          parser.add_argument("--vector_k", type=int, default=DEFAULT_K_VECTOR, help=f"Initial candidates from vector search (default: {DEFAULT_K_VECTOR}).")
          parser.add_argument("--top_n_rerank", type=int, default=10, help="Number of candidates to rerank and pass to the LLM (default: 10).")
          args = parser.parse_args()

          pipeline = None
          try:
              pipeline = RAGPipeline()
              final_result = pipeline.run(
                  query=args.query,
                  lexical_k=args.lexical_k,
                  vector_k=args.vector_k,
                  rerank_top_n=args.top_n_rerank
              )

              print("\n--- Final LLM Selection ---")
              if not final_result:
                  print("Could not determine a matching ontology term.")
              else:
                  print(f"Query: '{args.query}'")
                  print("---------------------------")
                  print(f"Chosen Term ID: {final_result.get('id', 'N/A')}")
                  print(f"Label:          {final_result.get('label', 'N/A')}")
                  print(f"Definition:     {final_result.get('definition', 'N/A')}")
                  print(f"Synonyms:       {'; '.join(final_result.get('synonyms', [])) or 'None'}")
                  print("\nLLM Explanation:")
                  print(f"  > {final_result.get('explanation', 'No explanation provided.')}")
              print("---------------------------\n")

          except Exception as e:
              print(f"\nAn error occurred during the pipeline execution: {e}", file=sys.stderr)
          finally:
              if pipeline:
                  pipeline.close()

      if __name__ == "__main__":
          main()
    "run_on_off.py": "# scripts/map_ingredients.py\n\"\"\"\nThis script loads extracted\
      \ ingredient entities from a JSON file,\nruns them through the RAG pipeline\
      \ to link them to ontology terms,\nand saves the structured results to a new\
      \ JSON file.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport logging\n\
      from tqdm import tqdm\n\n# --- Add project root to sys.path ---\n# This allows\
      \ the script to import modules from the 'src' directory\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),\
      \ '..'))\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n\
      \nfrom src.pipeline.gemini_pipeline import RAGPipeline\nfrom src import config\n\
      \n# --- Configuration ---\nLOGGING_LEVEL = logging.INFO\n# Limit the number\
      \ of products to process. Set to None to process all.\nPRODUCT_LIMIT = 5 \n\n\
      INPUT_FILE = os.path.join(PROJECT_ROOT, 'data', 'outputs', 'parsed_ingredients_output.json')\n\
      OUTPUT_FILE = os.path.join(PROJECT_ROOT, 'data', 'outputs', 'mapped_ingredients_output.json')\n\
      \n\n# --- Setup Logging ---\nlogging.basicConfig(level=LOGGING_LEVEL, format='%(asctime)s\
      \ - %(levelname)s - %(name)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\
      \n\ndef main():\n    \"\"\"\n    Main function to run the ingredient mapping\
      \ process.\n    \"\"\"\n    logger.info(\"Starting ingredient to ontology mapping\
      \ process...\")\n    \n    # Ensure the output directory exists\n    os.makedirs(os.path.dirname(OUTPUT_FILE),\
      \ exist_ok=True)\n\n    pipeline = None\n    try:\n        # --- 1. Load Input\
      \ Data ---\n        logger.info(f\"Loading ingredients from: {INPUT_FILE}\"\
      )\n        if not os.path.exists(INPUT_FILE):\n            logger.error(f\"\
      Input file not found: {INPUT_FILE}\")\n            logger.error(\"Please ensure\
      \ you have run the parsing script first.\")\n            return\n\n        with\
      \ open(INPUT_FILE, 'r', encoding='utf-8') as f:\n            all_ingredients_data\
      \ = json.load(f)\n\n        # --- 2. Initialize RAG Pipeline ---\n        logger.info(\"\
      Initializing RAG pipeline...\")\n        pipeline = RAGPipeline()\n        logger.info(\"\
      RAG pipeline initialized successfully.\")\n\n        # --- 3. Process Ingredients\
      \ ---\n        all_mappings = {}\n        \n        # Get items to process,\
      \ applying the limit if specified\n        items_to_process = list(all_ingredients_data.items())\n\
      \        if PRODUCT_LIMIT is not None:\n            items_to_process = items_to_process[:PRODUCT_LIMIT]\n\
      \            logger.warning(f\"Processing a limited set of {PRODUCT_LIMIT} products.\"\
      )\n\n        # Use tqdm for a progress bar\n        for product_id, ingredients\
      \ in tqdm(items_to_process, desc=\"Processing Products\"):\n            logger.info(f\"\
      --- Processing Product ID: {product_id} ---\")\n            \n            product_mappings\
      \ = []\n            unique_ingredients = sorted(list(set(ingredients))) # Process\
      \ unique ingredients to avoid duplicate work\n\n            for ingredient_query\
      \ in unique_ingredients:\n                logger.info(f\"Querying for: '{ingredient_query}'\"\
      )\n                \n                # Run the pipeline for the ingredient string\n\
      \                mapping_result = pipeline.run(query=ingredient_query)\n   \
      \             \n                if mapping_result:\n                    logger.info(f\"\
      \  -> Found mapping: '{mapping_result.get('label')}' (ID: {mapping_result.get('id')})\"\
      )\n                else:\n                    logger.warning(f\"  -> No mapping\
      \ found for '{ingredient_query}'\")\n                \n                # Store\
      \ the result in a structured format\n                product_mappings.append({\n\
      \                    \"original_ingredient\": ingredient_query,\n          \
      \          \"mapping_result\": mapping_result if mapping_result else \"No mapping\
      \ found\"\n                })\n            \n            all_mappings[product_id]\
      \ = product_mappings\n\n        # --- 4. Save Results ---\n        logger.info(f\"\
      Saving mapped results to: {OUTPUT_FILE}\")\n        with open(OUTPUT_FILE, 'w',\
      \ encoding='utf-8') as f:\n            json.dump(all_mappings, f, indent=4)\n\
      \            \n        logger.info(\"Mapping process completed successfully!\"\
      )\n\n    except Exception as e:\n        logger.error(f\"An unexpected error\
      \ occurred during the mapping process: {e}\", exc_info=True)\n    finally:\n\
      \        # --- 5. Clean up ---\n        if pipeline:\n            logger.info(\"\
      Closing pipeline resources.\")\n            pipeline.close()\n\nif __name__\
      \ == \"__main__\":\n    main()"
    "embeddings":
      "embed_docs.py": "import json\nimport os\nimport logging\nimport time\nfrom\
        \ typing import List, Dict, Any\n\n# Ensure src is in path for imports if\
        \ run directly\nimport sys\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..'))) # To import from src\n\ntry:\n    from sentence_transformers\
        \ import SentenceTransformer\n    import torch \nexcept ModuleNotFoundError:\n\
        \    logging.error(\"Modules sentence_transformers or torch not found. Please\
        \ install them: pip install sentence-transformers torch\")\n    sys.exit(1)\n\
        \ntry:\n    from src import config # Use 'from src import config'\nexcept\
        \ ModuleNotFoundError:\n    # Fallback for running script directly from src/embeddings\
        \ where src might not be seen as a package root\n    # This is less ideal\
        \ but can help during direct script execution\n    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..')))\n    try:\n        import config\n    except ModuleNotFoundError:\n\
        \        logging.error(\"Error: Could not import 'config'. \"\n          \
        \          \"Ensure the script is run from the project root or 'src' is in\
        \ PYTHONPATH.\")\n        sys.exit(1)\n\n\nlogging.basicConfig(level=logging.INFO,\
        \ format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef load_enriched_documents(file_path:\
        \ str) -> List[Dict[str, Any]]:\n    \"\"\"Loads enriched documents from a\
        \ JSON file.\"\"\"\n    logging.info(f\"Loading enriched documents from {file_path}...\"\
        )\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n \
        \           documents = json.load(f)\n        logging.info(f\"Successfully\
        \ loaded {len(documents)} documents.\")\n        return documents\n    except\
        \ FileNotFoundError:\n        logging.error(f\"Error: Enriched documents file\
        \ not found at {file_path}\")\n        return []\n    except json.JSONDecodeError:\n\
        \        logging.error(f\"Error: Could not decode JSON from {file_path}\"\
        )\n        return []\n\ndef batch_embed_documents(\n    documents: List[Dict[str,\
        \ Any]], \n    model_name: str, \n    batch_size: int = 32,\n    device: str\
        \ = None\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Generates embeddings\
        \ for a list of documents using a SentenceTransformer model.\n\n    Args:\n\
        \        documents: A list of dictionaries, each with at least \"id\", \"\
        label\", and \"text\" keys.\n        model_name: The name of the SentenceTransformer\
        \ model to use.\n        batch_size: The number of documents to process in\
        \ each batch.\n        device: The device to use for computation (e.g., \"\
        cuda\", \"cpu\"). Auto-detects if None.\n\n    Returns:\n        A list of\
        \ dictionaries, each containing \"id\", \"label\", and \"embedding\" (list\
        \ of floats).\n    \"\"\"\n    if not documents:\n        logging.warning(\"\
        No documents provided for embedding.\")\n        return []\n\n    if device\
        \ is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"\
        cpu\"\n    logging.info(f\"Using device: {device}\")\n\n    logging.info(f\"\
        Loading SentenceTransformer model: {model_name}\")\n    try:\n        model\
        \ = SentenceTransformer(model_name, device=device, trust_remote_code=True)\n\
        \    except Exception as e:\n        logging.error(f\"Failed to load model\
        \ {model_name}: {e}\")\n        # Fallback to CPU if CUDA error during model\
        \ loading (e.g. out of memory)\n        if \"cuda\" in str(e).lower() and\
        \ device == \"cuda\":\n            logging.warning(\"CUDA error during model\
        \ load. Attempting to load on CPU.\")\n            device = \"cpu\"\n    \
        \        model = SentenceTransformer(model_name, device=device, trust_remote_code=True)\n\
        \        else:\n            raise\n\n    logging.info(\"Model loaded. Starting\
        \ embedding process...\")\n\n    texts_to_embed = [doc.get(\"text\", \"\"\
        ) for doc in documents] # Get text, default to empty string if missing\n \
        \   ids = [doc.get(\"id\") for doc in documents]\n    labels = [doc.get(\"\
        label\") for doc in documents]\n\n    all_embeddings_data = []\n    start_time\
        \ = time.time()\n\n    for i in range(0, len(texts_to_embed), batch_size):\n\
        \        batch_texts = texts_to_embed[i:i+batch_size]\n        batch_ids =\
        \ ids[i:i+batch_size]\n        batch_labels = labels[i:i+batch_size]\n\n \
        \       logging.info(f\"Processing batch {i//batch_size + 1}/{(len(texts_to_embed)\
        \ -1)//batch_size + 1} (size: {len(batch_texts)})\")\n        \n        try:\n\
        \            embeddings = model.encode(batch_texts, convert_to_tensor=False,\
        \ show_progress_bar=False) # Returns numpy array\n            \n         \
        \   for idx, embedding in enumerate(embeddings):\n                all_embeddings_data.append({\n\
        \                    \"id\": batch_ids[idx],\n                    \"label\"\
        : batch_labels[idx],\n                    \"embedding\": embedding.tolist()\
        \ # Convert numpy array to list for JSON serialization\n                })\n\
        \        except Exception as e:\n            logging.error(f\"Error embedding\
        \ batch starting at index {i}: {e}\")\n            # Optionally, decide how\
        \ to handle batch errors (e.g., skip batch, add placeholders)\n          \
        \  # For now, we'll just log and continue, resulting in missing embeddings\
        \ for that batch.\n            # To add placeholders:\n            # for j\
        \ in range(len(batch_texts)):\n            #     all_embeddings_data.append({\n\
        \            #         \"id\": batch_ids[j],\n            #         \"label\"\
        : batch_labels[j],\n            #         \"embedding\": None # Or a zero\
        \ vector of correct dimensionality\n            #     })\n\n\n    end_time\
        \ = time.time()\n    logging.info(f\"Embedding process completed for {len(all_embeddings_data)}\
        \ documents in {end_time - start_time:.2f} seconds.\")\n    \n    if hasattr(model,\
        \ 'tokenizer') and hasattr(model.tokenizer, 'model_max_length'):\n       \
        \ logging.info(f\"Model max sequence length: {model.tokenizer.model_max_length}\"\
        )\n    elif hasattr(model, 'max_seq_length'):\n         logging.info(f\"Model\
        \ max sequence length: {model.max_seq_length}\")\n\n\n    return all_embeddings_data\n\
        \ndef save_embeddings(embeddings_data: List[Dict[str, Any]], output_path:\
        \ str):\n    \"\"\"Saves the embeddings data to a JSON file.\"\"\"\n    if\
        \ not embeddings_data:\n        logging.warning(\"No embeddings data to save.\"\
        )\n        return\n\n    logging.info(f\"Saving {len(embeddings_data)} embeddings\
        \ to {output_path}...\")\n    try:\n        with open(output_path, 'w', encoding='utf-8')\
        \ as f:\n            json.dump(embeddings_data, f, indent=2) # Use indent\
        \ for readability, can remove for smaller file size\n        logging.info(\"\
        Successfully saved embeddings.\")\n    except IOError:\n        logging.error(f\"\
        Error: Could not write embeddings to {output_path}\")\n    except TypeError\
        \ as e:\n        logging.error(f\"TypeError during JSON serialization: {e}.\
        \ Check embedding data format.\")\n\n\ndef main():\n    \"\"\"Main function\
        \ to generate and save document embeddings.\"\"\"\n    # Ensure data directory\
        \ exists\n    os.makedirs(config.DATA_DIR, exist_ok=True)\n\n    enriched_docs\
        \ = load_enriched_documents(config.ENRICHED_DOCUMENTS_FILE)\n    if not enriched_docs:\n\
        \        logging.error(\"Failed to load enriched documents. Exiting.\")\n\
        \        return\n\n    embeddings_data = batch_embed_documents(\n        documents=enriched_docs,\n\
        \        model_name=config.EMBEDDING_MODEL_NAME,\n        batch_size=64 #\
        \ Adjust batch size based on available VRAM/RAM\n    )\n\n    if embeddings_data:\n\
        \        save_embeddings(embeddings_data, config.EMBEDDINGS_FILE)\n      \
        \  logging.info(f\"Generated and saved {len(embeddings_data)} embeddings.\"\
        )\n        if embeddings_data:\n            sample_embedding = embeddings_data[0]['embedding']\n\
        \            logging.info(f\"Sample embedding vector dimension: {len(sample_embedding)\
        \ if sample_embedding else 'N/A'}\")\n            logging.info(f\"Sample embedding\
        \ data point: {json.dumps(embeddings_data[0], indent=2, default=lambda x:\
        \ str(x)[:100])}\") # Truncate long embedding\n    else:\n        logging.warning(\"\
        No embeddings were generated.\")\n\nif __name__ == \"__main__\":\n    main()"
    "evaluation":
      "evaluate_pipeline.py": "# src/evaluation/evaluate_pipeline.py\n\nimport xml.etree.ElementTree\
        \ as ET\nimport os\nimport sys\nimport logging\nimport json\nfrom typing import\
        \ Tuple, List, Dict\n\n# Add project root to Python path to allow direct imports\
        \ from src\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..'))\nsys.path.append(PROJECT_ROOT)\n\nfrom src.retriever.hybrid_retriever\
        \ import HybridRetriever\nfrom src.rag_selectors.ollama_selector import OllamaSelector\
        \ # Import the new selector\nfrom src.rag_selectors.gemini_selector import\
        \ GeminiSelector # Import the old selector for reference\n\nfrom src.config\
        \ import (\n    ONTOLOGY_DUMP_JSON,\n    WHOOSH_INDEX_DIR,\n    FAISS_INDEX_PATH,\n\
        \    FAISS_METADATA_PATH,\n    EMBEDDING_MODEL_NAME,\n    CURIE_PREFIX_MAP,\n\
        \    DEFAULT_K_LEXICAL,\n    DEFAULT_K_VECTOR,\n    OLLAMA_SELECTOR_MODEL_NAME\
        \ # Import the model name\n)\nfrom src.utils.ontology_utils import uri_to_curie\n\
        \n# --- Configuration for this specific evaluation script ---\nEVALUATION_XML_FILE\
        \ = os.path.join(PROJECT_ROOT, \"data\", \"CafeteriaFCD_foodon_unique.xml\"\
        )\n# File to save detailed results of incorrect selections\nEVALUATION_OUTPUT_FILE\
        \ = os.path.join(PROJECT_ROOT, \"evaluation_results.json\")\n\n# --- Logging\
        \ Setup ---\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s -\
        \ %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n\
        # --- Re-used from evaluate_retriever_recall.py (no changes needed) ---\n\
        def parse_evaluation_xml(xml_file_path: str) -> list:\n    \"\"\"\n    Parses\
        \ the evaluation XML file to extract entities and their ground truth semantic\
        \ tags.\n    (This function is identical to the one in evaluate_retriever_recall.py)\n\
        \    \"\"\"\n    if not os.path.exists(xml_file_path):\n        logger.error(f\"\
        Evaluation XML file not found: {xml_file_path}\")\n        return []\n\n \
        \   gold_standard_data = []\n    try:\n        tree = ET.parse(xml_file_path)\n\
        \        root = tree.getroot()\n        for doc_idx, document_node in enumerate(root.findall('.//document')):\n\
        \            doc_id_node = document_node.find('id')\n            doc_id =\
        \ doc_id_node.text if doc_id_node is not None else f\"doc_{doc_idx}\"\n  \
        \          \n            for ann_idx, annotation_node in enumerate(document_node.findall('annotation')):\n\
        \                entity_text_node = annotation_node.find('text')\n       \
        \         semantic_tags_node = annotation_node.find('infon[@key=\"semantic_tags\"\
        ]')\n                ann_id_val = annotation_node.get('id', f\"ann_{doc_idx}_{ann_idx}\"\
        )\n\n                if entity_text_node is not None and semantic_tags_node\
        \ is not None and entity_text_node.text is not None and semantic_tags_node.text\
        \ is not None:\n                    entity_text = entity_text_node.text.strip()\n\
        \                    raw_tags = semantic_tags_node.text.strip()\n        \
        \            true_uris = {tag.strip() for tag in raw_tags.split(';') if tag.strip()}\n\
        \                    true_curies = {uri_to_curie(uri, CURIE_PREFIX_MAP) for\
        \ uri in true_uris} - {None}\n                    \n                    if\
        \ entity_text and true_curies:\n                        gold_standard_data.append({\n\
        \                            'text': entity_text,\n                      \
        \      'true_curies': true_curies,\n                            'doc_id':\
        \ doc_id,\n                            'ann_id': ann_id_val\n            \
        \            })\n    except ET.ParseError as e:\n        logger.error(f\"\
        Error parsing XML file {xml_file_path}: {e}\")\n        return []\n    \n\
        \    logger.info(f\"Successfully parsed {len(gold_standard_data)} entities\
        \ from {xml_file_path}\")\n    return gold_standard_data\n\n# --- New Evaluation\
        \ Logic for the Full Pipeline ---\ndef evaluate_full_pipeline(\n    retriever:\
        \ HybridRetriever, \n    selector: GeminiSelector,  #\n    gold_standard_data:\
        \ list, \n    lexical_k: int, \n    vector_k: int\n) -> Tuple[float, int,\
        \ int, int, int, List[Dict]]:\n    \"\"\"\n    Evaluates the full retrieval\
        \ and selection pipeline against the gold standard data.\n\n    Returns:\n\
        \        A tuple containing:\n        - accuracy_score (float)\n        -\
        \ total_entities_processed (int)\n        - correct_selections (int)\n   \
        \     - retrieval_failures (int): Count of queries that returned no candidates.\n\
        \        - selection_failures (int): Count of queries where the selector failed.\n\
        \        - incorrect_selections (list): A list of dictionaries detailing the\
        \ misses.\n    \"\"\"\n    total_entities_processed = 0\n    correct_selections\
        \ = 0\n    retrieval_failures = 0\n    selection_failures = 0\n    incorrect_selections\
        \ = []\n\n    if not gold_standard_data:\n        logger.warning(\"No gold\
        \ standard data provided for evaluation.\")\n        return 0.0, 0, 0, 0,\
        \ 0, []\n\n    for i, item in enumerate(gold_standard_data):\n        query_text\
        \ = item['text']\n        true_curies = item['true_curies']\n        \n  \
        \      total_entities_processed += 1\n        logger.info(f\"--- Processing\
        \ ({i+1}/{len(gold_standard_data)}): '{query_text}' (True: {true_curies})\
        \ ---\")\n\n        # 1. RETRIEVAL STEP: Get candidates\n        try:\n  \
        \          # Note: The HybridRetriever's search method should return a combined,\
        \ reranked list of candidates.\n            # We adapt to the provided retriever's\
        \ output format.\n            retriever_output_dict = retriever.search(\n\
        \                query_string=query_text, lexical_limit=lexical_k, vector_k=vector_k\n\
        \            )\n            lexical_results = retriever_output_dict.get(\"\
        lexical_results\", [])\n            vector_results = retriever_output_dict.get(\"\
        vector_results\", [])\n\n            # Combine and deduplicate\n         \
        \   candidates = []\n            seen_ids = set()\n            for doc in\
        \ lexical_results + vector_results:\n                doc_id = doc.get('id')\n\
        \                if doc_id and doc_id not in seen_ids:\n                 \
        \   candidates.append(doc)\n                    seen_ids.add(doc_id)\n   \
        \         \n            if not candidates:\n                logger.warning(f\"\
        Retrieval Failure: No candidates found for '{query_text}'.\")\n          \
        \      retrieval_failures += 1\n                continue\n\n        except\
        \ Exception as e:\n            logger.error(f\"Error during retrieval for\
        \ query '{query_text}': {e}\", exc_info=True)\n            retrieval_failures\
        \ += 1\n            continue\n\n        # 2. SELECTION STEP: Use OllamaSelector\n\
        \        try:\n            selection_result = selector.select_best_term(query=query_text,\
        \ candidates=candidates)\n\n            if not selection_result or \"chosen_id\"\
        \ not in selection_result:\n                logger.warning(f\"Selection Failure:\
        \ Selector did not return a valid choice for '{query_text}'.\")\n        \
        \        selection_failures += 1\n                continue\n            \n\
        \            chosen_curie = selection_result[\"chosen_id\"]\n\n        except\
        \ Exception as e:\n            logger.error(f\"Error during selection for\
        \ query '{query_text}': {e}\", exc_info=True)\n            selection_failures\
        \ += 1\n            continue\n        \n        # 3. COMPARISON STEP\n   \
        \     if chosen_curie in true_curies:\n            correct_selections += 1\n\
        \            logger.info(f\"✅ HIT! Query: '{query_text}'. Chosen: '{chosen_curie}'.\
        \ Correct.\")\n        else:\n            logger.info(f\"❌ MISS! Query: '{query_text}'.\
        \ Chosen: '{chosen_curie}', Expected: {true_curies}.\")\n            incorrect_selections.append({\n\
        \                \"query\": query_text,\n                \"chosen_curie\"\
        : chosen_curie,\n                \"true_curies\": list(true_curies),\n   \
        \             \"explanation\": selection_result.get(\"explanation\", \"N/A\"\
        ),\n                \"candidates_provided\": [cand['id'] for cand in candidates]\n\
        \            })\n\n    if (total_entities_processed - retrieval_failures -\
        \ selection_failures) == 0:\n        accuracy_score = 0.0\n    else:\n   \
        \     # Accuracy is based on the number of times the selector could make a\
        \ choice\n        accuracy_score = correct_selections / (total_entities_processed\
        \ - retrieval_failures - selection_failures)\n\n    return accuracy_score,\
        \ total_entities_processed, correct_selections, retrieval_failures, selection_failures,\
        \ incorrect_selections\n\ndef main():\n    logger.info(\"Starting Full Pipeline\
        \ Evaluation Script...\")\n    \n    # 1. Check for necessary model name in\
        \ config\n    if not OLLAMA_SELECTOR_MODEL_NAME:\n        logger.error(\"\
        OLLAMA_SELECTOR_MODEL_NAME is not set in config.py. Exiting.\")\n        return\n\
        \n    # 2. Parse Gold Standard XML\n    logger.info(f\"Loading gold standard\
        \ data from: {EVALUATION_XML_FILE}\")\n    gold_standard_data = parse_evaluation_xml(EVALUATION_XML_FILE)\n\
        \    if not gold_standard_data:\n        logger.error(\"Failed to load or\
        \ parse gold standard data. Exiting.\")\n        return\n\n    # 3. Initialize\
        \ Pipeline Components\n    try:\n        logger.info(\"Initializing HybridRetriever...\"\
        )\n        retriever = HybridRetriever(\n            ontology_data_path=ONTOLOGY_DUMP_JSON,\n\
        \            whoosh_index_dir=WHOOSH_INDEX_DIR,\n            faiss_index_path=FAISS_INDEX_PATH,\n\
        \            faiss_metadata_path=FAISS_METADATA_PATH,\n            embedding_model_name=EMBEDDING_MODEL_NAME\n\
        \        )\n        logger.info(\"HybridRetriever initialized successfully.\"\
        )\n\n        logger.info(f\"Initializing OllamaSelector with model '{OLLAMA_SELECTOR_MODEL_NAME}'...\"\
        )\n        selector = GeminiSelector(retriever=retriever)\n        logger.info(\"\
        OllamaSelector initialized successfully.\")\n\n    except Exception as e:\n\
        \        logger.error(f\"Failed to initialize pipeline components: {e}\",\
        \ exc_info=True)\n        return\n\n    # 4. Perform Evaluation\n    logger.info(\n\
        \        f\"Starting evaluation with Retriever(lexical_k={DEFAULT_K_LEXICAL},\
        \ vector_k={DEFAULT_K_VECTOR}) \"\n        f\"and Selector(model={OLLAMA_SELECTOR_MODEL_NAME})\"\
        \n    )\n    \n    accuracy, total, correct, ret_fails, sel_fails, misses\
        \ = evaluate_full_pipeline(\n        retriever, \n        selector, \n   \
        \     gold_standard_data, \n        lexical_k=DEFAULT_K_LEXICAL,\n       \
        \ vector_k=DEFAULT_K_VECTOR\n    )\n\n    # 5. Print and Save Results\n  \
        \  logger.info(\"--- Evaluation Complete ---\")\n    logger.info(f\"Total\
        \ entities evaluated: {total}\")\n    logger.info(f\"Retrieval Failures (no\
        \ candidates): {ret_fails}\")\n    logger.info(f\"Selection Failures (LLM\
        \ error): {sel_fails}\")\n    logger.info(\"-\" * 27)\n    valid_attempts\
        \ = total - ret_fails - sel_fails\n    logger.info(f\"Valid attempts for selector:\
        \ {valid_attempts}\")\n    logger.info(f\"Correct selections (Hits): {correct}\"\
        )\n    if valid_attempts > 0:\n        logger.info(f\"Accuracy: {accuracy:.4f}\
        \ ({correct}/{valid_attempts})\")\n    else:\n        logger.info(\"Accuracy:\
        \ N/A (no valid attempts were made)\")\n    \n    logger.info(f\"Saving {len(misses)}\
        \ incorrect selections to {EVALUATION_OUTPUT_FILE}\")\n    with open(EVALUATION_OUTPUT_FILE,\
        \ 'w', encoding='utf-8') as f:\n        json.dump(misses, f, indent=4)\n \
        \       \n    logger.info(\"Evaluation finished.\")\n\nif __name__ == \"__main__\"\
        :\n    if not os.path.exists(EVALUATION_XML_FILE):\n        logger.error(f\"\
        Evaluation XML file '{EVALUATION_XML_FILE}' not found.\")\n    else:\n   \
        \     main()"
      "evaluate_retriever_recall.py": "# src/evaluation/evaluate_retriever_recall.py\n\
        \nimport xml.etree.ElementTree as ET\nimport os\nimport sys\nimport logging\n\
        from collections import defaultdict\n\n# Add project root to Python path to\
        \ allow direct imports from src\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..'))\nsys.path.append(PROJECT_ROOT)\n\nfrom src.retriever.hybrid_retriever\
        \ import HybridRetriever\nfrom src.config import (\n    ONTOLOGY_DUMP_JSON,\n\
        \    WHOOSH_INDEX_DIR,\n    FAISS_INDEX_PATH,\n    FAISS_METADATA_PATH,\n\
        \    EMBEDDING_MODEL_NAME,\n    CURIE_PREFIX_MAP,\n    DEFAULT_K_LEXICAL,\n\
        \    DEFAULT_K_VECTOR,\n    DEFAULT_RERANK_K\n)\nfrom src.utils.ontology_utils\
        \ import uri_to_curie\n\n# --- Configuration for this specific evaluation\
        \ script ---\n# Path to your evaluation XML file\nEVALUATION_XML_FILE = os.path.join(PROJECT_ROOT,\
        \ \"data\", \"CafeteriaFCD_foodon.xml\")\n# Top K results from the combined\
        \ retriever output to consider for a match\nRECALL_AT_K = DEFAULT_RERANK_K\
        \ # How many retrieved items to check for a match\n\n# --- Logging Setup ---\n\
        # Stays with basicConfig as src.utils.logging.get_logger is \"to be developed\"\
        \nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s\
        \ - %(message)s')\nlogger = logging.getLogger(__name__)\n\ndef parse_evaluation_xml(xml_file_path:\
        \ str) -> list:\n    \"\"\"\n    Parses the evaluation XML file to extract\
        \ entities and their ground truth semantic tags.\n\n    Args:\n        xml_file_path\
        \ (str): Path to the XML file.\n\n    Returns:\n        list: A list of dictionaries,\
        \ each containing:\n              {'text': str, 'true_curies': set_of_str,\
        \ 'doc_id': str, 'ann_id': str}\n              Returns an empty list if parsing\
        \ fails.\n    \"\"\"\n    if not os.path.exists(xml_file_path):\n        logger.error(f\"\
        Evaluation XML file not found: {xml_file_path}\")\n        return []\n\n \
        \   gold_standard_data = []\n    try:\n        tree = ET.parse(xml_file_path)\n\
        \        root = tree.getroot()\n        for doc_idx, document_node in enumerate(root.findall('.//document')):\n\
        \            doc_id_node = document_node.find('id')\n            doc_id =\
        \ doc_id_node.text if doc_id_node is not None else f\"doc_{doc_idx}\"\n  \
        \          \n            annotations = document_node.findall('annotation')\n\
        \            for ann_idx, annotation_node in enumerate(annotations):\n   \
        \             entity_text_node = annotation_node.find('text')\n          \
        \      semantic_tags_node = annotation_node.find('infon[@key=\"semantic_tags\"\
        ]')\n                ann_id_val = annotation_node.get('id', f\"ann_{doc_idx}_{ann_idx}\"\
        )\n\n\n                if entity_text_node is not None and semantic_tags_node\
        \ is not None and entity_text_node.text is not None and semantic_tags_node.text\
        \ is not None:\n                    entity_text = entity_text_node.text.strip()\n\
        \                    \n                    raw_tags = semantic_tags_node.text.strip()\n\
        \                    true_uris = {tag.strip() for tag in raw_tags.split(';')\
        \ if tag.strip()}\n                    \n                    true_curies =\
        \ set()\n                    for uri in true_uris:\n                     \
        \   try:\n                            # Adjusted: Use CURIE_PREFIX_MAP\n \
        \                           curie = uri_to_curie(uri, CURIE_PREFIX_MAP)\n\
        \                            if curie: \n                                true_curies.add(curie)\n\
        \                            else:\n                                logger.warning(f\"\
        Could not convert URI to CURIE: {uri} for entity '{entity_text}' in {doc_id}\
        \ (ann: {ann_id_val})\")\n                        except Exception as e:\n\
        \                            logger.error(f\"Error converting URI {uri} to\
        \ CURIE: {e}\")\n                    \n                    if entity_text\
        \ and true_curies:\n                        gold_standard_data.append({\n\
        \                            'text': entity_text,\n                      \
        \      'true_curies': true_curies,\n                            'doc_id':\
        \ doc_id,\n                            'ann_id': ann_id_val\n            \
        \            })\n                    elif entity_text: # Has text but no convertible\
        \ true CURIEs\n                        logger.warning(f\"Entity '{entity_text}'\
        \ in {doc_id} (ann: {ann_id_val}) had no convertible true CURIEs from URIs:\
        \ {true_uris}\")\n\n                # else: # This can be too verbose if many\
        \ annotations don't have these specific fields\n                #     logger.debug(f\"\
        Annotation missing text or semantic_tags in {doc_id}, ann_id={ann_id_val}\"\
        )\n\n\n    except ET.ParseError as e:\n        logger.error(f\"Error parsing\
        \ XML file {xml_file_path}: {e}\")\n        return []\n    except Exception\
        \ as e:\n        logger.error(f\"An unexpected error occurred during XML parsing:\
        \ {e}\", exc_info=True)\n        return []\n        \n    logger.info(f\"\
        Successfully parsed {len(gold_standard_data)} entities with text and true\
        \ CURIEs from {xml_file_path}\")\n    return gold_standard_data\n\ndef evaluate_retriever(retriever:\
        \ HybridRetriever, gold_standard_data: list, recall_at_k: int, lexical_k:\
        \ int, vector_k: int) -> tuple[float, int, int]:\n    \"\"\"\n    Evaluates\
        \ the retriever against the gold standard data.\n\n    Args:\n        retriever\
        \ (HybridRetriever): The initialized hybrid retriever.\n        gold_standard_data\
        \ (list): List of gold standard entities and their CURIEs.\n        recall_at_k\
        \ (int): The K value for Recall@K (slice of combined results).\n        lexical_k\
        \ (int): Number of results to fetch from lexical search.\n        vector_k\
        \ (int): Number of results to fetch from vector search.\n\n    Returns:\n\
        \        tuple: (recall_score, total_entities_processed, hits)\n    \"\"\"\
        \n    total_entities_processed = 0\n    hits = 0\n    \n    if not gold_standard_data:\n\
        \        logger.warning(\"No gold standard data provided for evaluation.\"\
        )\n        return 0.0, 0, 0\n\n    for i, item in enumerate(gold_standard_data):\n\
        \        query_text = item['text']\n        true_curies = item['true_curies']\n\
        \        \n        if not query_text or not true_curies:\n            # This\
        \ should ideally be filtered by parse_evaluation_xml already\n           \
        \ logger.warning(f\"Skipping item with empty query text or true_curies: {item}\"\
        )\n            continue\n            \n        total_entities_processed +=\
        \ 1\n        \n        logger.debug(f\"({i+1}/{len(gold_standard_data)}) Querying\
        \ for: '{query_text}', True CURIEs: {true_curies}\")\n\n        try:\n   \
        \         # Adjusted: HybridRetriever.search returns a dict: {\"lexical_results\"\
        : [], \"vector_results\": []}\n            retriever_output_dict = retriever.search(\n\
        \                query_string=query_text,\n                lexical_limit=lexical_k,\n\
        \                vector_k=vector_k\n            )\n            \n        \
        \    lexical_results = retriever_output_dict.get(\"lexical_results\", [])\n\
        \            vector_results = retriever_output_dict.get(\"vector_results\"\
        , [])\n\n            # Combine and deduplicate results, lexical first then\
        \ vector.\n            # Scores are not comparable, so this is a simple merge\
        \ strategy.\n            # The 'id' field from result dicts contains the CURIE.\n\
        \            combined_ordered_results = []\n            seen_ids = set()\n\
        \n            for doc in lexical_results:\n                doc_id = doc.get('id')\
        \ # 'id' is the CURIE\n                if doc_id and doc_id not in seen_ids:\n\
        \                    combined_ordered_results.append(doc) # doc contains 'id',\
        \ 'label', 'score', etc.\n                    seen_ids.add(doc_id)\n     \
        \       \n            for doc in vector_results:\n                doc_id =\
        \ doc.get('id') # 'id' is the CURIE\n                if doc_id and doc_id\
        \ not in seen_ids:\n                    combined_ordered_results.append(doc)\n\
        \                    seen_ids.add(doc_id)\n            \n            # Extract\
        \ the CURIEs from the top `recall_at_k` combined documents\n            #\
        \ Adjusted: use doc['id'] as it stores the CURIE\n            retrieved_curies_set\
        \ = {doc['id'] for doc in combined_ordered_results[:recall_at_k]}\n      \
        \      logger.debug(f\"Retrieved CURIEs (top {recall_at_k} from combined):\
        \ {retrieved_curies_set}\")\n\n            if not true_curies.isdisjoint(retrieved_curies_set):\n\
        \                hits += 1\n                logger.info(f\"HIT! Query: '{query_text}'.\
        \ True: {true_curies}. Found in top {recall_at_k}: {true_curies.intersection(retrieved_curies_set)}\"\
        )\n            else:\n                logger.info(f\"MISS. Query: '{query_text}'.\
        \ True: {true_curies}. Top {recall_at_k} (combined) CURIEs: {retrieved_curies_set}\"\
        )\n                # For misses, log more details if needed, e.g. full combined_ordered_results\n\
        \                # logger.debug(f\"Full combined/ordered results for miss:\
        \ {combined_ordered_results}\")\n\n\n        except Exception as e:\n    \
        \        logger.error(f\"Error during retrieval or processing for query '{query_text}':\
        \ {e}\", exc_info=True)\n            \n    if total_entities_processed ==\
        \ 0:\n        logger.warning(\"No valid entities were processed for evaluation.\"\
        )\n        return 0.0, 0, 0\n        \n    recall_score = hits / total_entities_processed\n\
        \    return recall_score, total_entities_processed, hits\n\ndef main():\n\
        \    logger.info(\"Starting Retriever Evaluation Script...\")\n\n    # 1.\
        \ Parse the Gold Standard XML\n    logger.info(f\"Loading gold standard data\
        \ from: {EVALUATION_XML_FILE}\")\n    gold_standard_data = parse_evaluation_xml(EVALUATION_XML_FILE)\n\
        \    if not gold_standard_data:\n        logger.error(\"Failed to load or\
        \ parse gold standard data. Exiting.\")\n        return\n\n    # 2. Initialize\
        \ the HybridRetriever\n    logger.info(\"Initializing HybridRetriever...\"\
        )\n    try:\n        # Ensure all paths are valid and files exist where expected\
        \ by HybridRetriever\n        # Adjusted: Use ONTOLOGY_DUMP_JSON for ontology_data_path\n\
        \        if not os.path.exists(ONTOLOGY_DUMP_JSON):\n            logger.error(f\"\
        Ontology dump not found: {ONTOLOGY_DUMP_JSON}. Run ingestion pipeline.\")\n\
        \            return\n        if not os.path.exists(WHOOSH_INDEX_DIR) or not\
        \ os.listdir(WHOOSH_INDEX_DIR): # Check if dir exists and is not empty\n \
        \            logger.error(f\"Whoosh index directory is empty or not found:\
        \ {WHOOSH_INDEX_DIR}. Run ingestion pipeline.\")\n             return\n  \
        \      if not os.path.exists(FAISS_INDEX_PATH):\n            logger.error(f\"\
        FAISS index not found: {FAISS_INDEX_PATH}. Run ingestion pipeline.\")\n  \
        \          return\n        if not os.path.exists(FAISS_METADATA_PATH):\n \
        \           logger.error(f\"FAISS metadata not found: {FAISS_METADATA_PATH}.\
        \ Run ingestion pipeline.\")\n            return\n\n        retriever = HybridRetriever(\n\
        \            ontology_data_path=ONTOLOGY_DUMP_JSON, # Adjusted\n         \
        \   whoosh_index_dir=WHOOSH_INDEX_DIR,\n            faiss_index_path=FAISS_INDEX_PATH,\n\
        \            faiss_metadata_path=FAISS_METADATA_PATH,\n            embedding_model_name=EMBEDDING_MODEL_NAME\n\
        \        )\n        logger.info(\"HybridRetriever initialized successfully.\"\
        )\n    except Exception as e:\n        logger.error(f\"Failed to initialize\
        \ HybridRetriever: {e}\", exc_info=True)\n        return\n\n    # 3. Perform\
        \ Evaluation\n    logger.info(f\"Starting evaluation with Recall@{RECALL_AT_K}...\"\
        )\n    # Adjusted: Use DEFAULT_K_LEXICAL and DEFAULT_K_VECTOR\n    logger.info(f\"\
        HybridRetriever search params: Lexical K={DEFAULT_K_LEXICAL}, Vector K={DEFAULT_K_VECTOR}\"\
        )\n    \n    recall_score, total_entities, hits = evaluate_retriever(\n  \
        \      retriever, \n        gold_standard_data, \n        recall_at_k=RECALL_AT_K,\n\
        \        lexical_k=DEFAULT_K_LEXICAL, # Adjusted\n        vector_k=DEFAULT_K_VECTOR\
        \    # Adjusted\n    )\n\n    # 4. Print Results\n    logger.info(\"--- Evaluation\
        \ Results ---\")\n    logger.info(f\"Total entities processed: {total_entities}\"\
        )\n    logger.info(f\"Number of hits (at least one true CURIE found in top\
        \ {RECALL_AT_K} combined results): {hits}\")\n    if total_entities > 0:\n\
        \        logger.info(f\"Recall@{RECALL_AT_K}: {recall_score:.4f}\")\n    else:\n\
        \        logger.info(\"Recall not calculated as no entities were processed.\"\
        )\n\n    # 5. Clean up\n    try:\n        if hasattr(retriever, 'close') and\
        \ callable(retriever.close):\n            retriever.close()\n            logger.info(\"\
        Retriever resources closed.\")\n    except Exception as e:\n        logger.error(f\"\
        Error closing retriever resources: {e}\")\n\nif __name__ == \"__main__\":\n\
        \    eval_dir = os.path.dirname(EVALUATION_XML_FILE)\n    if not os.path.exists(eval_dir):\n\
        \        try:\n            os.makedirs(eval_dir)\n            logger.info(f\"\
        Created directory: {eval_dir}\")\n            logger.info(f\"Please place\
        \ '{os.path.basename(EVALUATION_XML_FILE)}' in {eval_dir} to run the evaluation.\"\
        )\n        except OSError as e:\n            logger.error(f\"Failed to create\
        \ directory {eval_dir}: {e}\")\n            sys.exit(1) # Exit if cannot create\
        \ data directory for eval file\n    \n    if not os.path.exists(EVALUATION_XML_FILE):\n\
        \        logger.error(f\"Evaluation XML file '{EVALUATION_XML_FILE}' not found.\
        \ Please place it in the correct directory.\")\n    else:\n        main()"
    "ingestion":
      "__init__.py": ""
      "build_lexical_index.py": "# src/ingestion/build_lexical_index.py\nimport sys\n\
        import os\n\n# --- Add project root to sys.path ---\nproject_root = os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..'))\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n\
        # --- End sys.path modification ---\n\nimport json\nfrom whoosh.index import\
        \ create_in, open_dir\nfrom whoosh.fields import Schema, ID, TEXT\nfrom whoosh.qparser\
        \ import QueryParser\nimport traceback\n\n# Now import using the 'src' package\
        \ prefix\nfrom src.config import ONTOLOGY_DUMP_JSON, WHOOSH_INDEX_DIR\n\n\
        # ... (rest of the build_lexical_index.py code, ensuring imports like `from\
        \ src.config` are used)\ndef build_index(json_path: str, index_dir: str):\n\
        \    print(f\"Building lexical index from {json_path} into {index_dir}\")\n\
        \n    schema = Schema(\n        curie=ID(stored=True, unique=True),\n    \
        \    label=TEXT(stored=True, analyzer=None), # Keep None analyzer for exact\
        \ matches if needed, or use default\n        synonyms=TEXT(stored=True),\n\
        \        definition=TEXT(stored=True),\n        relations_text=TEXT(stored=False)\n\
        \    )\n\n    os.makedirs(index_dir, exist_ok=True)\n    \n    try:\n    \
        \    # create_in will overwrite if index exists. If you want to open, use\
        \ open_dir.\n        ix = create_in(index_dir, schema)\n        print(f\"\
        Whoosh index schema created in {index_dir}\")\n    except Exception as e:\n\
        \         print(f\"Error creating Whoosh index directory or schema: {e}\"\
        )\n         traceback.print_exc()\n         return\n\n    writer = ix.writer()\n\
        \    try:\n        with open(json_path, 'r', encoding='utf-8') as f:\n   \
        \         ontology_data = json.load(f)\n\n        print(f\"Indexing {len(ontology_data)}\
        \ entities...\")\n        indexed_count = 0\n        for curie, data in ontology_data.items():\n\
        \            label = data.get('label', '') or '' # Ensure string\n       \
        \     synonyms_list = data.get('synonyms', [])\n            synonyms = \"\
        \ \".join(s for s in synonyms_list if s) # Join non-empty synonyms\n\n   \
        \         definition = data.get('definition', '') or ''\n\n            relations_text\
        \ = \"\"\n            relations_dict = data.get('relations', {})\n       \
        \     if relations_dict:\n                 relations_text = \" \".join([\n\
        \                     f\"{prop.replace('_', ' ')} {' '.join(targets)}\" for\
        \ prop, targets in relations_dict.items()\n                 ])\n         \
        \   \n            # Skip adding document if all text fields are empty (optional)\n\
        \            # if not label and not synonyms and not definition and not relations_text:\n\
        \            #     print(f\"Skipping empty document for CURIE: {curie}\")\n\
        \            #     continue\n\n            writer.add_document(\n        \
        \        curie=curie,\n                label=label,\n                synonyms=synonyms,\n\
        \                definition=definition,\n                relations_text=relations_text\n\
        \            )\n            indexed_count += 1\n            if indexed_count\
        \ % 1000 == 0:\n                 print(f\"Indexed {indexed_count} entities...\"\
        )\n        \n        print(f\"Committing index with {indexed_count} documents.\"\
        )\n        writer.commit()\n        print(\"Lexical index built successfully.\"\
        )\n\n    except FileNotFoundError:\n        print(f\"Error: Ontology dump\
        \ file not found at {json_path}\")\n        traceback.print_exc()\n      \
        \  writer.cancel() \n    except Exception as e:\n        print(f\"An error\
        \ occurred during index building: {e}\")\n        traceback.print_exc()\n\
        \        writer.cancel()\n\nif __name__ == \"__main__\":\n    if not os.path.exists(ONTOLOGY_DUMP_JSON):\n\
        \        print(f\"Error: Ontology dump file {ONTOLOGY_DUMP_JSON} not found.\"\
        )\n        print(\"Please run src/ingestion/parse_ontology.py first.\")\n\
        \    else:\n        build_index(ONTOLOGY_DUMP_JSON, WHOOSH_INDEX_DIR)"
      "enrich_documents.py": "import json\nimport os\nimport logging\nfrom typing\
        \ import Dict, Any, List, Optional\n\n# --- Start of corrected import block\
        \ ---\nimport sys\n# Calculate the project root directory based on the script's\
        \ location\n# For .../onto_rag/src/ingestion/enrich_documents.py, _PROJECT_ROOT\
        \ becomes .../onto_rag\n_PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..'))\n\n# Add the project root to sys.path if it's not already there\n\
        if _PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, _PROJECT_ROOT) #\
        \ Insert at the beginning\n\ntry:\n    from src import config\n    # from\
        \ src.utils.ontology_utils import curie_to_uri # This specific utility is\
        \ not used in this script,\n                                             \
        \         # but this is how you'd import it if needed.\nexcept ModuleNotFoundError\
        \ as e:\n    print(f\"CRITICAL ERROR: Could not import project modules. Exception:\
        \ {e}\")\n    print(f\"This script expects to be run in a way that the 'src'\
        \ package is discoverable.\")\n    print(f\"Attempted to add project root\
        \ '{_PROJECT_ROOT}' to sys.path.\")\n    print(f\"Current sys.path: {sys.path}\"\
        )\n    print(\"Please ensure you are running this script from the project's\
        \ root directory ('onto_rag/'), for example:\")\n    print(\"  python src/ingestion/enrich_documents.py\"\
        )\n    print(\"Also ensure that 'src/__init__.py' and 'src/utils/__init__.py'\
        \ (if using utils) exist.\")\n    sys.exit(1)\n\nlogging.basicConfig(level=logging.INFO,\
        \ format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef get_label_for_curie(curie:\
        \ str, ontology_data: Dict[str, Dict[str, Any]], default_value: Optional[str]\
        \ = None) -> Optional[str]:\n    \"\"\"\n    Retrieves the label for a given\
        \ CURIE from the ontology data.\n    Args:\n        curie: The CURIE string\
        \ (e.g., \"FOODON:00001234\").\n        ontology_data: The loaded ontology\
        \ data dictionary.\n        default_value: Value to return if CURIE not found\
        \ or has no label. Defaults to the CURIE itself.\n    Returns:\n        The\
        \ label string, or the default_value.\n    \"\"\"\n    if default_value is\
        \ None:\n        default_value = curie # Fallback to CURIE if no specific\
        \ default\n\n    term_info = ontology_data.get(curie)\n    if term_info and\
        \ term_info.get(\"label\"):\n        return term_info[\"label\"]\n    return\
        \ default_value\n\ndef get_relation_name(relation_curie: str) -> str:\n  \
        \  \"\"\"\n    Gets a human-readable name for a relation CURIE using RELATION_CONFIG.\n\
        \    \"\"\"\n    # Exact match\n    if relation_curie in config.RELATION_CONFIG:\n\
        \        return config.RELATION_CONFIG[relation_curie][\"label\"]\n    \n\
        \    # Check for generic FoodON prefix if specific one not found\n    generic_foodon_prefix\
        \ = \"obo:FOODON_\"\n    if relation_curie.startswith(generic_foodon_prefix)\
        \ and generic_foodon_prefix in config.RELATION_CONFIG:\n         # Attempt\
        \ to make it slightly more readable if it's like \"obo:FOODON_0000XXXX\"\n\
        \        relation_suffix = relation_curie.split('_')[-1]\n        return f\"\
        FoodON relation {relation_suffix}\" # or config.RELATION_CONFIG[generic_foodon_prefix][\"\
        label\"]\n\n    # Fallback for other OBO relations\n    if relation_curie.startswith(\"\
        obo:\"):\n        name_part = relation_curie.split(':')[-1].replace(\"_\"\
        , \" \")\n        return name_part\n\n    return relation_curie # Fallback\
        \ to the CURIE itself\n\ndef create_enriched_documents(ontology_data_path:\
        \ str, output_path: str) -> List[Dict[str, Any]]:\n    \"\"\"\n    Creates\
        \ enriched text documents for each ontology entry.\n    Args:\n        ontology_data_path:\
        \ Path to the ontology_dump.json file.\n        output_path: Path to save\
        \ the enriched_documents.json file.\n    Returns:\n        A list of enriched\
        \ document dictionaries.\n    \"\"\"\n    logging.info(f\"Loading ontology\
        \ data from {ontology_data_path}...\")\n    try:\n        with open(ontology_data_path,\
        \ 'r', encoding='utf-8') as f:\n            ontology_data = json.load(f)\n\
        \    except FileNotFoundError:\n        logging.error(f\"Error: Ontology data\
        \ file not found at {ontology_data_path}\")\n        return []\n    except\
        \ json.JSONDecodeError:\n        logging.error(f\"Error: Could not decode\
        \ JSON from {ontology_data_path}\")\n        return []\n\n    logging.info(f\"\
        Successfully loaded {len(ontology_data)} terms.\")\n\n    enriched_docs =\
        \ []\n\n    for term_curie, term_data in ontology_data.items():\n        doc_parts\
        \ = []\n\n        # 1. Label\n        label = term_data.get(\"label\")\n \
        \       if not label:\n            logging.warning(f\"Term {term_curie} has\
        \ no label. Skipping for enriched document (or using CURIE as label).\")\n\
        \            # Consider if we want to enrich docs for terms without labels.\
        \ For now, let's use CURIE if no label.\n            label = term_curie \n\
        \        \n        doc_parts.append(f\"{label}.\")\n\n        # 2. Definition\n\
        \        definition = term_data.get(\"definition\")\n        if definition:\n\
        \            doc_parts.append(f\"{definition}.\")\n\n        # 3. Synonyms\n\
        \        synonyms = term_data.get(\"synonyms\")\n        if synonyms:\n  \
        \          synonyms_text = \"; \".join(synonyms)\n            doc_parts.append(f\"\
        Also known as: {synonyms_text}.\")\n\n        # 4. Parents (direct subclasses)\n\
        \        parent_curies = term_data.get(\"parents\", [])\n        if parent_curies:\n\
        \            parent_labels = [get_label_for_curie(p_curie, ontology_data)\
        \ for p_curie in parent_curies]\n            parent_labels_filtered = [l for\
        \ l in parent_labels if l] # Filter out None if get_label_for_curie returns\
        \ None\n            if parent_labels_filtered:\n                if len(parent_labels_filtered)\
        \ == 1:\n                    doc_parts.append(f\"Is a type of: {parent_labels_filtered[0]}.\"\
        )\n                else:\n                    doc_parts.append(f\"Is a type\
        \ of: {'; '.join(parent_labels_filtered)}.\")\n        \n        # 5. Relations\
        \ (including facets expressed as object properties)\n        relations = term_data.get(\"\
        relations\", {})\n        relation_texts = []\n        for rel_curie, target_curies_list\
        \ in relations.items():\n            rel_name = get_relation_name(rel_curie)\n\
        \            target_labels = [get_label_for_curie(t_curie, ontology_data)\
        \ for t_curie in target_curies_list]\n            target_labels_filtered =\
        \ [l for l in target_labels if l]\n            if target_labels_filtered:\n\
        \                relation_texts.append(f\"{rel_name}: {', '.join(target_labels_filtered)}\"\
        )\n        \n        if relation_texts:\n            doc_parts.append(\"Key\
        \ characteristics include: \" + \"; \".join(relation_texts) + \".\")\n\n \
        \       # Combine all parts into a single text\n        enriched_text = \"\
        \ \".join(doc_parts).replace(\"..\", \".\").strip() # Clean up potential double\
        \ periods\n\n        enriched_docs.append({\n            \"id\": term_curie,\n\
        \            \"label\": label if label != term_curie else term_data.get(\"\
        label\", term_curie), # Store original label if available\n            \"\
        text\": enriched_text\n        })\n\n    logging.info(f\"Created {len(enriched_docs)}\
        \ enriched documents.\")\n\n    logging.info(f\"Saving enriched documents\
        \ to {output_path}...\")\n    try:\n        with open(output_path, 'w', encoding='utf-8')\
        \ as f:\n            json.dump(enriched_docs, f, indent=2)\n        logging.info(\"\
        Successfully saved enriched documents.\")\n    except IOError:\n        logging.error(f\"\
        Error: Could not write enriched documents to {output_path}\")\n\n    return\
        \ enriched_docs\n\ndef main():\n    \"\"\"Main function to create enriched\
        \ documents.\"\"\"\n    # Ensure data directory exists (though config.py should\
        \ handle it)\n    os.makedirs(config.DATA_DIR, exist_ok=True)\n\n    enriched_documents\
        \ = create_enriched_documents(\n        ontology_data_path=config.ONTOLOGY_DUMP_JSON,\n\
        \        output_path=config.ENRICHED_DOCUMENTS_FILE\n    )\n\n    if enriched_documents:\n\
        \        logging.info(f\"Processed {len(enriched_documents)} documents.\"\
        )\n        # Optionally print a sample\n        if len(enriched_documents)\
        \ > 0:\n            logging.info(\"Sample enriched document:\")\n        \
        \    logging.info(json.dumps(enriched_documents[0], indent=2))\n    else:\n\
        \        logging.warning(\"No enriched documents were created.\")\n\nif __name__\
        \ == \"__main__\":\n    main()"
      "parse_ontology.py": "# src/ingestion/parse_ontology.py\nimport sys\nimport\
        \ os\nimport logging # Import logging\n\n# --- Add project root to sys.path\
        \ ---\nproject_root = os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..'))\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n\
        # --- End sys.path modification ---\n\nimport rdflib\nfrom rdflib import Graph,\
        \ Namespace, URIRef, RDFS, OWL, RDF\nfrom typing import Dict, List, Any\n\
        import json\nimport traceback\n\n# Now import using the 'src' package prefix\n\
        from src.config import (\n    FOODON_PATH,                # Adjusted: Was\
        \ ONTOLOGY_FILE\n    CHEBI_PATH,                # Adjusted: Was CHEBI_FILE\n\
        \    ONTOLOGY_DUMP_JSON,         # Adjusted: Was ONTOLOGY_DUMP_PATH\n    CURIE_PREFIX_MAP,\
        \           # Adjusted: Was NAMESPACE_MAP\n    RELATION_CONFIG,          \
        \  # New: For relation names\n    TARGET_RELATIONS_CURIES,    # New: For relation\
        \ URIs\n    IAO_NS_STR,                 # Using string constants for Namespace\
        \ definitions\n    OBOINOWL_NS_STR,\n    # Add other NS_STR if needed for\
        \ local Namespace objects\n)\n# Assuming src.utils.ontology_utils is already\
        \ correct and uses CURIE_PREFIX_MAP\nfrom src.utils.ontology_utils import\
        \ uri_to_curie, curie_to_uri\n\n# --- Logging Setup ---\n# Using basicConfig\
        \ as src.utils.logging.get_logger is \"to be developed\"\n# You can customize\
        \ this further if needed.\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s\
        \ - %(levelname)s - %(module)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\
        \n\n# Define commonly used namespaces (can still use these locally for convenience)\n\
        # It's safer to use the full URI string from config if available, or construct\
        \ it.\nIAO = Namespace(IAO_NS_STR)\nOBOINOWL = Namespace(OBOINOWL_NS_STR)\n\
        # FOODON_BASE_URI = None # Find FOODON base URI from CURIE_PREFIX_MAP\n# for\
        \ base, prefix in CURIE_PREFIX_MAP.items():\n#     if prefix == \"FOODON\"\
        :\n#         FOODON_BASE_URI = base\n#         break\n# FOODON = Namespace(FOODON_BASE_URI)\
        \ if FOODON_BASE_URI else None\n# If FOODON Namespace object is not strictly\
        \ needed for queries, direct URI construction is fine.\n\ndef load_ontology(path:\
        \ str) -> rdflib.Graph:\n    g = Graph()\n    try:\n        logger.info(f\"\
        Loading ontology from: {path}\")\n        try:\n            g.parse(path,\
        \ format=\"application/rdf+xml\")\n        except Exception as e_xml:\n  \
        \          logger.warning(f\"Failed to parse as RDF/XML: {e_xml}. Trying Turtle...\"\
        )\n            try:\n                g.parse(path, format=\"turtle\")\n  \
        \          except Exception as e_ttl:\n                logger.warning(f\"\
        Failed to parse as Turtle: {e_ttl}. Trying auto-detection...\")\n        \
        \        g.parse(path)\n\n        logger.info(f\"Ontology loaded successfully.\
        \ Contains {len(g)} triples.\")\n        return g\n    except FileNotFoundError:\n\
        \        logger.error(f\"Error: Ontology file not found at {path}\")\n   \
        \     raise\n    except Exception as e:\n        logger.error(f\"Error parsing\
        \ ontology file {path}: {e}\")\n        traceback.print_exc()\n        raise\n\
        \ndef get_ancestors(g: Graph, term_uri: URIRef, prefix_map: Dict[str, str],\
        \ visited_uris: set = None) -> List[str]:\n    if visited_uris is None:\n\
        \        visited_uris = set()\n\n    ancestor_curies = set()\n    for parent_uri\
        \ in g.objects(subject=term_uri, predicate=RDFS.subClassOf):\n        if isinstance(parent_uri,\
        \ URIRef) and parent_uri != OWL.Thing:\n            if parent_uri not in visited_uris:\n\
        \                visited_uris.add(parent_uri)\n                # Pass the\
        \ prefix_map explicitly\n                parent_curie = uri_to_curie(parent_uri,\
        \ prefix_map)\n                if parent_curie and parent_curie != str(parent_uri):\n\
        \                    ancestor_curies.add(parent_curie)\n                 \
        \   ancestor_curies.update(get_ancestors(g, parent_uri, prefix_map, visited_uris))\n\
        \    return list(ancestor_curies)\n\n\ndef extract_labels_and_synonyms(g:\
        \ Graph, prefix_map: Dict[str, str]) -> Dict[str, Dict[str, Any]]:\n    data\
        \ = {}\n    relevant_predicates = [\n        RDFS.label,\n        OBOINOWL.hasExactSynonym,\
        \ OBOINOWL.hasRelatedSynonym,\n        OBOINOWL.hasNarrowSynonym, OBOINOWL.hasBroadSynonym\n\
        \    ]\n    \n    processed_subjects = set()\n\n    for pred in relevant_predicates:\n\
        \        for s_uri in g.subjects(predicate=pred):\n            if not isinstance(s_uri,\
        \ URIRef) or s_uri in processed_subjects:\n                continue\n    \
        \        \n            # Pass the prefix_map explicitly\n            curie\
        \ = uri_to_curie(s_uri, prefix_map)\n            # Optional: Filter for specific\
        \ prefixes if desired, e.g., only FOODON, IAO, OBO terms.\n            # This\
        \ was present in your original script; keeping it commented for now for broader\
        \ extraction.\n            # if not any(curie.startswith(p) for p in [\"FOODON:\"\
        , \"IAO:\", \"RO:\", \"BFO:\", \"obo:\", \"CHEBI:\"]): # Example\n       \
        \     #     # logger.debug(f\"Skipping non-ontology CURIE for labels/syns:\
        \ {curie} from {s_uri}\")\n            #     continue\n            if not\
        \ curie or curie == str(s_uri): # Skip if not converted to a CURIE effectively\n\
        \                continue\n\n            if curie not in data:\n         \
        \       data[curie] = {\"label\": None, \"synonyms\": []}\n\n            #\
        \ Label\n            label_val = g.value(subject=s_uri, predicate=RDFS.label)\n\
        \            if label_val and isinstance(label_val, rdflib.Literal):\n   \
        \             data[curie][\"label\"] = str(label_val)\n\n            # Synonyms\n\
        \            current_synonyms = []\n            for syn_prop in [OBOINOWL.hasExactSynonym,\
        \ OBOINOWL.hasRelatedSynonym,\n                             OBOINOWL.hasNarrowSynonym,\
        \ OBOINOWL.hasBroadSynonym]:\n                for syn_obj in g.objects(subject=s_uri,\
        \ predicate=syn_prop):\n                    if isinstance(syn_obj, rdflib.Literal):\n\
        \                        current_synonyms.append(str(syn_obj))\n         \
        \   \n            # Ensure synonyms list exists and extend uniquely\n    \
        \        if \"synonyms\" not in data[curie] or data[curie][\"synonyms\"] is\
        \ None: # Handle if somehow becomes None\n                data[curie][\"synonyms\"\
        ] = []\n            for s in current_synonyms:\n                if s not in\
        \ data[curie][\"synonyms\"]:\n                    data[curie][\"synonyms\"\
        ].append(s)\n\n            processed_subjects.add(s_uri)\n            \n \
        \   final_data = {k: v for k, v in data.items() if v.get(\"label\") or v.get(\"\
        synonyms\")}\n    logger.info(f\"Extracted labels and synonyms for {len(final_data)}\
        \ terms.\")\n    return final_data\n\n\ndef extract_definitions(g: Graph,\
        \ prefix_map: Dict[str, str]) -> Dict[str, str]:\n    definitions = {}\n \
        \   definition_prop_uri = IAO['0000115'] # IAO:0000115 is 'definition'\n \
        \   for s_uri in g.subjects(predicate=definition_prop_uri):\n        if not\
        \ isinstance(s_uri, URIRef):\n            continue\n        \n        # Pass\
        \ the prefix_map explicitly\n        curie = uri_to_curie(s_uri, prefix_map)\n\
        \        # Optional: Filter for specific prefixes\n        # if not any(curie.startswith(p)\
        \ for p in [\"FOODON:\", \"IAO:\", \"RO:\", \"BFO:\", \"obo:\", \"CHEBI:\"\
        ]):\n        #     # logger.debug(f\"Skipping non-ontology CURIE for definitions:\
        \ {curie} from {s_uri}\")\n        #     continue\n        if not curie or\
        \ curie == str(s_uri):\n            continue\n        \n        def_obj =\
        \ g.value(subject=s_uri, predicate=definition_prop_uri)\n        if def_obj\
        \ and isinstance(def_obj, rdflib.Literal):\n            definitions[curie]\
        \ = str(def_obj)\n            \n    logger.info(f\"Extracted definitions for\
        \ {len(definitions)} terms.\")\n    return definitions\n\ndef extract_hierarchy(g:\
        \ Graph, prefix_map: Dict[str, str]) -> Dict[str, Dict[str, List[str]]]:\n\
        \    hierarchy_data = {}\n    all_terms_in_hierarchy = set()\n    for s, p,\
        \ o in g.triples((None, RDFS.subClassOf, None)):\n        if isinstance(s,\
        \ URIRef): all_terms_in_hierarchy.add(s)\n        if isinstance(o, URIRef):\
        \ all_terms_in_hierarchy.add(o)\n    \n    for term_uri in all_terms_in_hierarchy:\n\
        \        if term_uri == OWL.Thing:\n            continue\n\n        # Pass\
        \ the prefix_map explicitly\n        curie = uri_to_curie(term_uri, prefix_map)\n\
        \        # Optional: Filter\n        # if not any(curie.startswith(p) for\
        \ p in [\"FOODON:\", \"IAO:\", \"RO:\", \"BFO:\", \"obo:\", \"CHEBI:\"]):\n\
        \        #     # logger.debug(f\"Skipping non-ontology CURIE for hierarchy:\
        \ {curie} from {term_uri}\")\n        #     continue\n        if not curie\
        \ or curie == str(term_uri):\n            continue\n\n        direct_parent_curies\
        \ = []\n        for parent_uri in g.objects(subject=term_uri, predicate=RDFS.subClassOf):\n\
        \            if isinstance(parent_uri, URIRef) and parent_uri != OWL.Thing:\n\
        \                # Pass the prefix_map explicitly\n                parent_curie\
        \ = uri_to_curie(parent_uri, prefix_map)\n                if parent_curie\
        \ and parent_curie != str(parent_uri):\n                    direct_parent_curies.append(parent_curie)\n\
        \        \n        ancestor_curies = get_ancestors(g, term_uri, prefix_map,\
        \ visited_uris=set())\n        \n        if direct_parent_curies or ancestor_curies:\n\
        \            hierarchy_data[curie] = {\n                \"parents\": list(set(direct_parent_curies)),\n\
        \                \"ancestors\": list(set(ancestor_curies))\n            }\n\
        \            \n    logger.info(f\"Extracted hierarchy data for {len(hierarchy_data)}\
        \ terms.\")\n    return hierarchy_data\n\n\n# Adjusted to match outline: props_to_extract:\
        \ Dict[str, str] (readable_name -> URI_string)\ndef extract_relations(g: Graph,\
        \ props_to_extract: Dict[str, str], prefix_map: Dict[str, str]) -> Dict[str,\
        \ Dict[str, List[str]]]:\n    relations_data = {}\n    \n    for term_uri\
        \ in g.subjects(unique=True):\n        if not isinstance(term_uri, URIRef):\n\
        \            continue\n\n        # Pass the prefix_map explicitly\n      \
        \  curie = uri_to_curie(term_uri, prefix_map)\n        # Optional: Filter\n\
        \        # if not any(curie.startswith(p) for p in [\"FOODON:\", \"IAO:\"\
        , \"RO:\", \"BFO:\", \"obo:\", \"CHEBI:\"]):\n        #     # logger.debug(f\"\
        Skipping non-ontology CURIE for relations: {curie} from {term_uri}\")\n  \
        \      #     continue\n        if not curie or curie == str(term_uri):\n \
        \           continue\n\n        term_specific_relations = {}\n        for\
        \ rel_readable_name, rel_uri_str in props_to_extract.items(): # rel_name is\
        \ readable_name\n            rel_uri = URIRef(rel_uri_str)\n            target_curies\
        \ = []\n            for target_obj in g.objects(subject=term_uri, predicate=rel_uri):\n\
        \                if isinstance(target_obj, URIRef):\n                    #\
        \ Pass the prefix_map explicitly\n                    target_curie = uri_to_curie(target_obj,\
        \ prefix_map)\n                    if target_curie and target_curie != str(target_obj):\n\
        \                        target_curies.append(target_curie)\n            \n\
        \            if target_curies:\n                term_specific_relations[rel_readable_name]\
        \ = list(set(target_curies))\n\n        if term_specific_relations:\n    \
        \        relations_data[curie] = term_specific_relations\n            \n \
        \   logger.info(f\"Extracted relations for {len(relations_data)} terms based\
        \ on {len(props_to_extract)} specified properties.\")\n    return relations_data\n\
        \n\ndef main():\n    logger.info(\"Starting ontology parsing...\")\n    #\
        \ Adjusted: Use ONTOLOGY_DUMP_JSON for output path\n    data_dir = os.path.dirname(ONTOLOGY_DUMP_JSON)\n\
        \    os.makedirs(data_dir, exist_ok=True)\n\n    try:\n        # Adjusted:\
        \ Use FOODON_PATH as input ontology file\n        ontology_paths = [FOODON_PATH,\
        \ CHEBI_PATH]\n        merged_graph = Graph()\n\n        # 3. Load and merge\
        \ all ontologies\n        for path in ontology_paths:\n            if os.path.exists(path):\n\
        \                logger.info(f\"Loading and merging ontology from: {path}\"\
        )\n                g = load_ontology(path) # Your existing load_ontology function\n\
        \                merged_graph += g\n            else:\n                logger.warning(f\"\
        Ontology file not found at {path}, skipping.\")\n        \n        logger.info(f\"\
        Total merged graph contains {len(merged_graph)} triples.\")\n        \n  \
        \      # From now on, use the `merged_graph` for all extractions\n       \
        \ g = merged_graph\n\n        # Prepare relation properties for extract_relations\n\
        \        # extract_relations expects: Dict[readable_name, full_uri_string]\n\
        \        # RELATION_CONFIG has: {curie_str: {\"label\": readable_name, \"\
        prefix\": prefix_str}}\n        # TARGET_RELATIONS_CURIES is List[curie_str]\n\
        \        \n        relation_properties_for_extraction = {}\n        for rel_curie_str\
        \ in TARGET_RELATIONS_CURIES:\n            if rel_curie_str in RELATION_CONFIG:\n\
        \                config_entry = RELATION_CONFIG[rel_curie_str]\n         \
        \       readable_name = config_entry.get(\"label\", rel_curie_str) # Fallback\
        \ to CURIE if no label\n                \n                # Convert relation\
        \ CURIE to full URI using curie_to_uri and CURIE_PREFIX_MAP\n            \
        \    full_rel_uri = curie_to_uri(rel_curie_str, CURIE_PREFIX_MAP)\n      \
        \          if full_rel_uri:\n                    relation_properties_for_extraction[readable_name]\
        \ = str(full_rel_uri)\n                else:\n                    logger.warning(f\"\
        Could not convert relation CURIE {rel_curie_str} to URI. Skipping this relation.\"\
        )\n            else:\n                logger.warning(f\"Relation CURIE {rel_curie_str}\
        \ from TARGET_RELATIONS_CURIES not found in RELATION_CONFIG.\")\n        \n\
        \        logger.info(f\"Prepared {len(relation_properties_for_extraction)}\
        \ relation properties for extraction.\")\n\n\n        logger.info(\"\\nExtracting\
        \ data...\")\n        # Pass CURIE_PREFIX_MAP to all extraction functions\n\
        \        labels_synonyms = extract_labels_and_synonyms(g, CURIE_PREFIX_MAP)\n\
        \        definitions = extract_definitions(g, CURIE_PREFIX_MAP)\n        hierarchy\
        \ = extract_hierarchy(g, CURIE_PREFIX_MAP)\n        relations = extract_relations(g,\
        \ relation_properties_for_extraction, CURIE_PREFIX_MAP)\n\n        logger.info(\"\
        \\nMerging extracted data...\")\n        merged_data = {}\n        all_curies\
        \ = set(labels_synonyms.keys()) | \\\n                     set(definitions.keys())\
        \ | \\\n                     set(hierarchy.keys()) | \\\n                \
        \     set(relations.keys())\n\n        for curie_key in all_curies:\n    \
        \        # No need to filter again here if extraction functions already did\
        \ or if we want all extracted CURIEs\n            merged_data[curie_key] =\
        \ {\n                \"label\": labels_synonyms.get(curie_key, {}).get(\"\
        label\"),\n                \"synonyms\": labels_synonyms.get(curie_key, {}).get(\"\
        synonyms\", []),\n                \"definition\": definitions.get(curie_key),\n\
        \                \"parents\": hierarchy.get(curie_key, {}).get(\"parents\"\
        , []),\n                \"ancestors\": hierarchy.get(curie_key, {}).get(\"\
        ancestors\", []),\n                \"relations\": relations.get(curie_key,\
        \ {})\n            }\n        \n        final_merged_data = {}\n        for\
        \ curie_key, data_dict in merged_data.items():\n            if any(data_dict.values()):\
        \ # Check if any value in the dict is non-empty/non-None\n               \
        \ final_merged_data[curie_key] = data_dict\n\n        logger.info(f\"\\nTotal\
        \ merged entities with some data: {len(final_merged_data)}\")\n\n        #\
        \ Adjusted: Use ONTOLOGY_DUMP_JSON for output\n        logger.info(f\"Writing\
        \ merged data to {ONTOLOGY_DUMP_JSON}\")\n        with open(ONTOLOGY_DUMP_JSON,\
        \ 'w', encoding='utf-8') as f:\n            json.dump(final_merged_data, f,\
        \ indent=4, ensure_ascii=False)\n\n        logger.info(\"Ontology parsing\
        \ and data dump complete.\")\n\n    except FileNotFoundError:\n        # Adjusted:\
        \ Use FOODON_PATH in error message\n        logger.error(f\"Parsing aborted:\
        \ Ontology file not found at {FOODON_PATH}\")\n        # traceback.print_exc()\
        \ # Already handled by load_ontology\n    except Exception as e:\n       \
        \ logger.error(f\"An error occurred during parsing: {e}\")\n        traceback.print_exc()\n\
        \nif __name__ == \"__main__\":\n    main()"
    "pipeline":
      "gemini_pipeline.py": "# src/pipeline/pipeline.py\nimport os\nimport sys\nimport\
        \ logging\nfrom typing import List, Dict, Any, Optional\n\n# --- Add project\
        \ root to sys.path ---\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..'))\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n\
        \nfrom src.retriever.hybrid_retriever import HybridRetriever\nfrom src.reranker.llm_reranker\
        \ import LLMReranker\nfrom src.rag_selectors.gemini_selector import GeminiSelector\n\
        from src import config\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s\
        \ - %(levelname)s - %(name)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\
        \nclass RAGPipeline:\n    def __init__(self):\n        logger.info(\"Initializing\
        \ RAG Pipeline...\")\n        try:\n            self.retriever = HybridRetriever()\n\
        \            self.reranker = LLMReranker(\n                model_name=config.RERANKER_MODEL_NAME,\n\
        \                enriched_docs_path=config.ENRICHED_DOCUMENTS_FILE,\n    \
        \            device=config.EMBEDDING_DEVICE\n            )\n            self.selector\
        \ = GeminiSelector(retriever=self.retriever) # <--- UPDATED INSTANTIATION\n\
        \            logger.info(\"RAG Pipeline initialized successfully.\")\n   \
        \     except (FileNotFoundError, ValueError) as e:\n            logger.error(f\"\
        Failed to initialize pipeline: {e}\")\n            logger.error(\"Please run\
        \ 'scripts/rebuild_base.bash' and ensure GEMINI_API_KEY is set in your .env\
        \ file.\")\n            raise\n        except Exception as e:\n          \
        \  logger.error(f\"An unexpected error occurred during pipeline initialization:\
        \ {e}\", exc_info=True)\n            raise\n\n    def run(self, \n       \
        \     query: str, \n            lexical_k: int = config.DEFAULT_K_LEXICAL,\
        \ \n            vector_k: int = config.DEFAULT_K_VECTOR, \n            rerank_top_n:\
        \ int = 10\n            ) -> Optional[Dict[str, Any]]:\n        \"\"\"\n \
        \       Executes the full pipeline for a given query.\n\n        Returns:\n\
        \            A dictionary of the selected term with its details and the LLM's\
        \ explanation, or None.\n        \"\"\"\n        logger.info(f\"Running pipeline\
        \ for query: '{query}'\")\n\n        # 1. Retrieve\n        retriever_output\
        \ = self.retriever.search(query, lexical_limit=lexical_k, vector_k=vector_k)\n\
        \        lexical_results = retriever_output.get(\"lexical_results\", [])\n\
        \        vector_results = retriever_output.get(\"vector_results\", [])\n \
        \       \n        # 2. Merge\n        combined_candidates = []\n        seen_ids\
        \ = set()\n        for doc in lexical_results + vector_results:\n        \
        \    doc_id = doc.get('id')\n            if doc_id and doc_id not in seen_ids:\n\
        \                combined_candidates.append(doc)\n                seen_ids.add(doc_id)\n\
        \        \n        if not combined_candidates:\n            logger.warning(\"\
        No candidates found.\")\n            return None\n\n        # 3. Rerank\n\
        \        reranked_candidates = self.reranker.rerank(query, combined_candidates,\
        \ top_n=rerank_top_n)\n\n        if not reranked_candidates:\n           \
        \ logger.warning(\"No candidates left after reranking.\")\n            return\
        \ None\n        \n        logger.info(f\"Top {len(reranked_candidates)} candidates\
        \ after reranking passed to LLM selector.\")\n\n        # 4. Select with LLM\n\
        \        selection = self.selector.select_best_term(query, reranked_candidates)\n\
        \n        if not selection:\n            logger.error(\"LLM selection failed.\
        \ Returning the top reranked result as a fallback.\")\n            top_fallback\
        \ = reranked_candidates[0]\n            chosen_term_details = self.retriever.get_term_details(top_fallback['id'])\n\
        \            chosen_term_details['explanation'] = \"FALLBACK: LLM selection\
        \ failed. This is the top result from the reranker.\"\n            return\
        \ chosen_term_details\n\n        # 5. Get final details and return\n     \
        \   chosen_id = selection['chosen_id']\n        chosen_term_details = self.retriever.get_term_details(chosen_id)\n\
        \        logger.debug(f\"Chosen term details: {chosen_term_details}\")\n \
        \       if not chosen_term_details:\n            logger.error(f\"LLM chose\
        \ ID '{chosen_id}', but its details could not be retrieved.\")\n         \
        \   return None\n        \n        chosen_term_details['explanation'] = selection['explanation']\n\
        \        return chosen_term_details\n\n    def close(self):\n        if hasattr(self.retriever,\
        \ 'close'):\n            self.retriever.close()\n        logger.info(\"Pipeline\
        \ resources closed.\")"
      "ollama_pipeline.py": "# src/pipeline/pipeline.py\nimport os\nimport sys\nimport\
        \ logging\nfrom typing import List, Dict, Any, Optional\n\n# --- Add project\
        \ root to sys.path ---\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..'))\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n\
        \nfrom src.retriever.hybrid_retriever import HybridRetriever\nfrom src.reranker.llm_reranker\
        \ import LLMReranker\nfrom src.rag_selectors.ollama_selector import OllamaSelector\n\
        from src import config\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s\
        \ - %(levelname)s - %(name)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\
        \nclass RAGPipeline:\n    def __init__(self):\n        logger.info(\"Initializing\
        \ RAG Pipeline...\")\n        try:\n            self.retriever = HybridRetriever()\n\
        \            self.reranker = LLMReranker(\n                model_name=config.RERANKER_MODEL_NAME,\n\
        \                enriched_docs_path=config.ENRICHED_DOCUMENTS_FILE,\n    \
        \            device=config.EMBEDDING_DEVICE\n            )\n            self.selector\
        \ = OllamaSelector(retriever=self.retriever) # <--- UPDATED INSTANTIATION\n\
        \            logger.info(\"RAG Pipeline initialized successfully.\")\n   \
        \     except (FileNotFoundError, ValueError) as e:\n            logger.error(f\"\
        Failed to initialize pipeline: {e}\")\n            logger.error(\"Please run\
        \ 'scripts/rebuild_base.bash' and ensure GEMINI_API_KEY is set in your .env\
        \ file.\")\n            raise\n        except Exception as e:\n          \
        \  logger.error(f\"An unexpected error occurred during pipeline initialization:\
        \ {e}\", exc_info=True)\n            raise\n\n    def run(self, \n       \
        \     query: str, \n            lexical_k: int = config.DEFAULT_K_LEXICAL,\
        \ \n            vector_k: int = config.DEFAULT_K_VECTOR, \n            rerank_top_n:\
        \ int = 10\n            ) -> Optional[Dict[str, Any]]:\n        \"\"\"\n \
        \       Executes the full pipeline for a given query.\n\n        Returns:\n\
        \            A dictionary of the selected term with its details and the LLM's\
        \ explanation, or None.\n        \"\"\"\n        logger.info(f\"Running pipeline\
        \ for query: '{query}'\")\n\n        # 1. Retrieve\n        retriever_output\
        \ = self.retriever.search(query, lexical_limit=lexical_k, vector_k=vector_k)\n\
        \        lexical_results = retriever_output.get(\"lexical_results\", [])\n\
        \        vector_results = retriever_output.get(\"vector_results\", [])\n \
        \       \n        # 2. Merge\n        combined_candidates = []\n        seen_ids\
        \ = set()\n        for doc in lexical_results + vector_results:\n        \
        \    doc_id = doc.get('id')\n            if doc_id and doc_id not in seen_ids:\n\
        \                combined_candidates.append(doc)\n                seen_ids.add(doc_id)\n\
        \        \n        if not combined_candidates:\n            logger.warning(\"\
        No candidates found.\")\n            return None\n\n        # 3. Rerank\n\
        \        reranked_candidates = self.reranker.rerank(query, combined_candidates,\
        \ top_n=rerank_top_n)\n\n        if not reranked_candidates:\n           \
        \ logger.warning(\"No candidates left after reranking.\")\n            return\
        \ None\n        \n        logger.info(f\"Top {len(reranked_candidates)} candidates\
        \ after reranking passed to LLM selector.\")\n\n        # 4. Select with LLM\n\
        \        selection = self.selector.select_best_term(query, reranked_candidates)\n\
        \n        if not selection:\n            logger.error(\"LLM selection failed.\
        \ Returning the top reranked result as a fallback.\")\n            top_fallback\
        \ = reranked_candidates[0]\n            chosen_term_details = self.retriever.get_term_details(top_fallback['id'])\n\
        \            chosen_term_details['explanation'] = \"FALLBACK: LLM selection\
        \ failed. This is the top result from the reranker.\"\n            return\
        \ chosen_term_details\n\n        # 5. Get final details and return\n     \
        \   chosen_id = selection['chosen_id']\n        chosen_term_details = self.retriever.get_term_details(chosen_id)\n\
        \        if not chosen_term_details:\n            logger.error(f\"LLM chose\
        \ ID '{chosen_id}', but its details could not be retrieved.\")\n         \
        \   return None\n        \n        chosen_term_details['explanation'] = selection['explanation']\n\
        \        return chosen_term_details\n\n    def close(self):\n        if hasattr(self.retriever,\
        \ 'close'):\n            self.retriever.close()\n        logger.info(\"Pipeline\
        \ resources closed.\")"
    "rag_selectors":
      "__init__.py": ""
      "gemini_selector.py": "# src/selectors/gemini_selector.py\nimport os\nimport\
        \ logging\nimport json\nfrom typing import List, Dict, Any, Optional\n\nfrom\
        \ google import genai\nfrom google.api_core import exceptions  # <-- Import\
        \ for better error handling\n\nfrom src.retriever.hybrid_retriever import\
        \ HybridRetriever\nfrom src import config\n\nlogger = logging.getLogger(__name__)\n\
        \nclass GeminiSelector:\n    \"\"\"\n    Uses the Google Gemini model to select\
        \ the best ontology term\n    from a list of candidates.\n    \"\"\"\n   \
        \ def __init__(self, retriever: HybridRetriever):\n        \"\"\"\n      \
        \  Initializes the GeminiSelector.\n\n        Args:\n            retriever\
        \ (HybridRetriever): An initialized retriever instance,\n                \
        \                         used to fetch full details of candidate terms.\n\
        \        \"\"\"\n        self.retriever = retriever\n        self.prompt_template\
        \ = self._load_prompt_template()\n\n        if not config.GEMINI_API_KEY:\n\
        \            raise ValueError(\"GEMINI_API_KEY not found in environment variables.\"\
        )\n        \n        # --- CORRECTED PART 1: Client Instantiation ---\n  \
        \      # Instead of genai.configure(), we instantiate a client.\n        self.client\
        \ = genai.Client(api_key=config.GEMINI_API_KEY)\n        self.model_name =\
        \ config.LLM_SELECTOR_MODEL_NAME\n        \n        logger.info(f\"GeminiSelector\
        \ initialized for model: {self.model_name}\")\n\n    def _load_prompt_template(self)\
        \ -> str:\n        \"\"\"Loads the prompt template from the file.\"\"\"\n\
        \        template_path = os.path.join(config.PROJECT_ROOT, \"prompts\", \"\
        final_selection.tpl\")\n        try:\n            with open(template_path,\
        \ 'r', encoding='utf-8') as f:\n                return f.read()\n        except\
        \ FileNotFoundError:\n            logger.error(f\"Prompt template not found\
        \ at {template_path}\")\n            raise\n\n    def _format_candidates_for_prompt(self,\
        \ candidates: List[Dict[str, Any]]) -> str:\n        \"\"\"Formats the list\
        \ of candidate documents into a string for the prompt.\"\"\"\n        formatted_list\
        \ = []\n        for i, candidate in enumerate(candidates):\n            term_id\
        \ = candidate.get(\"id\")\n            if not term_id:\n                continue\n\
        \            \n            # Fetch full details using the retriever\n    \
        \        details = self.retriever.get_term_details(term_id)\n            if\
        \ not details:\n                continue\n\n            # Format the details\
        \ for display\n            label = details.get(\"label\", \"N/A\")\n     \
        \       definition = details.get(\"definition\", \"No definition available.\"\
        )\n            synonyms = \"; \".join(details.get(\"synonyms\", [])) or \"\
        None\"\n\n            formatted_list.append(\n                f\"{i+1}. ID:\
        \ {term_id}\\n\"\n                f\"   Label: {label}\\n\"\n            \
        \    f\"   Definition: {definition}\\n\"\n                f\"   Synonyms:\
        \ {synonyms}\"\n            )\n        return \"\\n\\n\".join(formatted_list)\n\
        \n    def select_best_term(self, query: str, candidates: List[Dict[str, Any]])\
        \ -> Optional[Dict[str, str]]:\n        \"\"\"\n        Prompts Gemini to\
        \ select the best term and parses the JSON response.\n\n        Args:\n  \
        \          query (str): The original user query.\n            candidates (List[Dict[str,\
        \ Any]]): The list of candidate documents.\n\n        Returns:\n         \
        \   A dictionary with {\"chosen_id\": str, \"explanation\": str}, or None\
        \ on failure.\n        \"\"\"\n        if not candidates:\n            return\
        \ None\n\n        candidate_str = self._format_candidates_for_prompt(candidates)\n\
        \        prompt = self.prompt_template.replace(\"[USER_ENTITY]\", query).replace(\"\
        [CANDIDATE_LIST]\", candidate_str)\n        \n        logger.info(f\"Sending\
        \ request to Gemini for query: '{query}'\")\n        try:\n            # ---\
        \ CORRECTED PART 2: The API Call ---\n            # Call generate_content\
        \ on the client.models service.\n            # Pass the prompt string to the\
        \ 'contents' parameter.\n            generation_config = {'temperature': 0}\n\
        \            response = self.client.models.generate_content(\n           \
        \     model=self.model_name,\n                contents=prompt,\n         \
        \       config=generation_config\n            )\n\n            # The rest\
        \ of your logic is good.\n            # Safety feedback check from your example\
        \ code is a good practice to add here if needed.\n            feedback = getattr(response,\
        \ 'prompt_feedback', None)\n            if feedback and any(r.blocked for\
        \ r in feedback.safety_ratings or []):\n                logger.warning(f\"\
        Request for query '{query}' was blocked by safety filters.\")\n          \
        \      return None\n\n            # Clean up the response text to extract\
        \ the JSON part\n            cleaned_response = response.text.strip().lstrip(\"\
        ```json\").rstrip(\"```\").strip()\n            \n            # Parse the\
        \ JSON response\n            result = json.loads(cleaned_response)\n     \
        \       if \"chosen_id\" in result and \"explanation\" in result:\n      \
        \          return result\n            else:\n                logger.error(f\"\
        LLM response is valid JSON but missing required keys: {result}\")\n      \
        \          return None\n                \n        except json.JSONDecodeError:\n\
        \            logger.error(f\"Failed to decode JSON from LLM response: {response.text}\"\
        )\n            return None\n        except exceptions.GoogleAPIError as e:\
        \  # <-- Specific API error handling\n            logger.error(f\"A Google\
        \ API error occurred with the Gemini call: {e}\", exc_info=True)\n       \
        \     return None\n        except Exception as e:\n            logger.error(f\"\
        An unexpected error occurred with the Gemini API call: {e}\", exc_info=True)\n\
        \            return None"
      "ollama_selector.py": "# src/selectors/ollama_selector.py\nimport os\nimport\
        \ logging\nimport json\nfrom typing import List, Dict, Any, Optional\n\nimport\
        \ ollama # <-- New import\n\nfrom src.retriever.hybrid_retriever import HybridRetriever\n\
        from src import config\n\nlogger = logging.getLogger(__name__)\n\nclass OllamaSelector:\n\
        \    \"\"\"\n    Uses a local LLM via Ollama to select the best ontology term\n\
        \    from a list of candidates.\n    \"\"\"\n    def __init__(self, retriever:\
        \ HybridRetriever):\n        \"\"\"\n        Initializes the OllamaSelector.\n\
        \n        Args:\n            retriever (HybridRetriever): An initialized retriever\
        \ instance,\n                                         used to fetch full details\
        \ of candidate terms.\n        \"\"\"\n        self.retriever = retriever\n\
        \        self.prompt_template = self._load_prompt_template()\n        \n \
        \       # We get the model name from config, but no API key or client is needed\
        \ for Ollama.\n        self.model_name = config.OLLAMA_SELECTOR_MODEL_NAME\
        \ # Assumes you have this in your config\n        \n        # You might want\
        \ to add a check here to ensure the Ollama service is running.\n        try:\n\
        \            ollama.ps()\n            logger.info(f\"Ollama service is running.\
        \ Selector initialized for model: {self.model_name}\")\n        except Exception:\n\
        \            logger.error(\"Ollama service not detected. Please ensure Ollama\
        \ is running.\")\n            raise ConnectionError(\"Ollama service not available.\"\
        )\n\n\n    def _load_prompt_template(self) -> str:\n        \"\"\"Loads the\
        \ prompt template from the file.\"\"\"\n        # This method is unchanged\n\
        \        template_path = os.path.join(config.PROJECT_ROOT, \"prompts\", \"\
        final_selection.tpl\")\n        try:\n            with open(template_path,\
        \ 'r', encoding='utf-8') as f:\n                return f.read()\n        except\
        \ FileNotFoundError:\n            logger.error(f\"Prompt template not found\
        \ at {template_path}\")\n            raise\n\n    def _format_candidates_for_prompt(self,\
        \ candidates: List[Dict[str, Any]]) -> str:\n        \"\"\"Formats the list\
        \ of candidate documents into a string for the prompt.\"\"\"\n        # This\
        \ method is unchanged\n        formatted_list = []\n        for i, candidate\
        \ in enumerate(candidates):\n            term_id = candidate.get(\"id\")\n\
        \            if not term_id:\n                continue\n            \n   \
        \         details = self.retriever.get_term_details(term_id)\n           \
        \ if not details:\n                continue\n\n            label = details.get(\"\
        label\", \"N/A\")\n            definition = details.get(\"definition\", \"\
        No definition available.\")\n            synonyms = \"; \".join(details.get(\"\
        synonyms\", [])) or \"None\"\n\n            formatted_list.append(\n     \
        \           f\"{i+1}. ID: {term_id}\\n\"\n                f\"   Label: {label}\\\
        n\"\n                f\"   Definition: {definition}\\n\"\n               \
        \ f\"   Synonyms: {synonyms}\"\n            )\n        return \"\\n\\n\".join(formatted_list)\n\
        \n    def select_best_term(self, query: str, candidates: List[Dict[str, Any]])\
        \ -> Optional[Dict[str, str]]:\n        \"\"\"\n        Prompts Ollama to\
        \ select the best term and parses the JSON response.\n\n        Args:\n  \
        \          query (str): The original user query.\n            candidates (List[Dict[str,\
        \ Any]]): The list of candidate documents.\n\n        Returns:\n         \
        \   A dictionary with {\"chosen_id\": str, \"explanation\": str}, or None\
        \ on failure.\n        \"\"\"\n        if not candidates:\n            return\
        \ None\n\n        candidate_str = self._format_candidates_for_prompt(candidates)\n\
        \        # The full prompt is created just like before.\n        full_prompt\
        \ = self.prompt_template.replace(\"[USER_ENTITY]\", query).replace(\"[CANDIDATE_LIST]\"\
        , candidate_str)\n        \n        logger.info(f\"Sending request to Ollama\
        \ for query: '{query}' with model '{self.model_name}'\")\n        try:\n \
        \           # --- CORRECTED OLLAMA CALL ---\n            # The entire prompt\
        \ goes into a single 'user' message.\n            # We use format='json' to\
        \ ensure the output is valid JSON.\n            response = ollama.chat(\n\
        \                model=self.model_name,\n                messages=[\n    \
        \                {\n                        'role': 'user',\n            \
        \            'content': full_prompt,\n                    },\n           \
        \     ],\n                format='json' # This is a key feature to get structured\
        \ output!\n            )\n\n            # The response content should be a\
        \ JSON string.\n            response_content = response['message']['content']\n\
        \            \n            # Parse the JSON response\n            result =\
        \ json.loads(response_content)\n            \n            if \"chosen_id\"\
        \ in result and \"explanation\" in result:\n                return result\n\
        \            else:\n                logger.error(f\"LLM response is valid\
        \ JSON but missing required keys: {result}\")\n                return None\n\
        \                \n        except json.JSONDecodeError:\n            # This\
        \ is less likely with format='json' but is good practice to keep.\n      \
        \      logger.error(f\"Failed to decode JSON from Ollama response: {response_content}\"\
        )\n            return None\n        except ollama.ResponseError as e:\n  \
        \          logger.error(f\"An error occurred with the Ollama API call: {e.status_code}\
        \ - {e.error}\")\n            return None\n        except Exception as e:\n\
        \            logger.error(f\"An unexpected error occurred during the Ollama\
        \ call: {e}\", exc_info=True)\n            return None"
    "reranker":
      "__init__.py": |-
        # src/reranker/__init__.py
        # This file makes Python treat the directory 'reranker' as a package.

        from .llm_reranker import LLMReranker

        __all__ = ["LLMReranker"]
      "llm_reranker.py": "# src/reranker/llm_reranker.py\n\nimport json\nimport logging\n\
        from typing import List, Dict, Optional, Tuple, Any\n\nfrom sentence_transformers\
        \ import CrossEncoder\nimport torch\n\n# Configure logging for the module\n\
        logger = logging.getLogger(__name__)\n# Basic configuration for the logger\
        \ if no handlers are configured by the calling application\nif not logger.hasHandlers():\n\
        \    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s\
        \ - %(levelname)s - %(message)s')\n\n\nclass LLMReranker:\n    \"\"\"\n  \
        \  Reranks documents using a CrossEncoder model.\n    It takes a query and\
        \ a list of candidate documents, computes a relevance score\n    for each\
        \ query-document pair, and returns the documents sorted by these scores.\n\
        \    \"\"\"\n\n    def __init__(self,\n                 model_name: str,\n\
        \                 enriched_docs_path: str,\n                 device: Optional[str]\
        \ = None):\n        \"\"\"\n        Initializes the LLMReranker.\n\n     \
        \   Args:\n            model_name (str): The name of the CrossEncoder model\
        \ to use\n                              (e.g., \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\
        ).\n            enriched_docs_path (str): Path to the JSON file containing\
        \ enriched documents,\n                                      expected to be\
        \ a list of dicts with \"id\" and \"text\" keys.\n            device (Optional[str]):\
        \ The device to run the model on (\"cuda\", \"cpu\").\n                  \
        \                  If None, tries to use CUDA, otherwise CPU.\n        \"\"\
        \"\n        self.model_name = model_name\n        self.enriched_docs_path\
        \ = enriched_docs_path\n\n        if device:\n            self.device = device\n\
        \        elif torch.cuda.is_available():\n            self.device = \"cuda\"\
        \n            logger.info(\"CUDA available. Using CUDA for reranker.\")\n\
        \        else:\n            self.device = \"cpu\"\n            logger.info(\"\
        CUDA not available. Using CPU for reranker.\")\n        \n        try:\n \
        \           self.model = CrossEncoder(self.model_name, device=self.device,\
        \ trust_remote_code=True) # Added trust_remote_code for models like e5\n \
        \           logger.info(f\"Successfully loaded CrossEncoder model: {self.model_name}\
        \ on {self.device}\")\n            model_max_length = self.model.tokenizer.model_max_length\n\
        \            if model_max_length > 10000: # often indicates not properly set,\
        \ e.g. 1e30\n                model_max_length = 512 # Fallback to a common\
        \ default\n                logger.warning(f\"Model tokenizer returned a very\
        \ large max_length. Using fallback: {model_max_length}\")\n            self.model.max_length\
        \ = model_max_length\n            logger.info(f\"Set CrossEncoder max_length\
        \ to: {self.model.max_length}\")\n\n        except Exception as e:\n     \
        \       logger.error(f\"Failed to load CrossEncoder model {self.model_name}:\
        \ {e}\")\n            if \"intfloat/e5-mistral-7b-instruct\" in self.model_name:\n\
        \                 logger.warning(\n                    f\"Note: '{self.model_name}'\
        \ is primarily an encoder model. \"\n                    \"Using it directly\
        \ with `CrossEncoder` might not yield optimal \"\n                    \"results\
        \ unless it's a variant fine-tuned for sequence-pair classification \"\n \
        \                   \"or requires `trust_remote_code=True`. \"\n         \
        \           \"Consider a bi-encoder approach (separate embeddings + cosine\
        \ similarity) \"\n                    \"or a prompt-based LLM reranking for\
        \ this model type if standard CrossEncoder fails.\"\n                )\n \
        \           raise\n\n        self.doc_texts = self._load_enriched_documents()\n\
        \        if not self.doc_texts:\n            logger.error(f\"Enriched documents\
        \ could not be loaded from {self.enriched_docs_path}. Reranker might not function\
        \ correctly.\")\n\n\n    def _load_enriched_documents(self) -> Dict[str, str]:\n\
        \        \"\"\"\n        Loads enriched documents and creates a mapping from\
        \ document ID to text.\n\n        Returns:\n            Dict[str, str]: A\
        \ dictionary mapping document CURIEs to their enriched text.\n        \"\"\
        \"\n        try:\n            with open(self.enriched_docs_path, 'r', encoding='utf-8')\
        \ as f:\n                enriched_data = json.load(f)\n            \n    \
        \        doc_map = {}\n            for item in enriched_data:\n          \
        \      if \"id\" in item and \"text\" in item:\n                    doc_map[item[\"\
        id\"]] = item[\"text\"]\n                else:\n                    logger.warning(f\"\
        Skipping item due to missing 'id' or 'text' in {self.enriched_docs_path}:\
        \ {item}\")\n            logger.info(f\"Loaded {len(doc_map)} enriched documents\
        \ for reranking from {self.enriched_docs_path}.\")\n            return doc_map\n\
        \        except FileNotFoundError:\n            logger.error(f\"Enriched documents\
        \ file not found: {self.enriched_docs_path}\")\n            return {}\n  \
        \      except json.JSONDecodeError:\n            logger.error(f\"Error decoding\
        \ JSON from {self.enriched_docs_path}\")\n            return {}\n        except\
        \ Exception as e:\n            logger.error(f\"An unexpected error occurred\
        \ while loading enriched documents: {e}\")\n            return {}\n\n    def\
        \ _get_document_text_for_reranking(self, doc_id: str) -> Optional[str]:\n\
        \        \"\"\"\n        Retrieves the pre-loaded enriched text for a given\
        \ document ID.\n\n        Args:\n            doc_id (str): The CURIE of the\
        \ document.\n\n        Returns:\n            Optional[str]: The enriched text\
        \ of the document, or None if not found.\n        \"\"\"\n        return self.doc_texts.get(doc_id)\n\
        \n    def rerank(self, query: str, documents: List[Dict[str, Any]], top_n:\
        \ Optional[int] = None) -> List[Dict[str, Any]]:\n        \"\"\"\n       \
        \ Reranks a list of documents based on their relevance to a query.\n\n   \
        \     Args:\n            query (str): The search query.\n            documents\
        \ (List[Dict[str, Any]]): A list of document dictionaries retrieved\n    \
        \                                          from a previous stage (e.g., HybridRetriever).\n\
        \                                              Each dict must contain an 'id'\
        \ key.\n            top_n (Optional[int]): The maximum number of documents\
        \ to return after reranking.\n                                   If None,\
        \ all reranked documents are returned.\n\n        Returns:\n            List[Dict[str,\
        \ Any]]: The list of documents, sorted by reranked scores,\n             \
        \                     with a new 'rerank_score' key added to each document.\n\
        \                                  Documents for which text could not be found\
        \ are excluded.\n        \"\"\"\n        if not query:\n            logger.warning(\"\
        Rerank called with an empty query. Returning original documents.\")\n    \
        \        return documents[:top_n] if top_n is not None else documents\n  \
        \      \n        if not documents:\n            logger.info(\"Rerank called\
        \ with no documents. Returning empty list.\")\n            return []\n\n \
        \       if not self.doc_texts:\n            logger.error(\"No enriched document\
        \ texts loaded. Cannot perform reranking. Returning original documents.\"\
        )\n            return documents[:top_n] if top_n is not None else documents\n\
        \n        sentence_pairs: List[Tuple[str, str]] = []\n        valid_documents_for_reranking:\
        \ List[Dict[str, Any]] = []\n\n        for doc in documents:\n           \
        \ doc_id = doc.get(\"id\")\n            if not doc_id:\n                logger.warning(f\"\
        Document missing 'id' field, cannot rerank: {doc}\")\n                continue\n\
        \n            doc_text = self._get_document_text_for_reranking(doc_id)\n \
        \           if doc_text:\n                sentence_pairs.append((query, doc_text))\n\
        \                valid_documents_for_reranking.append(doc)\n            else:\n\
        \                logger.warning(f\"Could not find enriched text for document\
        \ ID '{doc_id}'. Skipping for reranking.\")\n        \n        if not sentence_pairs:\n\
        \            logger.warning(\"No valid documents found to create sentence\
        \ pairs for reranking. Returning original documents that were passed in.\"\
        )\n            return documents[:top_n] if top_n is not None else documents\n\
        \n        logger.info(f\"Reranking {len(sentence_pairs)} document(s) for query:\
        \ '{query}'\")\n        \n        try:\n            scores = self.model.predict(sentence_pairs,\
        \ \n                                        show_progress_bar=False, # Set\
        \ to True for verbose progress\n                                        batch_size=32)\
        \ # Adjust batch_size based on VRAM\n        except Exception as e:\n    \
        \        logger.error(f\"Error during CrossEncoder prediction: {e}\")\n  \
        \          # Fallback: return original documents without reranking scores\n\
        \            return documents[:top_n] if top_n is not None else documents\n\
        \n        # Add scores to documents and sort\n        for i, doc in enumerate(valid_documents_for_reranking):\n\
        \            doc[\"rerank_score\"] = float(scores[i]) # Ensure score is float\n\
        \n        # Sort documents by rerank_score in descending order\n        reranked_documents\
        \ = sorted(valid_documents_for_reranking, key=lambda x: x.get(\"rerank_score\"\
        , -float('inf')), reverse=True)\n\n        if top_n is not None:\n       \
        \     reranked_documents = reranked_documents[:top_n]\n            logger.info(f\"\
        Returning top {len(reranked_documents)} reranked documents.\")\n        else:\n\
        \            logger.info(f\"Returning all {len(reranked_documents)} reranked\
        \ documents.\")\n            \n        return reranked_documents\n\n\nif __name__\
        \ == '__main__':\n    import sys\n    from pathlib import Path\n    import\
        \ random # For dummy retriever scores\n\n    # --- Path Setup & Config Import\
        \ ---\n    # Add project root to sys.path to allow imports like src.config\n\
        \    # Assumes this script is in onto_rag/src/reranker/\n    PROJECT_ROOT_FOR_MAIN\
        \ = Path(__file__).resolve().parent.parent.parent\n    if str(PROJECT_ROOT_FOR_MAIN)\
        \ not in sys.path:\n        sys.path.append(str(PROJECT_ROOT_FOR_MAIN))\n\n\
        \    try:\n        from src.config import (\n            ENRICHED_DOCUMENTS_FILE,\n\
        \            RERANKER_MODEL_NAME,\n            EMBEDDING_DEVICE,\n       \
        \     DEFAULT_RERANK_K,\n            LOG_LEVEL, # Optional: if you want to\
        \ use config's log level\n            LOG_FILE   # Optional: if you want to\
        \ use config's log file\n        )\n    except ImportError as e:\n       \
        \ print(f\"Error importing from src.config: {e}\")\n        print(\"Please\
        \ ensure that src/config.py exists and the script is run from the project\
        \ root,\")\n        print(\"or that the PROJECT_ROOT_FOR_MAIN path is correctly\
        \ set for your structure.\")\n        sys.exit(1)\n\n    # --- Basic Logging\
        \ Setup for the Example ---\n    # Uses the main logger configured at the\
        \ top of the file.\n    # You can customize this further if needed, e.g.,\
        \ by setting level from config.LOG_LEVEL\n    logger.setLevel(LOG_LEVEL if\
        \ 'LOG_LEVEL' in locals() else \"INFO\")\n    # Example of adding a file handler\
        \ if LOG_FILE is configured:\n    # if 'LOG_FILE' in locals() and LOG_FILE:\n\
        \    #     fh = logging.FileHandler(LOG_FILE)\n    #     fh.setFormatter(logging.Formatter('%(asctime)s\
        \ - %(name)s - %(levelname)s - %(message)s'))\n    #     logger.addHandler(fh)\n\
        \    #     logging.getLogger().addHandler(fh) # also add to root logger if\
        \ basicConfig was called\n\n    logger.info(\"--- Running LLMReranker Example\
        \ with Real Data ---\")\n    logger.info(f\"Using ENRICHED_DOCUMENTS_FILE:\
        \ {ENRICHED_DOCUMENTS_FILE}\")\n    logger.info(f\"Using RERANKER_MODEL_NAME:\
        \ {RERANKER_MODEL_NAME}\")\n    logger.info(f\"Using EMBEDDING_DEVICE: {EMBEDDING_DEVICE}\"\
        )\n    logger.info(f\"Using DEFAULT_RERANK_K: {DEFAULT_RERANK_K}\")\n\n\n\
        \    # --- Load Sample Documents from Real Enriched Data ---\n    sample_documents_from_retriever:\
        \ List[Dict[str, Any]] = []\n    try:\n        with open(ENRICHED_DOCUMENTS_FILE,\
        \ 'r', encoding='utf-8') as f:\n            all_enriched_docs = json.load(f)\n\
        \        \n        if not all_enriched_docs:\n            logger.error(f\"\
        No documents found in {ENRICHED_DOCUMENTS_FILE}. Cannot proceed with the example.\"\
        )\n            sys.exit(1)\n\n        # Select a few documents to simulate\
        \ retriever output\n        # Take up to 5, or fewer if the file has less\n\
        \        num_docs_to_sample = min(len(all_enriched_docs), 5)\n        if num_docs_to_sample\
        \ == 0 :\n             logger.error(f\"The file {ENRICHED_DOCUMENTS_FILE}\
        \ is empty. Cannot create sample documents.\")\n             sys.exit(1)\n\
        \             \n        for i in range(num_docs_to_sample):\n            doc\
        \ = all_enriched_docs[i]\n            sample_documents_from_retriever.append({\n\
        \                \"id\": doc.get(\"id\"),\n                \"label\": doc.get(\"\
        label\", \"N/A\"), # Get label if available\n                \"retriever_score\"\
        : random.uniform(0.5, 1.0) # Dummy score\n            })\n        logger.info(f\"\
        Loaded {len(sample_documents_from_retriever)} sample documents for reranking.\"\
        )\n\n    except FileNotFoundError:\n        logger.error(f\"ERROR: The enriched\
        \ documents file was not found: {ENRICHED_DOCUMENTS_FILE}\")\n        logger.error(\"\
        Please ensure you have run the data ingestion and enrichment scripts first\
        \ (e.g., `src.ingestion.enrich_documents.py`).\")\n        sys.exit(1)\n \
        \   except json.JSONDecodeError:\n        logger.error(f\"ERROR: Could not\
        \ decode JSON from {ENRICHED_DOCUMENTS_FILE}. The file might be corrupted.\"\
        )\n        sys.exit(1)\n    except Exception as e:\n        logger.error(f\"\
        An unexpected error occurred while loading sample documents: {e}\", exc_info=True)\n\
        \        sys.exit(1)\n\n    if not sample_documents_from_retriever:\n    \
        \    logger.error(\"No sample documents could be prepared. Exiting example.\"\
        )\n        sys.exit(1)\n        \n    # --- Reranker Initialization and Usage\
        \ ---\n    try:\n        reranker = LLMReranker(\n            model_name=RERANKER_MODEL_NAME,\n\
        \            enriched_docs_path=str(ENRICHED_DOCUMENTS_FILE), # Ensure path\
        \ is a string\n            device=EMBEDDING_DEVICE\n        )\n\n        query\
        \ = \"Garlic\" # Generic example, adjust to your data\n\n        logger.info(f\"\
        \\nOriginal sample documents (simulated retriever output) for query '{query}':\"\
        )\n        for doc in sample_documents_from_retriever:\n            logger.info(f\"\
        \  ID: {doc.get('id')}, Label: {doc.get('label')}, Retriever Score: {doc.get('retriever_score',\
        \ 0.0):.4f}\")\n\n        reranked_results = reranker.rerank(query, sample_documents_from_retriever,\
        \ top_n=DEFAULT_RERANK_K)\n\n        logger.info(f\"\\nReranked documents\
        \ (top {DEFAULT_RERANK_K} for query '{query}'):\")\n        if reranked_results:\n\
        \            for doc in reranked_results:\n                logger.info(f\"\
        \  ID: {doc.get('id')}, Label: {doc.get('label')}, Rerank Score: {doc.get('rerank_score',\
        \ 'N/A'):.4f}, Original Retriever Score: {doc.get('retriever_score', 'N/A')}\"\
        )\n        else:\n            logger.info(\"  No results after reranking.\"\
        )\n        \n        # Example of how you might switch to \"intfloat/e5-mistral-7b-instruct\"\
        \n        # Ensure it's set in your .env or config.py as RERANKER_MODEL_NAME\n\
        \        # and that you have the resources for it.\n        if RERANKER_MODEL_NAME\
        \ != \"intfloat/e5-mistral-7b-instruct\" and False: # Set to True to test\
        \ e5\n            logger.info(\"\\n--- Conceptual Test: Switching to intfloat/e5-mistral-7b-instruct\
        \ ---\")\n            logger.warning(\"This is a large model and may require\
        \ significant resources and download time.\")\n            logger.warning(\"\
        Ensure 'intfloat/e5-mistral-7b-instruct' is configured as RERANKER_MODEL_NAME\
        \ and 'trust_remote_code=True' might be needed.\")\n            \n       \
        \     try:\n                e5_model_name = \"intfloat/e5-mistral-7b-instruct\"\
        \n                e5_reranker = LLMReranker(\n                    model_name=e5_model_name,\n\
        \                    enriched_docs_path=str(ENRICHED_DOCUMENTS_FILE),\n  \
        \                  device=EMBEDDING_DEVICE # \"cuda\" recommended for this\
        \ model\n                )\n                e5_reranked_results = e5_reranker.rerank(query,\
        \ sample_documents_from_retriever, top_n=DEFAULT_RERANK_K)\n             \
        \   logger.info(f\"\\nReranked documents with {e5_model_name} (top {DEFAULT_RERANK_K}\
        \ for query '{query}'):\")\n                if e5_reranked_results:\n    \
        \                for doc in e5_reranked_results:\n                       \
        \ logger.info(f\"  ID: {doc.get('id')}, Label: {doc.get('label')}, Rerank\
        \ Score: {doc.get('rerank_score', 'N/A'):.4f}\")\n                else:\n\
        \                    logger.info(\"  No results after reranking with E5.\"\
        )\n            except Exception as e_e5:\n                logger.error(f\"\
        Could not initialize or use {e5_model_name} reranker: {e_e5}\", exc_info=True)\n\
        \                logger.warning(f\"Skipping {e5_model_name} reranking part\
        \ of the example.\")\n\n\n    except Exception as e:\n        logger.error(f\"\
        An error occurred in the main example: {e}\", exc_info=True)\n\n    logger.info(\"\
        --- LLMReranker Example Finished ---\")"
    "retriever":
      "__init__.py": ""
      "hybrid_retriever.py": "# src/retriever/hybrid_retriever.py\nimport json\nimport\
        \ os\n# import numpy as np # numpy is used by sentence_transformers and faiss\
        \ internally\nfrom whoosh.index import open_dir as open_whoosh_index\nfrom\
        \ whoosh.qparser import MultifieldParser, OrGroup\nfrom sentence_transformers\
        \ import SentenceTransformer\n\n# --- Add project root to sys.path if running\
        \ script directly ---\n# This block is useful if you ever run this script\
        \ directly (e.g., for debugging)\n# and not as a module (python -m src.retriever.hybrid_retriever)\n\
        if __name__ == '__main__':\n    import sys\n    PROJECT_ROOT_FOR_DIRECT_RUN\
        \ = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))\n\
        \    if PROJECT_ROOT_FOR_DIRECT_RUN not in sys.path:\n        sys.path.insert(0,\
        \ PROJECT_ROOT_FOR_DIRECT_RUN)\n# --- End sys.path modification ---\n\n\n\
        from src.vector_store.faiss_store import FAISSVectorStore\nfrom src.config\
        \ import (\n    ONTOLOGY_DUMP_JSON,\n    WHOOSH_INDEX_DIR,\n    FAISS_INDEX_PATH,\n\
        \    FAISS_METADATA_PATH,\n    EMBEDDING_MODEL_NAME,\n    DEFAULT_K_LEXICAL,\n\
        \    DEFAULT_K_VECTOR,\n)\n# from src.utils.logger import get_logger # Placeholder\
        \ for future logging\n# logger = get_logger(__name__) # Placeholder\n\nclass\
        \ HybridRetriever:\n    def __init__(self,\n                 ontology_data_path=ONTOLOGY_DUMP_JSON,\n\
        \                 whoosh_index_dir=WHOOSH_INDEX_DIR,\n                 faiss_index_path=FAISS_INDEX_PATH,\n\
        \                 faiss_metadata_path=FAISS_METADATA_PATH,\n             \
        \    embedding_model_name=EMBEDDING_MODEL_NAME):\n        \"\"\"\n       \
        \ Initializes the HybridRetriever.\n        \"\"\"\n        print(f\"Initializing\
        \ HybridRetriever...\")\n        # logger.info(\"Initializing HybridRetriever...\"\
        )\n\n        print(f\"Loading ontology data from: {ontology_data_path}\")\n\
        \        if not os.path.exists(ontology_data_path):\n            raise FileNotFoundError(f\"\
        Ontology data file not found: {ontology_data_path}\")\n        with open(ontology_data_path,\
        \ 'r', encoding='utf-8') as f:\n            self.ontology_data = json.load(f)\n\
        \        print(f\"Loaded {len(self.ontology_data)} ontology entries.\")\n\n\
        \        print(f\"Loading Whoosh index from: {whoosh_index_dir}\")\n     \
        \   if not os.path.exists(whoosh_index_dir) or not os.listdir(whoosh_index_dir):\n\
        \            raise FileNotFoundError(f\"Whoosh index directory not found or\
        \ empty: {whoosh_index_dir}. Run ingestion scripts.\")\n        self.whoosh_ix\
        \ = open_whoosh_index(whoosh_index_dir)\n        self.whoosh_searcher = self.whoosh_ix.searcher()\n\
        \        \n        # Fields to search in Whoosh, must match the schema in\
        \ build_lexical_index.py\n        # 'relations_text' is indexed (stored=False)\
        \ so it can be searched.\n        # 'curie' is an ID field, typically not\
        \ directly searched with MultifieldParser unless intended.\n        self.whoosh_fields_to_search\
        \ = [\"label\", \"synonyms\", \"definition\", \"relations_text\"]\n      \
        \  self.whoosh_parser = MultifieldParser(self.whoosh_fields_to_search, schema=self.whoosh_ix.schema,\
        \ group=OrGroup)\n        print(\"Whoosh index loaded.\")\n\n        print(f\"\
        Loading embedding model: {embedding_model_name}\")\n        self.embedding_model\
        \ = SentenceTransformer(embedding_model_name, trust_remote_code=True)\n  \
        \      print(\"Embedding model loaded.\")\n\n        print(f\"Initializing\
        \ FAISS vector store (index: {faiss_index_path}, metadata: {faiss_metadata_path})...\"\
        )\n        self.faiss_store = FAISSVectorStore(\n            index_path=faiss_index_path,\n\
        \            metadata_path=faiss_metadata_path,\n            embeddings_file_path=None\
        \ \n        )\n        if not self.faiss_store.index or not self.faiss_store.metadata:\n\
        \            raise FileNotFoundError(f\"FAISS index file '{faiss_index_path}'\
        \ or metadata file '{faiss_metadata_path}' not found or empty. Please build\
        \ it first.\")\n        print(\"FAISS vector store initialized.\")\n     \
        \   \n        print(\"HybridRetriever initialized successfully.\")\n\n   \
        \ def _lexical_search(self, query_string, limit=DEFAULT_K_LEXICAL):\n    \
        \    \"\"\"\n        Performs lexical search using Whoosh.\n        Returns\
        \ a list of dicts: {'id': str, 'label': str, 'score': float, 'source': 'lexical',\
        \ 'details': dict}\n        \"\"\"\n        results = []\n        if not query_string:\n\
        \            return results\n\n        try:\n            query = self.whoosh_parser.parse(query_string)\n\
        \            search_results = self.whoosh_searcher.search(query, limit=limit)\n\
        \            \n            for hit in search_results:\n                hit_fields\
        \ = hit.fields()  # Get all stored fields as a dictionary\n              \
        \  term_curie = hit_fields.get('curie') # *** CHANGED: 'id' to 'curie' ***\n\
        \n                if term_curie is None:\n                    print(f\"Warning:\
        \ Lexical search hit found without a 'curie'. Hit details: {hit}\")\n    \
        \                # logger.warning(f\"Lexical search hit found without a 'curie'.\
        \ Hit details: {hit}\")\n                    continue\n\n                #\
        \ 'relations_text' is not stored, so it won't be in hit_fields.\n        \
        \        # We retrieve label, synonyms, definition if they were stored.\n\
        \                retrieved_label = hit_fields.get('label')\n             \
        \   retrieved_synonyms_str = hit_fields.get('synonyms') # This will be a space-separated\
        \ string\n                retrieved_definition = hit_fields.get('definition')\n\
        \n                results.append({\n                    \"id\": term_curie,\
        \ # Keep 'id' as the key in the result for consistency with vector search\n\
        \                    \"label\": retrieved_label if retrieved_label is not\
        \ None else self.ontology_data.get(term_curie, {}).get('label', 'N/A'),\n\
        \                    \"score\": hit.score, \n                    \"source\"\
        : \"lexical\",\n                    \"details\": {\n                     \
        \   # Convert synonyms string back to list if needed, or keep as string\n\
        \                        \"retrieved_synonyms\": retrieved_synonyms_str.split()\
        \ if retrieved_synonyms_str else [], \n                        \"retrieved_definition\"\
        : retrieved_definition\n                    }\n                })\n      \
        \  except Exception as e:\n            print(f\"Error during lexical search\
        \ for '{query_string}': {e}\")\n            # logger.error(f\"Error during\
        \ lexical search for '{query_string}': {e}\", exc_info=True)\n           \
        \ import traceback\n            traceback.print_exc() # Print full traceback\
        \ for debugging\n        return results\n\n    def _vector_search(self, query_string,\
        \ k=DEFAULT_K_VECTOR):\n        \"\"\"\n        Performs vector search using\
        \ FAISS.\n        Returns a list of dicts: {'id': str, 'label': str, 'score':\
        \ float, 'source': 'vector', 'details': dict}\n        \"\"\"\n        results\
        \ = []\n        if not query_string:\n            return results\n\n     \
        \   try:\n            query_vector = self.embedding_model.encode([query_string],\
        \ convert_to_numpy=True)\n            distances, _, metadata_items = self.faiss_store.search(query_vector,\
        \ k=k)\n            \n            for i in range(len(metadata_items)):\n \
        \               term_id = metadata_items[i]['id'] # FAISS metadata stores\
        \ 'id'\n                results.append({\n                    \"id\": term_id,\n\
        \                    \"label\": metadata_items[i]['label'],\n            \
        \        \"score\": float(distances[i]), \n                    \"source\"\
        : \"vector\",\n                    \"details\": {}\n                })\n \
        \       except Exception as e:\n            print(f\"Error during vector search\
        \ for '{query_string}': {e}\")\n            # logger.error(f\"Error during\
        \ vector search for '{query_string}': {e}\", exc_info=True)\n            import\
        \ traceback\n            traceback.print_exc() # Print full traceback for\
        \ debugging\n        return results\n\n    def search(self, query_string,\
        \ lexical_limit=DEFAULT_K_LEXICAL, vector_k=DEFAULT_K_VECTOR):\n        \"\
        \"\"\n        Performs hybrid search.\n        \"\"\"\n        lexical_results\
        \ = self._lexical_search(query_string, limit=lexical_limit)\n        vector_results\
        \ = self._vector_search(query_string, k=vector_k)\n        \n        return\
        \ {\n            \"query\": query_string,\n            \"lexical_results\"\
        : lexical_results,\n            \"vector_results\": vector_results,\n    \
        \    }\n\n    def get_term_details(self, term_id):\n        \"\"\"\n     \
        \   Retrieves full details for a given term ID (CURIE) from the loaded ontology\
        \ data.\n        Returns a dictionary with all term details including the\
        \ ID, or None if not found.\n        \"\"\"\n        term_data = self.ontology_data.get(term_id)\n\
        \        if term_data is not None:\n            # Make a copy to avoid modifying\
        \ the original data\n            term_data = dict(term_data)\n           \
        \ # Add the ID to the returned data\n            term_data['id'] = term_id\n\
        \        return term_data\n\n    def close(self):\n        \"\"\"\n      \
        \  Closes any open resources, like the Whoosh searcher.\n        \"\"\"\n\
        \        if self.whoosh_searcher:\n            self.whoosh_searcher.close()\n\
        \        print(\"HybridRetriever resources closed.\")\n\n# Example Usage (for\
        \ testing purposes)\nif __name__ == '__main__':\n    # This sys.path modification\
        \ is now at the top of the file for when __name__ == '__main__'\n    \n  \
        \  from src.config import PROJECT_ROOT # Import after sys.path is potentially\
        \ modified\n    print(f\"Configured project root: {PROJECT_ROOT}\")\n    if\
        \ not os.getcwd().startswith(PROJECT_ROOT) and os.getcwd() != PROJECT_ROOT:\n\
        \         print(f\"Warning: Current working directory ({os.getcwd()}) might\
        \ not be the project root.\")\n         print(\"Consider running with 'python\
        \ -m src.retriever.hybrid_retriever' from the project root directory.\")\n\
        \n    print(\"Running HybridRetriever example...\")\n    retriever = None\n\
        \    try:\n        retriever = HybridRetriever()\n        \n        queries\
        \ = [\"GARLIC\", \"SALT\", \"GARBANZO\", \"TAHINI\", \"LEMON JUICE\", \"HONEY\"\
        \ ,\"WATER\", \"OLIVE OIL\", \"ROSMARY\", \"HUMMUS\"]\n        \n        for\
        \ query in queries:\n            print(f\"\\nSearching for: '{query}'\")\n\
        \            results = retriever.search(query, lexical_limit=3, vector_k=3)\n\
        \            \n            print(\"\\n--- Lexical Results ---\")\n       \
        \     if results[\"lexical_results\"]:\n                for res in results[\"\
        lexical_results\"]:\n                    print(f\"  ID: {res['id']}, Label:\
        \ {res['label']}, Score (Whoosh): {res['score']:.4f}\")\n                \
        \    # print(f\"    Details: {res['details']}\") # Uncomment to see retrieved\
        \ synonyms/def\n            else:\n                print(\"  No lexical results.\"\
        )\n\n            print(\"\\n--- Vector Results ---\")\n            if results[\"\
        vector_results\"]:\n                for res in results[\"vector_results\"\
        ]:\n                    print(f\"  ID: {res['id']}, Label: {res['label']},\
        \ Score (L2 Distance): {res['score']:.4f}\")\n            else:\n        \
        \        print(\"  No vector results.\")\n            print(\"-\" * 40)\n\
        \            \n    except FileNotFoundError as e:\n        print(f\"\\nERROR:\
        \ A required file was not found: {e}\")\n        print(\"Please ensure all\
        \ data files (ontology_dump.json) and indices (Whoosh, FAISS) are correctly\
        \ built and paths are set in src/config.py.\")\n        print(\"You might\
        \ need to run the ingestion and embedding scripts first.\")\n    except Exception\
        \ as e:\n        print(f\"\\nAn unexpected error occurred during example run:\
        \ {e}\")\n        import traceback\n        traceback.print_exc()\n    finally:\n\
        \        if retriever:\n            retriever.close()"
    "tests":
      "__init__.py": ""
      "test_lexical_index.py": |-
        import pytest
        import os
        import json
        import tempfile
        import shutil
        from whoosh.index import open_dir
        from whoosh.qparser import QueryParser

        # Adjust path to import modules from src
        import sys
        sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

        from ingestion.parse_ontology import main as parse_ontology_main # Need to run parsing first
        from ingestion.build_lexical_index import build_index
        import config # Need config to override paths

        # Define a known CURIE and search terms from the test snippet
        TEST_CURIE = "FOODON:00001100" # apple
        TEST_LABEL = "apple"
        TEST_SYNONYM = "eating apple"
        TEST_DEFINITION_KEYWORD = "fruit" # Keyword from definition "The fruit of a Malus domestica tree."
        TEST_RELATION_KEYWORD = "apple tree" # Keyword from relations_text (target label)


        # Fixture to create a temporary directory for the test index
        @pytest.fixture(scope="module")
        def temp_index_dir():
            """Creates a temporary directory for the Whoosh index."""
            path = tempfile.mkdtemp(prefix="whoosh_test_index_")
            print(f"Created temp index dir: {path}")
            yield path
            print(f"Cleaning up temp index dir: {path}")
            shutil.rmtree(path)

        # Fixture to run the full ingestion pipeline on the test snippet and build index
        @pytest.fixture(scope="module")
        def built_test_index(temp_index_dir):
            """Runs parse_ontology and build_index on the test snippet."""
            # Use temp files/dirs for test data and index
            with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as tmp_json:
                test_json_path = tmp_json.name

            # Temporarily override config paths for the test
            original_ontology_file = config.ONTOLOGY_FILE
            original_dump_path = config.ONTOLOGY_DUMP_PATH
            original_index_dir = config.WHOOSH_INDEX_DIR

            config.ONTOLOGY_FILE = config.TEST_ONTOLOGY_FILE # Use the small test snippet
            config.ONTOLOGY_DUMP_PATH = test_json_path
            config.WHOOSH_INDEX_DIR = temp_index_dir

            try:
                # 1. Run parsing
                print("\nRunning parse_ontology_main for test index build...")
                parse_ontology_main()
                assert os.path.exists(test_json_path), "Ontology dump JSON not created!"

                # 2. Run index building
                print("\nRunning build_index for test index build...")
                build_index(test_json_path, temp_index_dir)
                assert os.path.exists(os.path.join(temp_index_dir, 'SCHEMA')), "Whoosh index not created!"

                # Yield the index directory path
                yield temp_index_dir

            finally:
                # Restore original paths
                config.ONTOLOGY_FILE = original_ontology_file
                config.ONTOLOGY_DUMP_PATH = original_dump_path
                config.WHOOSH_INDEX_DIR = original_index_dir
                # Clean up temporary JSON file
                if os.path.exists(test_json_path):
                    os.unlink(test_json_path)


        def test_index_exists(built_test_index):
            """Tests if the index directory was created and contains index files."""
            assert os.path.exists(built_test_index)
            assert os.path.exists(os.path.join(built_test_index, 'SCHEMA'))
            assert os.path.exists(os.path.join(built_test_index, 'MAIN')) # Or other index files

        def test_query_by_label_exact(built_test_index):
            """Tests exact search on the label field."""
            ix = open_dir(built_test_index)
            searcher = ix.searcher()
            # Query the 'label' field
            parser = QueryParser("label", ix.schema)
            query = parser.parse(TEST_LABEL) # "apple"
            results = searcher.search(query)

            print(f"\nSearch results for '{TEST_LABEL}': {results}")

            assert len(results) > 0, f"No results found for '{TEST_LABEL}'"
            # Check if the expected CURIE is among the results, preferably the top one
            assert results[0]['curie'] == TEST_CURIE
            assert results[0]['label'] == TEST_LABEL # Verify stored label

            searcher.close()
            ix.close() # Close index readers/writers

        def test_query_by_label_fuzzy(built_test_index):
            """Tests fuzzy search on the label field."""
            ix = open_dir(built_test_index)
            searcher = ix.searcher()
            parser = QueryParser("label", ix.schema)
            # Fuzzy search for "appl" (e.g. "appl~")
            query = parser.parse("appl~") # Whoosh fuzzy syntax
            results = searcher.search(query)

            print(f"\nFuzzy search results for 'appl~': {results}")

            assert len(results) > 0, "No results found for 'appl~'"
            assert results[0]['curie'] == TEST_CURIE
            assert results[0]['label'] == TEST_LABEL

            searcher.close()
            ix.close()

        def test_query_by_synonym(built_test_index):
            """Tests search on the synonyms field."""
            ix = open_dir(built_test_index)
            searcher = ix.searcher()
            # Query the 'synonyms' field
            parser = QueryParser("synonyms", ix.schema)
            query = parser.parse(TEST_SYNONYM) # "eating apple"
            results = searcher.search(query)

            print(f"\nSearch results for synonym '{TEST_SYNONYM}': {results}")

            assert len(results) > 0, f"No results found for synonym '{TEST_SYNONYM}'"
            assert results[0]['curie'] == TEST_CURIE

            searcher.close()
            ix.close()

        def test_query_by_definition_keyword(built_test_index):
            """Tests search on the definition field."""
            ix = open_dir(built_test_index)
            searcher = ix.searcher()
            # Query the 'definition' field
            parser = QueryParser("definition", ix.schema)
            query = parser.parse(TEST_DEFINITION_KEYWORD) # "fruit"
            results = searcher.search(query)

            print(f"\nSearch results for definition keyword '{TEST_DEFINITION_KEYWORD}': {results}")

            assert len(results) > 0, f"No results found for definition keyword '{TEST_DEFINITION_KEYWORD}'"
            assert results[0]['curie'] == TEST_CURIE

            searcher.close()
            ix.close()

        def test_query_by_relations_keyword(built_test_index):
            """Tests search on the flattened relations_text field."""
            ix = open_dir(built_test_index)
            searcher = ix.searcher()
            # Query the 'relations_text' field
            parser = QueryParser("relations_text", ix.schema)
            # Search for a keyword expected in the relations_text (e.g., target's label or part of target's CURIE)
            # Note: Indexing targets as just CURIEs requires searching for 'FOODON:00001101'
            # Indexing target labels requires adding that logic in build_lexical_index
            # Let's assume for now we search for the CURIE or part of it if indexed as text
            # Or if we enhance build_index to add target labels to relations_text:
            # relations_text = " ".join([f"{prop}: {' '.join(targets)} {' '.join(target_labels)}" for ...])
            # For simplicity with current build_index, let's search the target CURIE part
            query = parser.parse("00001101") # Search part of the target CURIE FOODON:00001101
            results = searcher.search(query)

            print(f"\nSearch results for relations keyword '00001101': {results}")

            assert len(results) > 0, f"No results found for relations keyword '00001101'"
            assert results[0]['curie'] == TEST_CURIE

            searcher.close()
            ix.close()
      "test_parse_ontology.py": |-
        import pytest
        import os
        import json
        import rdflib
        from rdflib import Graph

        # Adjust path to import modules from src
        import sys
        sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

        from ingestion.parse_ontology import (
            load_ontology,
            extract_labels_and_synonyms,
            extract_definitions,
            extract_hierarchy,
            extract_relations,
        )
        from config import TEST_ONTOLOGY_FILE, NAMESPACE_MAP, RELATION_PROPERTIES
        from utils.ontology_utils import uri_to_curie # Import if needed for assertions

        # Define a fixture to load the test graph once for all tests
        @pytest.fixture(scope="module")
        def test_graph():
            """Loads the test ontology snippet into a graph."""
            if not os.path.exists(TEST_ONTOLOGY_FILE):
                pytest.skip(f"Test ontology snippet not found at {TEST_ONTOLOGY_FILE}")
            try:
                return load_ontology(TEST_ONTOLOGY_FILE)
            except Exception as e:
                pytest.fail(f"Failed to load test ontology: {e}")

        # Define a known CURIE from the test snippet
        TEST_CURIE = "FOODON:00001100" # apple
        TEST_CURIE_PARENT1 = "FOODON:00001000" # plant-based food
        TEST_CURIE_PARENT2 = "FOODON:00002000" # Pome fruit
        TEST_CURIE_RELATION_TARGET = "FOODON:00001101" # apple tree (example target)
        TEST_RELATION_NAME = "part_of" # example relation name

        def test_load_ontology(test_graph):
            """Tests if the ontology loads and is an RDFLib Graph."""
            assert isinstance(test_graph, Graph)
            assert len(test_graph) > 0, "Test graph is empty!"
            print(f"Loaded test graph with {len(test_graph)} triples.")


        def test_extract_labels_and_synonyms(test_graph):
            """Tests extraction of labels and synonyms."""
            labels_synonyms = extract_labels_and_synonyms(test_graph)
            print(f"Extracted labels/synonyms: {labels_synonyms}")

            assert TEST_CURIE in labels_synonyms
            apple_data = labels_synonyms[TEST_CURIE]
            assert apple_data['label'] == "apple"
            assert "eating apple" in apple_data['synonyms']
            assert "apple fruit" in apple_data['synonyms']
            assert TEST_CURIE_PARENT1 in labels_synonyms # Should also extract its label/synonyms
            assert labels_synonyms[TEST_CURIE_PARENT1]['label'] == "plant-based food"


        def test_extract_definitions(test_graph):
            """Tests extraction of definitions."""
            definitions = extract_definitions(test_graph)
            print(f"Extracted definitions: {definitions}")

            assert TEST_CURIE in definitions
            assert "The fruit of a Malus domestica tree." in definitions[TEST_CURIE] # Use 'in' for substring check or exact match


        def test_extract_hierarchy(test_graph):
            """Tests extraction of hierarchy (parents and ancestors)."""
            hierarchy = extract_hierarchy(test_graph)
            print(f"Extracted hierarchy: {hierarchy}")

            assert TEST_CURIE in hierarchy
            apple_hierarchy = hierarchy[TEST_CURIE]

            # Check direct parents
            assert TEST_CURIE_PARENT1 in apple_hierarchy['parents']
            assert TEST_CURIE_PARENT2 in apple_hierarchy['parents']
            assert len(apple_hierarchy['parents']) >= 2 # At least 2 parents from the snippet

            # Check ancestors (should include parents and parents' ancestors)
            assert TEST_CURIE_PARENT1 in apple_hierarchy['ancestors']
            assert TEST_CURIE_PARENT2 in apple_hierarchy['ancestors']
            # Assuming plant-based food has no ancestors in this snippet, ancestors == parents here
            # If plant-based food had parents, they should appear in apple's ancestors.
            # For this snippet, ancestors should be the same as parents.
            assert set(apple_hierarchy['ancestors']) == set(apple_hierarchy['parents'])


        def test_extract_relations(test_graph):
            """Tests extraction of specific relations."""
            relations = extract_relations(test_graph, RELATION_PROPERTIES)
            print(f"Extracted relations: {relations}")

            assert TEST_CURIE in relations
            apple_relations = relations[TEST_CURIE]

            # Check if the specific relation from the snippet was found
            assert TEST_RELATION_NAME in apple_relations
            assert TEST_CURIE_RELATION_TARGET in apple_relations[TEST_RELATION_NAME]
            assert len(apple_relations[TEST_RELATION_NAME]) >= 1


        # You could add a test that runs the full main parsing flow using the test snippet
        # and checks the structure/content of the resulting JSON file.
        # Example (requires a temporary file):
        # import tempfile
        # @pytest.fixture
        # def temp_json_path():
        #     with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as tmp:
        #         yield tmp.name
        #     os.unlink(tmp.name)
        #
        # def test_main_parsing_flow(temp_json_path):
        #      # Temporarily override config paths for the test
        #      original_ontology_file = config.ONTOLOGY_FILE
        #      original_dump_path = config.ONTOLOGY_DUMP_PATH
        #      config.ONTOLOGY_FILE = TEST_ONTOLOGY_FILE
        #      config.ONTOLOGY_DUMP_PATH = temp_json_path
        #
        #      try:
        #          parse_ontology.main()
        #          assert os.path.exists(temp_json_path)
        #          with open(temp_json_path, 'r', encoding='utf-8') as f:
        #              data = json.load(f)
        #
        #          assert TEST_CURIE in data
        #          apple_data = data[TEST_CURIE]
        #          assert apple_data['label'] == 'apple'
        #          assert 'synonyms' in apple_data and len(apple_data['synonyms']) > 0
        #          assert 'definition' in apple_data and apple_data['definition'] is not None
        #          assert 'parents' in apple_data and len(apple_data['parents']) > 0
        #          assert 'ancestors' in apple_data and len(apple_data['ancestors']) > 0
        #          assert 'relations' in apple_data and len(apple_data['relations']) > 0
        #
        #      finally:
        #          # Restore original paths
        #          config.ONTOLOGY_FILE = original_ontology_file
        #          config.ONTOLOGY_DUMP_PATH = original_dump_path
    "utils":
      "__init__.py": ""
      "ontology_utils.py": "# src/utils/ontology_utils.py\nimport rdflib\nfrom rdflib\
        \ import URIRef, Namespace\nfrom typing import Optional, Dict, Union\n\n#\
        \ Adjusted: Import CURIE_PREFIX_MAP and use it as the default.\n# This assumes\
        \ that the script/module *importing* ontology_utils\n# has already ensured\
        \ that the project root (e.g., 'onto_rag') is on sys.path,\n# so that 'src'\
        \ is resolvable as a top-level package.\nfrom src.config import CURIE_PREFIX_MAP\
        \ # Corrected import\n\ndef uri_to_curie(uri: Union[str, URIRef], namespace_map:\
        \ Dict[str, str] = CURIE_PREFIX_MAP) -> str:\n    \"\"\"\n    Converts a full\
        \ URI to a CURIE (e.g., http://...#term -> PREFIX:term).\n    Assumes namespace_map\
        \ is structured as {base_uri_str: prefix_str}.\n    \"\"\"\n    uri_str =\
        \ str(uri) # Ensure it's a string\n\n    # Iterate through the provided namespace_map\
        \ (base_uri: prefix)\n    # Sort by length of base_uri descending to match\
        \ longest first (more specific)\n    # This helps avoid issues where one base_uri\
        \ is a prefix of another.\n    # e.g., \"http://purl.obolibrary.org/obo/\"\
        \ and \"http://purl.obolibrary.org/obo/FOODON_\"\n    sorted_namespace_map_items\
        \ = sorted(namespace_map.items(), key=lambda item: len(item[0]), reverse=True)\n\
        \n    for base_uri, prefix in sorted_namespace_map_items:\n        if uri_str.startswith(base_uri):\n\
        \            return f\"{prefix}:{uri_str[len(base_uri):]}\"\n\n    # Fallback\
        \ for common RDF/RDFS/OWL/XSD prefixes if not found in the main map\n    #\
        \ This local map is prefix: base_uri_str\n    common_rdf_prefixes = {\n  \
        \      \"rdf\": str(rdflib.RDF),\n        \"rdfs\": str(rdflib.RDFS),\n  \
        \      \"owl\": str(rdflib.OWL),\n        \"xsd\": str(rdflib.XSD),\n    }\n\
        \    for prefix, base_uri in common_rdf_prefixes.items():\n        if uri_str.startswith(base_uri):\n\
        \            return f\"{prefix}:{uri_str[len(base_uri):]}\"\n            \n\
        \    # If rdflib can make a qname (usually for registered namespaces)\n  \
        \  try:\n        g = rdflib.Graph()\n        # Bind known namespaces to help\
        \ compute_qname\n        # For namespace_map (base_uri: prefix), we need to\
        \ iterate as base_uri, prefix\n        for ns_uri_str_from_map, prefix_from_map\
        \ in namespace_map.items():\n             g.bind(prefix_from_map, Namespace(ns_uri_str_from_map))\n\
        \n        # Bind common RDF ones too, in case they weren't in namespace_map\
        \ or to ensure standard prefixes\n        g.bind(\"owl\", rdflib.OWL)\n  \
        \      g.bind(\"rdf\", rdflib.RDF)\n        g.bind(\"rdfs\", rdflib.RDFS)\n\
        \        g.bind(\"xsd\", rdflib.XSD)\n\n        # compute_qname might fail\
        \ if the URI doesn't match any bound namespace's base\n        # It returns\
        \ (prefix, namespace_uri, local_name)\n        qname_tuple = g.compute_qname(URIRef(uri_str))\n\
        \        return f\"{qname_tuple[0]}:{qname_tuple[2]}\"\n    except Exception:\
        \ # Broad except as compute_qname can raise various things or return unexpected\
        \ tuples\n        pass # If rdflib fails, just return the original URI string\n\
        \n    return uri_str # If no CURIE conversion possible, return original URI\
        \ string\n\ndef curie_to_uri(curie: str, namespace_map: Dict[str, str] = CURIE_PREFIX_MAP)\
        \ -> Optional[URIRef]:\n    \"\"\"\n    Converts a CURIE (e.g., PREFIX:term)\
        \ to a full rdflib.URIRef.\n    Assumes namespace_map is structured as {base_uri_str:\
        \ prefix_str}.\n    \"\"\"\n    if ':' not in curie:\n        # Try to see\
        \ if it's a default rdflib qname like \"rdf:type\" that rdflib can expand\n\
        \        # This part might be less common if CURIEs are always expected with\
        \ user-defined prefixes\n        try:\n            g = rdflib.Graph()\n  \
        \          # Bind namespaces from the map (base_uri: prefix)\n           \
        \ for ns_uri_str_from_map, prefix_from_map in namespace_map.items():\n   \
        \             g.bind(prefix_from_map, Namespace(ns_uri_str_from_map))\n  \
        \          # Bind common RDF ones\n            g.bind(\"owl\", rdflib.OWL)\n\
        \            g.bind(\"rdf\", rdflib.RDF)\n            g.bind(\"rdfs\", rdflib.RDFS)\n\
        \            g.bind(\"xsd\", rdflib.XSD)\n            \n            # If it's\
        \ something like \"owl:Class\", g.namespace_manager.expand_curie will work\n\
        \            expanded_uri = g.namespace_manager.expand_curie(curie)\n    \
        \        if str(expanded_uri) != curie: # Check if expansion actually happened\n\
        \                 return URIRef(expanded_uri)\n        except Exception:\n\
        \            pass # If expansion fails, proceed to manual lookup\n       \
        \ return None # Not a valid CURIE format for our map, and rdflib couldn't\
        \ expand\n\n    prefix_part, local_name = curie.split(':', 1)\n\n    # Iterate\
        \ through namespace_map (base_uri: prefix) to find the matching prefix\n \
        \   found_base_uri = None\n    for base_uri_key, prefix_val in namespace_map.items():\n\
        \        if prefix_val == prefix_part:\n            found_base_uri = base_uri_key\n\
        \            break\n    \n    if found_base_uri:\n        return URIRef(found_base_uri\
        \ + local_name)\n    else:\n        # Fallback: Try common RDF prefixes if\
        \ not in the custom map\n        # This local map is prefix: rdflib.Namespace\
        \ object\n        common_rdf_namespaces = {\n            \"rdf\": rdflib.RDF,\n\
        \            \"rdfs\": rdflib.RDFS,\n            \"owl\": rdflib.OWL,\n  \
        \          \"xsd\": rdflib.XSD,\n        }\n        if prefix_part in common_rdf_namespaces:\n\
        \            # Access items in the namespace like attributes: common_rdf_namespaces[prefix_part].type\n\
        \            # or by string concatenation: URIRef(str(common_rdf_namespaces[prefix_part])\
        \ + local_name)\n            return URIRef(str(common_rdf_namespaces[prefix_part])\
        \ + local_name)\n        return None # Prefix not found in custom map or common\
        \ RDF prefixes"
    "vector_store":
      "__init__.py": ""
      "faiss_store.py": "# src/vector_store/faiss_store.py\nimport faiss\nimport json\n\
        import numpy as np\nimport os\nfrom src.config import EMBEDDINGS_FILE # Default\
        \ embeddings file to build from\n\n# from src.utils.logger import get_logger\
        \ # Placeholder for future logging\n# logger = get_logger(__name__)\n\nclass\
        \ FAISSVectorStore:\n    def __init__(self, index_path, metadata_path, embeddings_file_path=None,\
        \ dimension=None):\n        \"\"\"\n        Initializes the FAISSVectorStore.\n\
        \        Tries to load an existing index and metadata. If not found, and an\n\
        \        embeddings_file_path is provided, it will attempt to build them.\n\
        \n        Args:\n            index_path (str): Path to save/load the FAISS\
        \ index file (.bin).\n            metadata_path (str): Path to save/load the\
        \ metadata JSON file (.json).\n            embeddings_file_path (str, optional):\
        \ Path to the 'embeddings.json' file.\n                                  \
        \                Used only if the index/metadata needs to be built.\n    \
        \                                              Defaults to config.EMBEDDINGS_FILE.\n\
        \            dimension (int, optional): The dimension of the vectors.\n  \
        \                                     Required if building the index and it\
        \ cannot be inferred.\n                                       Usually inferred\
        \ from the first embedding.\n        \"\"\"\n        self.index_path = index_path\n\
        \        self.metadata_path = metadata_path\n        self.embeddings_file_path\
        \ = embeddings_file_path if embeddings_file_path else EMBEDDINGS_FILE\n  \
        \      self.dimension = dimension\n\n        self.index = None\n        self.metadata\
        \ = []  # List of dicts, e.g., [{'id': 'FOODON_123', 'label': 'Apple'}, ...]\n\
        \n        if os.path.exists(self.index_path) and os.path.exists(self.metadata_path):\n\
        \            print(f\"Loading existing FAISS index from {self.index_path}\
        \ and metadata from {self.metadata_path}\")\n            # logger.info(f\"\
        Loading existing FAISS index from {self.index_path} and metadata from {self.metadata_path}\"\
        )\n            try:\n                self.load_store()\n            except\
        \ Exception as e:\n                print(f\"Error loading existing FAISS store:\
        \ {e}. Will attempt to build if embeddings file provided.\")\n           \
        \     # logger.error(f\"Error loading existing FAISS store: {e}. Will attempt\
        \ to build if embeddings file provided.\", exc_info=True)\n              \
        \  self._try_build_store()\n        elif self.embeddings_file_path and os.path.exists(self.embeddings_file_path):\n\
        \            print(f\"FAISS index/metadata not found. Attempting to build\
        \ from {self.embeddings_file_path}\")\n            # logger.info(f\"FAISS\
        \ index/metadata not found. Attempting to build from {self.embeddings_file_path}\"\
        )\n            self._try_build_store()\n        else:\n            message\
        \ = \"FAISS index/metadata not found. \"\n            if self.embeddings_file_path:\n\
        \                message += f\"Embeddings file {self.embeddings_file_path}\
        \ also not found or not specified for building.\"\n            else:\n   \
        \             message += \"No embeddings file path provided to build a new\
        \ store.\"\n            print(f\"Warning: {message}\")\n            # logger.warning(message)\n\
        \n    def _try_build_store(self):\n        \"\"\"Helper method to attempt\
        \ building the store.\"\"\"\n        embeddings_data = self._load_embeddings_data()\n\
        \        if embeddings_data:\n            self.build_index_from_embeddings(embeddings_data)\n\
        \            if self.index and self.metadata: # Check if build was successful\n\
        \                 self.save_store()\n            else:\n                print(\"\
        Warning: FAISS index or metadata not built successfully from embeddings data.\"\
        )\n                # logger.warning(\"FAISS index or metadata not built successfully\
        \ from embeddings data.\")\n        else:\n            print(f\"Warning: No\
        \ embeddings data loaded from {self.embeddings_file_path}, FAISS index not\
        \ built.\")\n            # logger.warning(f\"No embeddings data loaded from\
        \ {self.embeddings_file_path}, FAISS index not built.\")\n\n\n    def _load_embeddings_data(self):\n\
        \        \"\"\"\n        Loads embeddings data from the specified JSON file.\n\
        \        Expected format: list of {'id': str, 'label': str, 'embedding': list[float]}\n\
        \        \"\"\"\n        if not self.embeddings_file_path or not os.path.exists(self.embeddings_file_path):\n\
        \            print(f\"Error: Embeddings file not found: {self.embeddings_file_path}\"\
        )\n            # logger.error(f\"Embeddings file not found: {self.embeddings_file_path}\"\
        )\n            return None\n        try:\n            with open(self.embeddings_file_path,\
        \ 'r', encoding='utf-8') as f:\n                data = json.load(f)\n    \
        \        # Sanity check the data structure\n            if not isinstance(data,\
        \ list) or not data:\n                print(f\"Error: Embeddings file {self.embeddings_file_path}\
        \ is empty or not a list.\")\n                # logger.error(f\"Embeddings\
        \ file {self.embeddings_file_path} is empty or not a list.\")\n          \
        \      return None\n            if not all('id' in item and 'label' in item\
        \ and 'embedding' in item for item in data):\n                print(f\"Error:\
        \ Embeddings data in {self.embeddings_file_path} has incorrect format.\")\n\
        \                # logger.error(f\"Embeddings data in {self.embeddings_file_path}\
        \ has incorrect format.\")\n                return None\n\n            print(f\"\
        Loaded {len(data)} embeddings from {self.embeddings_file_path}\")\n      \
        \      # logger.info(f\"Loaded {len(data)} embeddings from {self.embeddings_file_path}\"\
        )\n            return data\n        except json.JSONDecodeError as e:\n  \
        \          print(f\"Error decoding JSON from embeddings file {self.embeddings_file_path}:\
        \ {e}\")\n            # logger.error(f\"Error decoding JSON from embeddings\
        \ file {self.embeddings_file_path}: {e}\", exc_info=True)\n            return\
        \ None\n        except Exception as e:\n            print(f\"An unexpected\
        \ error occurred while loading embeddings file {self.embeddings_file_path}:\
        \ {e}\")\n            # logger.error(f\"An unexpected error occurred while\
        \ loading embeddings file {self.embeddings_file_path}: {e}\", exc_info=True)\n\
        \            return None\n\n    def build_index_from_embeddings(self, embeddings_data):\n\
        \        \"\"\"\n        Builds the FAISS index and prepares metadata from\
        \ loaded embeddings data.\n\n        Args:\n            embeddings_data (list):\
        \ List of dictionaries, where each dict has\n                            \
        \        'id', 'label', and 'embedding' keys.\n        \"\"\"\n        if\
        \ not embeddings_data:\n            print(\"Warning: No embeddings data provided\
        \ to build_index_from_embeddings.\")\n            # logger.warning(\"No embeddings\
        \ data provided to build_index_from_embeddings.\")\n            return\n\n\
        \        vectors = []\n        current_metadata = [] # Use a temporary list\
        \ to build metadata\n        for item in embeddings_data:\n            vectors.append(item['embedding'])\n\
        \            current_metadata.append({'id': item['id'], 'label': item['label']})\n\
        \n        if not vectors:\n            print(\"Warning: No vectors extracted\
        \ from embeddings_data.\")\n            # logger.warning(\"No vectors extracted\
        \ from embeddings_data.\")\n            return\n\n        vectors_np = np.array(vectors).astype('float32')\n\
        \n        if self.dimension is None:\n            self.dimension = vectors_np.shape[1]\n\
        \        elif self.dimension != vectors_np.shape[1]:\n            err_msg\
        \ = f\"Provided dimension {self.dimension} does not match embeddings dimension\
        \ {vectors_np.shape[1]}\"\n            print(f\"Error: {err_msg}\")\n    \
        \        # logger.error(err_msg)\n            raise ValueError(err_msg)\n\n\
        \        # Using IndexFlatL2, a simple L2 distance index.\n        # For larger\
        \ datasets, more advanced indexes like IndexIVFFlat might be better.\n   \
        \     try:\n            self.index = faiss.IndexFlatL2(self.dimension)\n \
        \           self.index.add(vectors_np)\n            self.metadata = current_metadata\
        \ # Assign once build is successful\n            print(f\"FAISS index built\
        \ successfully with {self.index.ntotal} vectors of dimension {self.dimension}.\"\
        )\n            # logger.info(f\"FAISS index built successfully with {self.index.ntotal}\
        \ vectors of dimension {self.dimension}.\")\n        except Exception as e:\n\
        \            print(f\"Error building FAISS index: {e}\")\n            # logger.error(f\"\
        Error building FAISS index: {e}\", exc_info=True)\n            self.index\
        \ = None # Ensure index is None if build fails\n            self.metadata\
        \ = []\n\n\n    def save_store(self):\n        \"\"\"Saves the FAISS index\
        \ and metadata to their respective files.\"\"\"\n        if self.index is\
        \ None:\n            print(\"Warning: No FAISS index to save.\")\n       \
        \     # logger.warning(\"No FAISS index to save.\")\n            return #\
        \ Do not save metadata if index is not there or failed to build\n\n      \
        \  os.makedirs(os.path.dirname(self.index_path), exist_ok=True)\n        faiss.write_index(self.index,\
        \ self.index_path)\n        print(f\"FAISS index saved to {self.index_path}\"\
        )\n        # logger.info(f\"FAISS index saved to {self.index_path}\")\n\n\
        \        if not self.metadata:\n            print(\"Warning: No FAISS metadata\
        \ to save (metadata list is empty).\")\n            # logger.warning(\"No\
        \ FAISS metadata to save (metadata list is empty).\")\n            # If index\
        \ exists but metadata is empty, this is an inconsistent state.\n         \
        \   # Depending on strictness, one might choose to not save the index either,\n\
        \            # or clear the index file if it exists. For now, we save index\
        \ if it exists.\n            return\n\n        os.makedirs(os.path.dirname(self.metadata_path),\
        \ exist_ok=True)\n        with open(self.metadata_path, 'w', encoding='utf-8')\
        \ as f:\n            json.dump(self.metadata, f, indent=4)\n        print(f\"\
        FAISS metadata saved to {self.metadata_path}\")\n        # logger.info(f\"\
        FAISS metadata saved to {self.metadata_path}\")\n\n    def load_store(self):\n\
        \        \"\"\"Loads the FAISS index and metadata from files.\"\"\"\n    \
        \    if not os.path.exists(self.index_path):\n            # logger.error(f\"\
        FAISS index file not found: {self.index_path}\")\n            raise FileNotFoundError(f\"\
        FAISS index file not found: {self.index_path}\")\n        self.index = faiss.read_index(self.index_path)\n\
        \        if self.dimension is None: # Infer dimension if not set\n       \
        \     self.dimension = self.index.d\n        elif self.dimension != self.index.d:\
        \ # Check consistency\n            # logger.warning(f\"Stored index dimension\
        \ {self.index.d} differs from configured {self.dimension}. Using stored.\"\
        )\n            print(f\"Warning: Stored index dimension {self.index.d} differs\
        \ from configured {self.dimension}. Using stored.\")\n            self.dimension\
        \ = self.index.d\n\n        print(f\"FAISS index loaded from {self.index_path}.\
        \ Index has {self.index.ntotal} vectors of dim {self.index.d}.\")\n      \
        \  # logger.info(f\"FAISS index loaded from {self.index_path}. Index has {self.index.ntotal}\
        \ vectors of dim {self.index.d}.\")\n\n        if not os.path.exists(self.metadata_path):\n\
        \            # logger.error(f\"FAISS metadata file not found: {self.metadata_path}\"\
        )\n            raise FileNotFoundError(f\"FAISS metadata file not found: {self.metadata_path}\"\
        )\n        with open(self.metadata_path, 'r', encoding='utf-8') as f:\n  \
        \          self.metadata = json.load(f)\n        print(f\"FAISS metadata loaded\
        \ from {self.metadata_path}. {len(self.metadata)} items.\")\n        # logger.info(f\"\
        FAISS metadata loaded from {self.metadata_path}. {len(self.metadata)} items.\"\
        )\n\n        if self.index and self.metadata and self.index.ntotal != len(self.metadata):\n\
        \            mismatch_msg = (f\"Mismatch between FAISS index size ({self.index.ntotal})\
        \ \"\n                            f\"and metadata size ({len(self.metadata)}).\
        \ This may lead to errors.\")\n            print(f\"Warning: {mismatch_msg}\"\
        )\n            # logger.warning(mismatch_msg)\n            # Potentially raise\
        \ an error or try to reconcile, for now, just warn.\n\n    def search(self,\
        \ query_vector, k=5):\n        \"\"\"\n        Performs a K-Nearest Neighbors\
        \ search on the FAISS index.\n\n        Args:\n            query_vector (np.ndarray):\
        \ A 2D numpy array of shape (num_queries, dimension)\n                   \
        \                    or a 1D numpy array (single query).\n            k (int):\
        \ The number of nearest neighbors to retrieve.\n\n        Returns:\n     \
        \       tuple: (distances, indices, metadata_items)\n                   distances\
        \ (np.ndarray): Distances to the k nearest neighbors.\n                  \
        \ indices (np.ndarray): FAISS internal indices of the k nearest neighbors.\n\
        \                   metadata_items (list of lists or list of dicts): Corresponding\
        \ metadata for the results.\n                                     If single\
        \ query, returns list of dicts. If multiple, list of lists of dicts.\n   \
        \     \"\"\"\n        if self.index is None:\n            print(\"Error: FAISS\
        \ index not initialized. Cannot perform search.\")\n            # logger.error(\"\
        FAISS index not initialized. Cannot perform search.\")\n            return\
        \ np.array([]), np.array([]), []\n\n        if not isinstance(query_vector,\
        \ np.ndarray):\n            query_vector = np.array(query_vector)\n\n    \
        \    if query_vector.ndim == 1: # Single query vector\n            query_vector\
        \ = np.expand_dims(query_vector, axis=0)\n        \n        if query_vector.shape[1]\
        \ != self.index.d:\n            err_msg = f\"Query vector dimension ({query_vector.shape[1]})\
        \ does not match index dimension ({self.index.d}).\"\n            print(f\"\
        Error: {err_msg}\")\n            # logger.error(err_msg)\n            raise\
        \ ValueError(err_msg)\n\n        distances, faiss_indices = self.index.search(query_vector.astype('float32'),\
        \ k)\n        \n        # faiss_indices will be shape (num_queries, k)\n \
        \       # distances will be shape (num_queries, k)\n        \n        all_results_metadata\
        \ = []\n        for i in range(faiss_indices.shape[0]): # Iterate over queries\n\
        \            query_results_metadata = []\n            for j in range(faiss_indices.shape[1]):\
        \ # Iterate over k results for that query\n                idx = faiss_indices[i][j]\n\
        \                if idx != -1 and idx < len(self.metadata): # faiss_index\
        \ can be -1 if k > ntotal\n                    query_results_metadata.append(self.metadata[idx])\n\
        \                # else: could append a placeholder, or log. For now, items\
        \ are just shorter if fewer than k found.\n            all_results_metadata.append(query_results_metadata)\n\
        \        \n        # For a single query_vector (most common use case here),\
        \ return the inner list directly.\n        if query_vector.shape[0] == 1:\n\
        \            return distances[0], faiss_indices[0], all_results_metadata[0]\n\
        \        else: # If multiple query vectors were passed\n            return\
        \ distances, faiss_indices, all_results_metadata\n\n# Example usage / test\
        \ function\nif __name__ == '__main__':\n    from src.vector_store.faiss_store\
        \ import FAISSVectorStore\n    from src.config import FAISS_INDEX_PATH, FAISS_METADATA_PATH\n\
        \n    # remove old files so the class knows it has to build\n    import os,\
        \ pathlib, json\n    for p in (FAISS_INDEX_PATH, FAISS_METADATA_PATH):\n \
        \       pathlib.Path(p).unlink(missing_ok=True)\n\n    store = FAISSVectorStore(\n\
        \        index_path     = FAISS_INDEX_PATH,\n        metadata_path  = FAISS_METADATA_PATH,\n\
        \        # embeddings_file_path=None  → default picks data/embeddings.json\n\
        \        # dimension=None            → it will infer 384 automatically\n \
        \   )\n    print(f\"Built FAISS index with {store.index.ntotal} vectors of\
        \ dim {store.index.d}\")"
