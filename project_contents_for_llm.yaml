"project_summary":
  "project_name": |-
    onto_rag
  "readme_content": |
    # onto_rag
    A Retrieval Augmented Generation system for matching extracted entities to ontologies
  "key_files":
    ".gitignore": |-
      # Byte-compiled / cache
      __pycache__/
      *.py[cod]
      .pytest_cache/

      # Environment & secrets
      .env
      api_key.json
      project_contents_for_llm.yaml
      project_contents_for_llm.txt

      # VSCode, macOS
      .vscode/
      .DS_Store

      # Large generated data and ontology files
      /data/
      /ontologies/
      /models/
    "requirements.txt": |-
      # onto_rag/requirements.txt

      # Core Ontology Processing
      rdflib>=6.0.0

      # Lexical Indexing
      whoosh>=2.7.0

      # Embeddings & Semantic Search
      sentence-transformers>=2.2.0
      faiss-cpu>=1.7.0

      # Prompt Templating <-- ADD THIS SECTION
      Jinja2>=3.0.0

      # Utilities
      numpy>=1.21.0
      pandas>=1.3.0
      python-dotenv>=0.19.0
      tqdm>=4.60.0
      streamlit-aggrid

      # Testing (optional, but good for development)
      pytest>=6.2.0
    "src/config.py": "# src/config.py\nfrom pathlib import Path\nfrom dotenv import\
      \ load_dotenv\nfrom rdflib import Namespace\nfrom os import getenv\n\n# ---\
      \ Path Configuration (using pathlib) ---\n\n# Project Root Directory\nPROJECT_ROOT\
      \ = Path(__file__).resolve().parent.parent\n\n# Load environment variables from\
      \ .env file in the project root\nload_dotenv(PROJECT_ROOT / \".env\")\n\n# Data\
      \ Directory (for ontology dump, indexes, etc.)\nDATA_DIR = PROJECT_ROOT / \"\
      data\"\n\n# Ontologies Directory\nONTOLOGIES_DIR = PROJECT_ROOT / \"ontologies\"\
      \n\n# --- Ontology Configuration ---\nONTOLOGIES_CONFIG = {\n    'foodon': {\n\
      \        'path': ONTOLOGIES_DIR / \"foodon.owl\",\n        'prefix': 'FOODON:',\n\
      \        'dump_json_path': DATA_DIR / \"ontology_dump_foodon.json\",\n     \
      \   'enriched_docs_path': DATA_DIR / \"enriched_documents_foodon.json\",\n \
      \       'embeddings_path': DATA_DIR / \"embeddings_foodon.json\",\n        'whoosh_index_dir':\
      \ DATA_DIR / \"whoosh_index_foodon\",\n        'faiss_index_path': DATA_DIR\
      \ / \"faiss_index_foodon.bin\",\n        'faiss_metadata_path': DATA_DIR / \"\
      faiss_metadata_foodon.json\",\n    },\n    'chebi': {\n        'path': ONTOLOGIES_DIR\
      \ / \"chebi.owl\",\n        'prefix': 'CHEBI:',\n        'dump_json_path': DATA_DIR\
      \ / \"ontology_dump_chebi.json\",\n        'enriched_docs_path': DATA_DIR /\
      \ \"enriched_documents_chebi.json\",\n        'embeddings_path': DATA_DIR /\
      \ \"embeddings_chebi.json\",\n        'whoosh_index_dir': DATA_DIR / \"whoosh_index_chebi\"\
      ,\n        'faiss_index_path': DATA_DIR / \"faiss_index_chebi.bin\",\n     \
      \   'faiss_metadata_path': DATA_DIR / \"faiss_metadata_chebi.json\",\n    }\n\
      }\n# NOTE: The loop that created Whoosh directories has been removed.\n# The\
      \ script responsible for building the Whoosh index should create its own directory.\n\
      \n# --- Model Configuration ---\nEMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\
      \nRERANKER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\nEMBEDDING_BATCH_SIZE\
      \ = 32\nEMBEDDING_DEVICE = 'cpu'\n\n# --- Retrieval and Reranking Configuration\
      \ ---\nDEFAULT_K_LEXICAL = 20\nDEFAULT_K_VECTOR = 20\nDEFAULT_RERANK_K = DEFAULT_K_LEXICAL\
      \ + DEFAULT_K_VECTOR\n\n# --- Namespace Configuration ---\nRDFS_NS_STR = \"\
      http://www.w3.org/2000/01/rdf-schema#\"\nRDF_NS_STR = \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\
      \nOWL_NS_STR = \"http://www.w3.org/2002/07/owl#\"\nSKOS_NS_STR = \"http://www.w3.org/2004/02/skos/core#\"\
      \nOBO_NS_STR = \"http://purl.obolibrary.org/obo/\"\nOBOINOWL_NS_STR = \"http://www.geneontology.org/formats/oboInOwl#\"\
      \nIAO_NS_STR = \"http://purl.obolibrary.org/obo/IAO_\"\n\nRDFS_NS = Namespace(RDFS_NS_STR)\n\
      RDF_NS = Namespace(RDF_NS_STR)\nOWL_NS = Namespace(OWL_NS_STR)\nSKOS_NS = Namespace(SKOS_NS_STR)\n\
      OBO_NS = Namespace(OBO_NS_STR)\nOBOINOWL_NS = Namespace(OBOINOWL_NS_STR)\nIAO_NS\
      \ = Namespace(IAO_NS_STR)\n\n\n# Mapping of common relation URIs/CURIEs to human-readable\
      \ names and default prefixes, used by parse_ontology.py and potentially enrich_documents.py\n\
      RELATION_CONFIG = {\n    \"obo:BFO_0000050\": {\"label\": \"part of\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0001000\": {\"label\": \"derives from\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0002200\": {\"label\": \"has phenotype\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0002451\": {\"label\": \"has part\", \"prefix\": \"\
      obo\"},\n    \"obo:FOODON_0000246\": {\"label\": \"has ingredient\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000056\": {\"label\": \"participates in\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000057\": {\"label\": \"has participant\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000085\": {\"label\": \"functionally related to\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002090\": {\"label\": \"adjacent to\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002131\": {\"label\": \"overlaps\", \"\
      prefix\": \"obo\"},\n    \"obo:RO_0002150\": {\"label\": \"connected to\", \"\
      prefix\": \"obo\"},\n    \"obo:RO_0002202\": {\"label\": \"develops from\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002350\": {\"label\": \"member of\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002351\": {\"label\": \"has member\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002353\": {\"label\": \"output of\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002440\": {\"label\": \"causally related\
      \ to\", \"prefix\": \"obo\"},\n    \"obo:RO_0002450\": {\"label\": \"contains\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002500\": {\"label\": \"surrounds\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002573\": {\"label\": \"has quality\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002606\": {\"label\": \"is transformation\
      \ of\", \"prefix\": \"obo\"},\n    \"obo:RO_0002607\": {\"label\": \"has input\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0003000\": {\"label\": \"used in\", \"\
      prefix\": \"obo\"},\n    \"obo:FOODON_0000380\": {\"label\": \"has preparation\
      \ method\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000226\": {\"label\": \"\
      has food source\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000388\": {\"label\"\
      : \"has maturity state\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000286\"\
      : {\"label\": \"has packaging\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000240\"\
      : {\"label\": \"has preservation method\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000440\"\
      : {\"label\": \"has physical state\", \"prefix\": \"obo\"},\n    \"obo:ERO_0000039\"\
      : {\"label\": \"has nutrient\", \"prefix\": \"obo\"},\n    \"obo:ERO_0000589\"\
      : {\"label\": \"dietary context of\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C25277\"\
      : {\"label\": \"is allergen of\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C48670\"\
      : {\"label\": \"has active ingredient\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C64548\"\
      : {\"label\": \"part of human diet\", \"prefix\": \"obo\"},\n    \"obo:PATO_0000001\"\
      : {\"label\": \"has quality PATO\", \"prefix\": \"obo\"},\n}\n\nTARGET_RELATIONS_CURIES\
      \ = list(RELATION_CONFIG.keys())\n\n# CURIE Prefix Map for uri_to_curie and\
      \ curie_to_uri conversions\nCURIE_PREFIX_MAP = {\n    \"http://purl.obolibrary.org/obo/FOODON_\"\
      : \"FOODON\",\n    \"http://purl.obolibrary.org/obo/BFO_\": \"BFO\",\n    \"\
      http://purl.obolibrary.org/obo/RO_\": \"RO\",\n    \"http://purl.obolibrary.org/obo/ERO_\"\
      : \"ERO\",\n    \"http://purl.obolibrary.org/obo/NCIT_\": \"NCIT\",\n    \"\
      http://purl.obolibrary.org/obo/PATO_\": \"PATO\",\n    \"http://purl.obolibrary.org/obo/IAO_\"\
      : \"IAO\",\n    \"http://www.w3.org/2000/01/rdf-schema#\": \"rdfs\",\n    \"\
      http://www.w3.org/1999/02/22-rdf-syntax-ns#\": \"rdf\",\n    \"http://www.w3.org/2002/07/owl#\"\
      : \"owl\",\n    \"http://www.w3.org/2004/02/skos/core#\": \"skos\",\n    \"\
      http://www.geneontology.org/formats/oboInOwl#\": \"oboInOwl\",\n    \"http://purl.obolibrary.org/obo/HANCESTRO_\"\
      : \"HANCESTRO\",  \n    \"http://purl.obolibrary.org/obo/GAZ_\": \"GAZ\",\n\
      \    \"http://purl.obolibrary.org/obo/CHEBI_\": \"CHEBI\",\n    \"http://purl.obolibrary.org/obo/NCBITaxon_\"\
      : \"NCBITaxon\",\n    \"http://purl.obolibrary.org/obo/UBERON_\": \"UBERON\"\
      ,\n    \"http://purl.obolibrary.org/obo/ENVO_\": \"ENVO\",\n    \"http://purl.obolibrary.org/obo/HP_\"\
      : \"HP\",\n    \"http://purl.obolibrary.org/obo/GO_\": \"GO\",\n    \"http://purl.obolibrary.org/obo/\"\
      : \"obo\",\n}\n\n# LLM API Key (placeholders)\nGEMINI_API_KEY = getenv(\"GEMINI_API_KEY\"\
      )\n#GEMINI_SELECTOR_MODEL_NAME = \"gemini-1.5-flash-latest\"\nGEMINI_SELECTOR_MODEL_NAME\
      \ = \"gemini-2.5-flash-lite-preview-06-17\"\nGEMINI_SCORER_MODEL_NAME = \"gemini-2.5-flash-lite-preview-06-17\"\
      \n\nOLLAMA_SELECTOR_MODEL_NAME = 'llama3.1:8b'\nOLLAMA_SCORER_MODEL_NAME = 'llama3.1:8b'\n\
      # OPENAI_API_KEY = getenv.OPENAI_API_KEY\n\n# Path to the prompt template for\
      \ the selector\nSELECTOR_PROMPT_TEMPLATE_PATH = PROJECT_ROOT / \"prompts\" /\
      \ \"strict_selection_minimal.tpl\"\nCONFIDENCE_PROMPT_TEMPLATE_PATH = PROJECT_ROOT\
      \ / \"prompts\" / \"confidence_assessment.tpl\" \n\nPIPELINE = \"ollama\" #\
      \ \"gemini\" or \"ollama\" \n\n# Logging configuration\nLOG_LEVEL = \"INFO\"\
      \nLOG_FILE = PROJECT_ROOT / \"app.log\""
    "src/main.py": "# src/main.py\nimport argparse\nimport json\nimport sys\nfrom\
      \ pathlib import Path\n\n# --- Add project root to sys.path ---\nPROJECT_ROOT\
      \ = Path(__file__).resolve().parent.parent\nif str(PROJECT_ROOT) not in sys.path:\n\
      \    sys.path.insert(0, str(PROJECT_ROOT))\n\nfrom src.pipeline.pipeline_factory\
      \ import get_pipeline\nfrom src.config import DEFAULT_K_LEXICAL, DEFAULT_K_VECTOR,\
      \ PIPELINE\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"\
      Run the Onto-RAG pipeline with LLM selection.\")\n    parser.add_argument(\"\
      query\", type=str, help=\"The entity or text to search for (e.g., 'raw milk').\"\
      )\n    parser.add_argument(\"--lexical_k\", type=int, default=DEFAULT_K_LEXICAL,\
      \ help=f\"Initial candidates from lexical search (default: {DEFAULT_K_LEXICAL}).\"\
      )\n    parser.add_argument(\"--vector_k\", type=int, default=DEFAULT_K_VECTOR,\
      \ help=f\"Initial candidates from vector search (default: {DEFAULT_K_VECTOR}).\"\
      )\n    parser.add_argument(\"--top_n_rerank\", type=int, default=100, help=\"\
      Number of candidates to rerank and pass to the LLM (default: 10).\")\n    parser.add_argument(\"\
      --show_candidates\", action=\"store_true\", help=\"Show the list of candidates\
      \ provided to the LLM.\")\n    args = parser.parse_args()\n\n    pipeline =\
      \ None\n    try:\n        pipeline = get_pipeline(PIPELINE)\n        \n    \
      \    # --- MODIFIED: Unpack the tuple returned by pipeline.run ---\n       \
      \ result_tuple = pipeline.run(\n            query=args.query,\n            lexical_k=args.lexical_k,\n\
      \            vector_k=args.vector_k,\n            rerank_top_n=args.top_n_rerank\n\
      \        )\n        \n        # Handle case where pipeline returns None\n  \
      \      if not result_tuple:\n            final_result, candidates = None, []\n\
      \        else:\n            final_result, candidates = result_tuple\n\n    \
      \    # --- Print the final selection (no changes here) ---\n        print(\"\
      \\n--- Final LLM Selection ---\")\n        if not final_result:\n          \
      \  print(\"Could not determine a matching ontology term.\")\n        else:\n\
      \            print(f\"Query: '{args.query}'\")\n            print(\"---------------------------\"\
      )\n            print(f\"Chosen Term ID: {final_result.get('id', 'N/A')}\")\n\
      \            print(f\"Label:          {final_result.get('label', 'N/A')}\")\n\
      \            print(f\"Confidence:     {final_result.get('confidence_score',\
      \ 0.0):.1%}\")\n            print(f\"Definition:     {final_result.get('definition',\
      \ 'N/A')}\")\n            print(f\"Synonyms:       {'; '.join(final_result.get('synonyms',\
      \ [])) or 'None'}\")\n            print(\"\\nLLM Explanation:\")\n         \
      \   print(f\"  > {final_result.get('explanation', 'No explanation provided.')}\"\
      )\n        print(\"---------------------------\\n\")\n\n        # --- NEW: Print\
      \ the candidates if requested ---\n        if args.show_candidates and candidates:\n\
      \            print(f\"--- Top {len(candidates)} Candidates Provided to LLM ---\"\
      )\n            chosen_id = final_result.get('id') if final_result else None\n\
      \            \n            for i, candidate in enumerate(candidates):\n    \
      \            # Fetch full details for printing\n                details = pipeline.retriever.get_term_details(candidate.get('id'))\n\
      \                if not details: continue\n\n                marker = \"⭐️\"\
      \ if details.get('id') == chosen_id else \"  \"\n                rerank_score\
      \ = candidate.get('rerank_score')\n                score_str = f\"(Score: {rerank_score:.4f})\"\
      \ if rerank_score is not None else \"\"\n\n                print(f\"{i+1}. {marker}\
      \ {details.get('label', 'N/A')} `{details.get('id', 'N/A')}` {score_str}\")\n\
      \                definition = details.get('definition')\n                if\
      \ definition:\n                    print(f\"       Def: {definition[:150]}...\"\
      )  # Print first 150 chars of definition\n                else:\n          \
      \          print(f\"       Def: No definition available.\")\n\n            \
      \    if details.get('synonyms'):\n                    print(f\"       Syns:\
      \ {'; '.join(details.get('synonyms', []))}\")\n                print(\"-\" *\
      \ 20)\n            print(\"-------------------------------------------\\n\"\
      )\n        elif args.show_candidates:\n            print(\"--- No Candidates\
      \ to Display ---\")\n\n\n    except Exception as e:\n        print(f\"\\nAn\
      \ error occurred during the pipeline execution: {e}\", file=sys.stderr)\n  \
      \      import traceback\n        traceback.print_exc()\n    finally:\n     \
      \   if pipeline:\n            pipeline.close()\n\nif __name__ == \"__main__\"\
      :\n    main()"
  "directory_structure": |2-
        ├── .devcontainer/ (excluded)
        ├── .pytest_cache/ (excluded)
        ├── data/ (excluded)
        ├── docs/ (excluded)
        ├── models/ (excluded)
        ├── ontologies/ (excluded)
        ├── prompts/
        ├── scripts/
        ├── src/
        ├── .DS_Store (excluded)
        ├── .env (excluded)
        ├── .env.example
        ├── .gitignore
        ├── README.md (excluded)
        ├── evaluation_results_gemini_1.5-flash.json (excluded)
        ├── evaluation_results_llama.json (excluded)
        ├── export_files_contents.py (excluded)
        ├── project_contents_for_llm.yaml (excluded)
        ├── requirements.txt
        ├── results.txt
        ├── review_app.py
        ├── review_app_copy.py
        ├── review_mappings_app.py
        └── tempCodeRunnerFile.py (excluded)
        ├── .devcontainer/ (excluded)
            └── devcontainer.json (excluded)
        ├── .pytest_cache/ (excluded)
            ├── v/ (excluded)
            ├── .gitignore (excluded)
            ├── CACHEDIR.TAG (excluded)
            └── README.md (excluded)
            ├── v/ (excluded)
                └── cache/ (excluded)
                ├── cache/ (excluded)
                    ├── lastfailed (excluded)
                    ├── nodeids (excluded)
                    └── stepwise (excluded)
        ├── data/ (excluded)
            ├── outputs/ (excluded)
            ├── whoosh_index/ (excluded)
            ├── whoosh_index_chebi/ (excluded)
            ├── whoosh_index_foodon/ (excluded)
            ├── CafeteriaFCD_foodon.xml (excluded)
            ├── CafeteriaFCD_foodon_unique.xml (excluded)
            ├── CafeteriaFCD_snomedct.xml (excluded)
            ├── embeddings.json (excluded)
            ├── embeddings_chebi.json (excluded)
            ├── embeddings_foodon.json (excluded)
            ├── enriched_documents.json (excluded)
            ├── enriched_documents_chebi.json (excluded)
            ├── enriched_documents_foodon.json (excluded)
            ├── example_for_scoreing.txt (excluded)
            ├── faiss_index.bin (excluded)
            ├── faiss_index_chebi.bin (excluded)
            ├── faiss_index_foodon.bin (excluded)
            ├── faiss_metadata.json (excluded)
            ├── faiss_metadata_chebi.json (excluded)
            ├── faiss_metadata_foodon.json (excluded)
            ├── ontology_dump.json (excluded)
            ├── ontology_dump_chebi.json (excluded)
            ├── ontology_dump_foodon.json (excluded)
            ├── ontology_syns_dump.json (excluded)
            └── readable_evaluation_results.json (excluded)
            ├── outputs/ (excluded)
                ├── mapped_ingredients_output.json (excluded)
                └── parsed_ingredients_output.json (excluded)
            ├── whoosh_index/ (excluded)
                ├── MAIN_29x4gpdinh37pl3n.seg (excluded)
                ├── MAIN_WRITELOCK (excluded)
                └── _MAIN_1.toc (excluded)
            ├── whoosh_index_chebi/ (excluded)
                ├── MAIN_WRITELOCK (excluded)
                ├── MAIN_gc5ro7nrv510y84k.seg (excluded)
                └── _MAIN_1.toc (excluded)
            ├── whoosh_index_foodon/ (excluded)
                ├── MAIN_WRITELOCK (excluded)
                ├── MAIN_lyojtd7artupuv3p.seg (excluded)
                └── _MAIN_1.toc (excluded)
        ├── docs/ (excluded)
            ├── project_outline.txt (excluded)
            └── tasks.txt (excluded)
        ├── models/ (excluded)
            ├── food_bert_v3/ (excluded)
            └── .DS_Store (excluded)
            ├── food_bert_v3/ (excluded)
                ├── checkpoint-2000/ (excluded)
                ├── checkpoint-2280/ (excluded)
                ├── runs/ (excluded)
                ├── config.json (excluded)
                ├── model.safetensors (excluded)
                ├── special_tokens_map.json (excluded)
                ├── tokenizer_config.json (excluded)
                └── vocab.txt (excluded)
                ├── checkpoint-2000/ (excluded)
                    ├── config.json (excluded)
                    ├── model.safetensors (excluded)
                    ├── optimizer.pt (excluded)
                    ├── rng_state.pth (excluded)
                    ├── scheduler.pt (excluded)
                    ├── trainer_state.json (excluded)
                    └── training_args.bin (excluded)
                ├── checkpoint-2280/ (excluded)
                    ├── config.json (excluded)
                    ├── model.safetensors (excluded)
                    ├── optimizer.pt (excluded)
                    ├── rng_state.pth (excluded)
                    ├── scheduler.pt (excluded)
                    ├── trainer_state.json (excluded)
                    └── training_args.bin (excluded)
                ├── runs/ (excluded)
                    └── Mar18_19-24-31_Mac/ (excluded)
                    ├── Mar18_19-24-31_Mac/ (excluded)
                        └── events.out.tfevents.1742340271.Mac.12182.0 (excluded)
        ├── ontologies/ (excluded)
            ├── .DS_Store (excluded)
            ├── chebi.owl (excluded)
            └── foodon.owl (excluded)
        ├── prompts/
            ├── confidence_assessment.tpl
            ├── final_selection.tpl
            ├── final_selection2.tpl
            ├── final_selection_ingredients.tpl
            ├── parse_ontology.tpl
            ├── selection_with_confidence.tpl
            ├── strict_final_selection.tpl
            └── strict_selection_minimal.tpl
        ├── scripts/
            ├── find_namespaces.py
            ├── format_evaluation_results.py
            ├── rebuild_base.bash
            └── reduce_evaluation_set.py
        ├── src/
            ├── __pycache__/
            ├── confidence_scorers/
            ├── embeddings/
            ├── evaluation/
            ├── ingestion/
            ├── pipeline/
            ├── rag_selectors/
            ├── reranker/
            ├── retriever/
            ├── tests/
            ├── utils/
            ├── vector_store/
            ├── __init__.py
            ├── config.py
            ├── main.py
            ├── run_on_off.py
            └── tempCodeRunnerFile.py
            ├── __pycache__/
                ├── __init__.cpython-39.pyc (excluded)
                └── config.cpython-39.pyc (excluded)
            ├── confidence_scorers/
                ├── __pycache__/
                ├── base_confidence_scorer.py
                ├── gemini_confidence_scorer.py
                └── ollama_confidence_scorer.py
                ├── __pycache__/
                    ├── base_confidence_scorer.cpython-39.pyc (excluded)
                    └── gemini_confidence_scorer.cpython-39.pyc (excluded)
            ├── embeddings/
                ├── __pycache__/
                └── embed_docs.py
                ├── __pycache__/
                    └── embed_docs.cpython-39.pyc (excluded)
            ├── evaluation/
                ├── __pycache__/
                ├── evaluate_pipeline.py
                └── evaluate_retriever_recall.py
                ├── __pycache__/
                    └── evaluate_retriever_recall.cpython-39.pyc (excluded)
            ├── ingestion/
                ├── __pycache__/
                ├── __init__.py
                ├── build_lexical_index.py
                ├── enrich_documents.py
                └── parse_ontology.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    ├── build_lexical_index.cpython-39.pyc (excluded)
                    ├── enrich_documents.cpython-39.pyc (excluded)
                    └── parse_ontology.cpython-39.pyc (excluded)
            ├── pipeline/
                ├── __pycache__/
                ├── base_pipeline.py
                ├── gemini_pipeline.py
                ├── ollama_pipeline.py
                └── pipeline_factory.py
                ├── __pycache__/
                    ├── base_pipeline.cpython-39.pyc (excluded)
                    ├── gemini_pipeline.cpython-39.pyc (excluded)
                    ├── ollama_pipeline.cpython-39.pyc (excluded)
                    ├── pipeline.cpython-39.pyc (excluded)
                    └── pipeline_factory.cpython-39.pyc (excluded)
            ├── rag_selectors/
                ├── __pycache__/
                ├── __init__.py
                ├── base_selector.py
                ├── gemini_selector.py
                └── ollama_selector.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    ├── base_selector.cpython-39.pyc (excluded)
                    ├── gemini_selector.cpython-39.pyc (excluded)
                    └── ollama_selector.cpython-39.pyc (excluded)
            ├── reranker/
                ├── __pycache__/
                ├── __init__.py
                └── llm_reranker.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    └── llm_reranker.cpython-39.pyc (excluded)
            ├── retriever/
                ├── __pycache__/
                ├── __init__.py
                └── hybrid_retriever.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    └── hybrid_retriever.cpython-39.pyc (excluded)
            ├── tests/
                ├── __pycache__/
                ├── __init__.py
                ├── test_lexical_index.py
                └── test_parse_ontology.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    ├── test_lexical_index.cpython-39-pytest-8.3.5.pyc (excluded)
                    └── test_parse_ontology.cpython-39-pytest-8.3.5.pyc (excluded)
            ├── utils/
                ├── __pycache__/
                ├── __init__.py
                └── ontology_utils.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    └── ontology_utils.cpython-39.pyc (excluded)
            ├── vector_store/
                ├── __pycache__/
                ├── __init__.py
                └── faiss_store.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    └── faiss_store.cpython-39.pyc (excluded)
"file_contents":
  ".env.example": |
    # OpenAI & vector DB credentials
    OPENAI_API_KEY=your_openai_api_key_here
    PINECONE_API_KEY=your_pinecone_api_key_here
    PINECONE_ENV=your_pinecone_env
  ".gitignore": |-
    # Byte-compiled / cache
    __pycache__/
    *.py[cod]
    .pytest_cache/

    # Environment & secrets
    .env
    api_key.json
    project_contents_for_llm.yaml
    project_contents_for_llm.txt

    # VSCode, macOS
    .vscode/
    .DS_Store

    # Large generated data and ontology files
    /data/
    /ontologies/
    /models/
  "requirements.txt": |-
    # onto_rag/requirements.txt

    # Core Ontology Processing
    rdflib>=6.0.0

    # Lexical Indexing
    whoosh>=2.7.0

    # Embeddings & Semantic Search
    sentence-transformers>=2.2.0
    faiss-cpu>=1.7.0

    # Prompt Templating <-- ADD THIS SECTION
    Jinja2>=3.0.0

    # Utilities
    numpy>=1.21.0
    pandas>=1.3.0
    python-dotenv>=0.19.0
    tqdm>=4.60.0
    streamlit-aggrid

    # Testing (optional, but good for development)
    pytest>=6.2.0
  "results.txt": |-
    llama:
    2025-06-11 16:06:24,274 - INFO - --- Evaluation Complete ---
    2025-06-11 16:06:24,274 - INFO - Total entities evaluated: 948
    2025-06-11 16:06:24,274 - INFO - Retrieval Failures (no candidates): 0
    2025-06-11 16:06:24,274 - INFO - Selection Failures (LLM error): 0
    2025-06-11 16:06:24,274 - INFO - ---------------------------
    2025-06-11 16:06:24,274 - INFO - Valid attempts for selector: 948
    2025-06-11 16:06:24,274 - INFO - Correct selections (Hits): 386
    2025-06-11 16:06:24,275 - INFO - Accuracy: 0.4072 (386/948)
    2025-06-11 16:06:24,275 - INFO - Saving 562 incorrect selections to /Users/jandrole/projects/onto_rag/evaluation_results.json
    2025-06-11 16:06:24,301 - INFO - Evaluation finished.

    gemini:
    2025-06-16 10:43:04,962 - INFO - --- Evaluation Complete ---
    2025-06-16 10:43:04,962 - INFO - Total entities evaluated: 948
    2025-06-16 10:43:04,962 - INFO - Retrieval Failures (no candidates): 0
    2025-06-16 10:43:04,962 - INFO - Selection Failures (LLM error): 0
    2025-06-16 10:43:04,962 - INFO - ---------------------------
    2025-06-16 10:43:04,962 - INFO - Valid attempts for selector: 948
    2025-06-16 10:43:04,962 - INFO - Correct selections (Hits): 600
    2025-06-16 10:43:04,962 - INFO - Accuracy: 0.6329 (600/948)
    2025-06-16 10:43:04,962 - INFO - Saving 348 incorrect selections to /Users/jandrole/projects/onto_rag/evaluation_results.json
    2025-06-16 10:43:04,975 - INFO - Evaluation finished.
  "review_app.py": "import streamlit as st\nimport pandas as pd\nimport json\nfrom\
    \ pathlib import Path\nfrom typing import Union, List, Dict, Any\nfrom st_aggrid\
    \ import AgGrid, GridOptionsBuilder, JsCode\n\n# --- Configuration ---\nMAPPED_DATA_PATH\
    \ = Path(__file__).resolve().parent / \"data\" / \"outputs\" / \"mapped_ingredients_output.json\"\
    \n\n# --- Data Loading (Cached) ---\n@st.cache_data\ndef load_data(file_path:\
    \ Path) -> Union[Dict, None]:\n    \"\"\"Loads the mapped ingredients data from\
    \ the specified JSON file.\"\"\"\n    if not file_path.exists():\n        st.error(f\"\
    ❌ **File Not Found:** The mapping file at '{file_path}' does not exist.\")\n \
    \       st.info(\"Please ensure you have run the appropriate scripts to generate\
    \ the data.\")\n        return None\n    try:\n        with file_path.open('r',\
    \ encoding='utf-8') as f:\n            return json.load(f)\n    except (json.JSONDecodeError,\
    \ IOError) as e:\n        st.error(f\"❌ **Error Loading Data:** Could not read\
    \ or parse '{file_path}'. Reason: {e}\")\n        return None\n\n# --- UI Helper\
    \ Functions ---\ndef display_details_drawer(selected_row_data: Dict[str, Any]):\n\
    \    \"\"\"Renders the details panel for a selected ingredient mapping.\"\"\"\n\
    \    st.divider()\n    st.header(f\"Details for: `{selected_row_data['original_ingredient']}`\"\
    )\n\n    mapping = selected_row_data.get('mapping_result', {})\n    if not isinstance(mapping,\
    \ dict):\n        st.warning(\"No valid mapping data found for this entry.\")\n\
    \        return\n\n    st.subheader(f\"✅ Chosen Term: {mapping.get('label', 'N/A')}\"\
    )\n    st.markdown(f\"**CURIE:** `{mapping.get('id', 'N/A')}` \U0001F4D1\")\n\
    \    \n    if mapping.get('definition'):\n        st.markdown(f\"**ℹ️ Definition:**\
    \ {mapping.get('definition')}\")\n    \n    if mapping.get('synonyms'):\n    \
    \    st.markdown(f\"**Synonyms:** *{', '.join(mapping.get('synonyms'))}*\")\n\
    \    \n    st.divider()\n\n    st.subheader(\"Model Explanation\")\n    st.info(mapping.get('explanation',\
    \ 'No explanation provided.'))\n\n    st.subheader(\"Candidates Considered\")\n\
    \    candidates = selected_row_data.get('candidates', [])\n    if candidates:\n\
    \        sort_key = 'rerank_score' if any('rerank_score' in c for c in candidates)\
    \ else 'score'\n        \n        cand_df = pd.DataFrame(candidates)\n       \
    \ cand_df['Is Chosen'] = cand_df['id'].apply(lambda x: '⭐' if x == mapping.get('id')\
    \ else '')\n        \n        display_cols = {'Is Chosen': 'Chosen', 'label':\
    \ 'Label', 'id': 'CURIE', sort_key: 'Score', 'source_ontology': 'Source'}\n  \
    \      existing_cols = [col for col in display_cols.keys() if col in cand_df.columns]\n\
    \        cand_df_display = cand_df[existing_cols].rename(columns=display_cols)\n\
    \n        st.dataframe(cand_df_display, use_container_width=True, hide_index=True,\
    \ column_config={\"Score\": st.column_config.NumberColumn(format=\"%.3f\")})\n\
    \    else:\n        st.info(\"No other candidates were provided for this mapping.\"\
    )\n\n    with st.expander(\"Show Raw JSON Data\"):\n        st.json(selected_row_data)\n\
    \n# JsCode for cell styling in AgGrid\ncellsytle_jscode = JsCode(\"\"\"\nfunction(params)\
    \ {\n    function interpolateColor(color1, color2, factor) {\n        let result\
    \ = color1.slice();\n        for (let i = 0; i < 3; i++) {\n            result[i]\
    \ = Math.round(result[i] + factor * (color2[i] - color1[i]));\n        }\n   \
    \     return result;\n    }\n\n    let confidence = params.data.Confidence;\n\
    \    if (confidence === null || confidence === undefined) {\n        return {\
    \ backgroundColor: '#f0f0f0' };\n    }\n    \n    const red_pastel = [255, 224,\
    \ 224];\n    const green_pastel = [224, 255, 224];\n    \n    let color = interpolateColor(red_pastel,\
    \ green_pastel, confidence);\n    \n    return {\n        'backgroundColor': `rgb(${color[0]},\
    \ ${color[1]}, ${color[2]})`\n    };\n};\n\"\"\")\n\n# --- DataFrame Preparation\
    \ ---\n# *** CHANGE: Made this function robust against non-dict mapping_result\
    \ values ***\ndef prepare_dataframe(mapped_ingredients: List[Dict]) -> pd.DataFrame:\n\
    \    \"\"\"Transforms the list of mapping results into a DataFrame for AgGrid.\"\
    \"\"\n    records = []\n    for i, item in enumerate(mapped_ingredients):\n  \
    \      item['_id'] = i\n        mapping = item.get('mapping_result')\n\n     \
    \   # Check if the mapping is a valid dictionary with an ID.\n        # This prevents\
    \ errors if mapping_result is a string (e.g., \"UNMAPPED\") or None.\n       \
    \ if isinstance(mapping, dict) and mapping.get('id'):\n            term = mapping.get('label',\
    \ '⚠️ UNMAPPED')\n            ont_id = mapping.get('id', 'N/A')\n            confidence\
    \ = mapping.get('confidence_score', 0.0)\n            explanation = mapping.get('explanation',\
    \ 'No explanation provided.')\n        else:\n            # If not a valid mapping,\
    \ set all values to their \"unmapped\" defaults.\n            term = '⚠️ UNMAPPED'\n\
    \            ont_id = 'N/A'\n            confidence = 0.0\n            explanation\
    \ = 'No valid mapping result found for this ingredient.'\n\n        record = {\n\
    \            \"Token\": item['original_ingredient'],\n            \"Ontology Term\"\
    : term,\n            \"Ontology ID\": ont_id,\n            \"Confidence\": confidence,\n\
    \            \"Explanation\": explanation,\n            \"_id\": i\n        }\n\
    \        records.append(record)\n    \n    return pd.DataFrame(records)\n\n# ---\
    \ Main Application ---\nst.set_page_config(page_title=\"Mapping Reviewer\", page_icon=\"\
    \U0001F50E\", layout=\"wide\")\n\nst.title(\"\U0001F50E Ontology Mapping Review\"\
    )\nst.markdown(\"An interactive interface to review, validate, and correct model-generated\
    \ ontology mappings.\")\n\ndata = load_data(MAPPED_DATA_PATH)\n\nif data:\n  \
    \  if 'product_keys' not in st.session_state:\n        st.session_state.product_keys\
    \ = list(data.keys())\n        st.session_state.current_index = 0\n        \n\
    \    with st.sidebar:\n        st.header(\"Product Navigation\")\n        selected_product_key\
    \ = st.selectbox(\n            \"Select Product to Review:\",\n            options=st.session_state.product_keys,\n\
    \            index=st.session_state.current_index,\n            key=\"product_selector\"\
    \n        )\n        new_index = st.session_state.product_keys.index(selected_product_key)\n\
    \        if new_index != st.session_state.current_index:\n            st.session_state.current_index\
    \ = new_index\n            st.rerun()\n\n        col1, col2 = st.columns(2)\n\
    \        if col1.button(\"⬅️ Previous\", use_container_width=True, disabled=(st.session_state.current_index\
    \ == 0)):\n            st.session_state.current_index -= 1\n            st.rerun()\n\
    \        if col2.button(\"Next ➡️\", use_container_width=True, disabled=(st.session_state.current_index\
    \ >= len(st.session_state.product_keys) - 1)):\n            st.session_state.current_index\
    \ += 1\n            st.rerun()\n        \n        st.divider()\n        st.caption(f\"\
    Progress: Product {st.session_state.current_index + 1} of {len(st.session_state.product_keys)}\"\
    )\n        st.progress((st.session_state.current_index + 1) / len(st.session_state.product_keys))\n\
    \n    product_id = st.session_state.product_keys[st.session_state.current_index]\n\
    \    product_data = data[product_id]\n    mapped_ingredients_list = product_data.get('mapped_ingredients',\
    \ [])\n\n    st.header(f\"Reviewing Product: `{product_id}`\", divider=\"rainbow\"\
    )\n    \n    if mapped_ingredients_list:\n        df_for_metrics = prepare_dataframe(mapped_ingredients_list)\n\
    \        total_ingredients = len(df_for_metrics)\n        mapped_count = len(df_for_metrics[df_for_metrics['Confidence']\
    \ > 0])\n        avg_confidence = df_for_metrics['Confidence'][df_for_metrics['Confidence']\
    \ > 0].mean()\n        low_confidence_count = len(df_for_metrics[df_for_metrics['Confidence'].between(0.01,\
    \ 0.5, inclusive=\"right\")])\n\n        c1, c2, c3, c4 = st.columns(4)\n    \
    \    c1.metric(\"Total Tokens\", f\"{total_ingredients}\")\n        c2.metric(\"\
    Mapped\", f\"{mapped_count}/{total_ingredients}\")\n        c3.metric(\"Avg. Confidence\"\
    , f\"{avg_confidence:.2f}\" if pd.notna(avg_confidence) else \"N/A\")\n      \
    \  c4.metric(\"Low Confidence (<0.5)\", f\"{low_confidence_count}\")\n       \
    \ \n    full_ingredients = product_data.get(\"original_ingredients\", \"N/A\"\
    )\n    \n    st.markdown(\"**Original Ingredients String:**\")\n    st.code(full_ingredients,\
    \ language=None)\n    st.markdown(\"---\")\n    \n    st.subheader(\"Ingredient\
    \ Mappings\")\n    st.info(\"\U0001F3A8 Row color indicates confidence.  Hover\
    \ over 'Ontology Term' or 'ID' for the model's explanation. Click a row to see\
    \ full details below.\")\n    \n    if not mapped_ingredients_list:\n        st.warning(\"\
    No mapped ingredients found for this product.\")\n    else:\n        df_for_display\
    \ = prepare_dataframe(mapped_ingredients_list)\n\n        gb = GridOptionsBuilder.from_dataframe(df_for_display)\n\
    \        \n        gb.configure_column(\"Ontology Term\", tooltipField=\"Explanation\"\
    )\n        gb.configure_column(\"Ontology ID\", tooltipField=\"Explanation\")\n\
    \        gb.configure_default_column(cellStyle=cellsytle_jscode)\n        \n \
    \       gb.configure_selection(selection_mode=\"single\", use_checkbox=False)\n\
    \        gb.configure_grid_options(domLayout='normal')\n        \n        gb.configure_column(\"\
    _id\", hide=True)\n        gb.configure_column(\"Confidence\", hide=True)\n  \
    \      gb.configure_column(\"Explanation\", hide=True)\n        \n        grid_options\
    \ = gb.build()\n\n        grid_response = AgGrid(\n            df_for_display,\n\
    \            gridOptions=grid_options,\n            height=400,\n            width='100%',\n\
    \            fit_columns_on_grid_load=True,\n            allow_unsafe_jscode=True,\n\
    \            key=f\"aggrid_{product_id}\"\n        )\n\n        selected_row_data\
    \ = None\n        \n        if grid_response['selected_rows'] is not None and\
    \ not grid_response['selected_rows'].empty:\n            selected_id = grid_response['selected_rows'].iloc[0]['_id']\n\
    \            selected_row_data = next((item for item in mapped_ingredients_list\
    \ if item.get('_id') == selected_id), None)\n        elif mapped_ingredients_list:\n\
    \            selected_row_data = mapped_ingredients_list[0]\n\n        if selected_row_data:\n\
    \            display_details_drawer(selected_row_data)\n\nelse:\n    st.warning(\"\
    Could not load data. Please check the file path and format.\")"
  "review_app_copy.py": "import streamlit as st\nimport json\nfrom pathlib import\
    \ Path\n\n# --- Configuration ---\n# Set the path to the human-readable evaluation\
    \ results file.\n# This script assumes the file is in the 'data' subdirectory.\n\
    EVALUATION_FILE_PATH = Path(\"data\") / \"readable_evaluation_results.json\"\n\
    \n# --- Helper Functions ---\n\n@st.cache_data\ndef load_data(file_path: Path)\
    \ -> list:\n    \"\"\"\n    Loads the evaluation data from the specified JSON\
    \ file.\n    The @st.cache_data decorator ensures the data is loaded only once.\n\
    \    \"\"\"\n    if not file_path.exists():\n        st.error(f\"Error: Evaluation\
    \ file not found at '{file_path}'.\")\n        st.info(\"Please run the `scripts/format_evaluation_results.py`\
    \ script first to generate this file.\")\n        return None\n    with open(file_path,\
    \ 'r', encoding='utf-8') as f:\n        return json.load(f)\n\ndef display_term_details(term_data:\
    \ dict):\n    \"\"\"Renders the details of a single ontology term without a main\
    \ title.\"\"\"\n    if not term_data or not term_data.get(\"curie\"):\n      \
    \  st.warning(\"No data available for this term.\")\n        return\n\n    # Display\
    \ the label and CURIE\n    label = term_data.get('label', 'N/A')\n    curie =\
    \ term_data.get('curie', 'N/A')\n    st.markdown(f\"**{label}** (`{curie}`)\"\
    )\n\n    # Display the definition in an info box\n    definition = term_data.get('definition')\n\
    \    if definition:\n        st.info(f\"**Definition:** {definition}\")\n    else:\n\
    \        st.info(\"No definition provided.\")\n\n    # Display synonyms if they\
    \ exist\n    synonyms = term_data.get('synonyms', [])\n    if synonyms:\n    \
    \    st.markdown(f\"**Synonyms:** *{', '.join(synonyms)}*\")\n\ndef display_term(term_data:\
    \ dict, title: str):\n    \"\"\"Renders a single ontology term's details in a\
    \ structured format.\"\"\"\n    st.subheader(title)\n    display_term_details(term_data)\n\
    \n# --- Main Application Logic ---\n\n# Set the page configuration (title, icon,\
    \ layout)\nst.set_page_config(\n    page_title=\"Ontology Linking Review\",\n\
    \    page_icon=\"\U0001F9EA\",\n    layout=\"wide\"\n)\n\nst.title(\"\U0001F9EA\
    \ Ontology Linking Evaluation Review\")\nst.markdown(\"An interface for experts\
    \ to review the performance of the entity linking model.\")\n\n# Load the data\
    \ using the cached function\ndata = load_data(EVALUATION_FILE_PATH)\n\nif data:\n\
    \    # --- Session State for Navigation ---\n    # Initialize the session state\
    \ to keep track of the current item index\n    if 'current_index' not in st.session_state:\n\
    \        st.session_state.current_index = 0\n\n    # --- Navigation Controls ---\n\
    \    st.sidebar.header(\"Navigation\")\n    # Allow selection by query text for\
    \ easy lookup\n    query_list = [f\"{i+1}. {item['query']}\" for i, item in enumerate(data)]\n\
    \    selected_query = st.sidebar.selectbox(\"Select a Query to Review:\", query_list,\
    \ index=st.session_state.current_index)\n\n    # Update index based on selection\n\
    \    st.session_state.current_index = query_list.index(selected_query)\n\n   \
    \ col1, col2 = st.sidebar.columns(2)\n    # \"Previous\" button\n    if col1.button(\"\
    ⬅️ Previous\", use_container_width=True):\n        if st.session_state.current_index\
    \ > 0:\n            st.session_state.current_index -= 1\n            st.rerun()\n\
    \    # \"Next\" button\n    if col2.button(\"Next ➡️\", use_container_width=True):\n\
    \        if st.session_state.current_index < len(data) - 1:\n            st.session_state.current_index\
    \ += 1\n            st.rerun()\n\n    # --- Display the selected item ---\n  \
    \  item = data[st.session_state.current_index]\n    query = item.get(\"query\"\
    )\n\n    st.header(f\"Reviewing Query: \\\"{query}\\\"\", divider=\"rainbow\"\
    )\n\n    # --- CHANGE 1: Removed the Correct/Incorrect status message ---\n  \
    \  # The block checking item.get(\"is_correct\") was removed from here.\n\n  \
    \  # Use columns for a side-by-side comparison\n    left_col, right_col = st.columns(2)\n\
    \n    with left_col:\n        # Display the model's chosen term\n        display_term(item.get(\"\
    chosen_term\"), \"\U0001F916 Model's Choice\")\n        # Display the model's\
    \ reasoning in an expandable section\n        with st.expander(\"Show Model's\
    \ Explanation\"):\n            st.info(item.get(\"explanation\", \"No explanation\
    \ provided.\"))\n\n    with right_col:\n        # --- CHANGE 2: Display ALL ground\
    \ truth terms with full details ---\n        st.subheader(\"\U0001F3AF Ground\
    \ Truth(s)\")\n        ground_truth_terms = item.get(\"ground_truth_terms\", [])\n\
    \        \n        if not ground_truth_terms:\n             st.warning(\"No ground\
    \ truth terms provided for this query.\")\n        else:\n            for i, term\
    \ in enumerate(ground_truth_terms):\n                # Add a separator between\
    \ terms for clarity, but not before the first one\n                if i > 0:\n\
    \                    st.markdown(\"---\")\n                display_term_details(term)\n\
    \n\n    # --- Display the list of all candidates provided to the model ---\n \
    \   st.markdown(\"---\")\n    with st.expander(\"\U0001F575️‍♀️ View All Candidate\
    \ Terms Provided to Model\"):\n        st.markdown(\"This is the full list of\
    \ options the model had to choose from.\")\n\n        candidates = item.get(\"\
    candidate_terms_provided\", [])\n        chosen_curie = item.get(\"chosen_term\"\
    , {}).get(\"curie\")\n        ground_truth_curies = [gt.get(\"curie\") for gt\
    \ in item.get(\"ground_truth_terms\", [])]\n\n        if not candidates:\n   \
    \         st.info(\"No candidate terms were provided to the model for this query.\"\
    )\n        else:\n            for candidate in candidates:\n                label\
    \ = candidate.get('label', 'N/A')\n                curie = candidate.get('curie',\
    \ 'N/A')\n\n                # Highlight chosen and ground truth terms\n      \
    \          marker = \"\"\n                if curie == chosen_curie:\n        \
    \            marker += \"\U0001F916\"\n                if curie in ground_truth_curies:\n\
    \                    marker += \"\U0001F3AF\"\n\n                st.markdown(f\"\
    **{marker} {label}** (`{curie}`)\")\n                definition = candidate.get('definition')\n\
    \                if definition:\n                    st.text(f\"  - {definition[:200]}...\"\
    ) # Truncate long definitions\n                else:\n                    st.text(\"\
    \  - No definition.\")"
  "review_mappings_app.py": "import streamlit as st\nimport pandas as pd\nimport json\n\
    from pathlib import Path\nfrom typing import Union, List, Dict, Any\nfrom st_aggrid\
    \ import AgGrid, GridOptionsBuilder, JsCode\n\n# --- Configuration ---\nMAPPED_DATA_PATH\
    \ = Path(__file__).resolve().parent / \"data\" / \"outputs\" / \"mapped_ingredients_output.json\"\
    \n\n# --- Data Loading (Cached) ---\n@st.cache_data\ndef load_data(file_path:\
    \ Path) -> Union[Dict, None]:\n    \"\"\"Loads the mapped ingredients data from\
    \ the specified JSON file.\"\"\"\n    if not file_path.exists():\n        st.error(f\"\
    ❌ **File Not Found:** The mapping file at '{file_path}' does not exist.\")\n \
    \       st.info(\"Please ensure you have run the appropriate scripts to generate\
    \ the data.\")\n        return None\n    try:\n        with file_path.open('r',\
    \ encoding='utf-8') as f:\n            return json.load(f)\n    except (json.JSONDecodeError,\
    \ IOError) as e:\n        st.error(f\"❌ **Error Loading Data:** Could not read\
    \ or parse '{file_path}'. Reason: {e}\")\n        return None\n\n# --- UI Helper\
    \ Functions ---\ndef display_details_drawer(selected_row_data: Dict[str, Any]):\n\
    \    \"\"\"Renders the details panel for a selected ingredient mapping.\"\"\"\n\
    \    st.divider()\n    st.header(f\"Details for: `{selected_row_data['original_ingredient']}`\"\
    )\n\n    mapping = selected_row_data.get('mapping_result', {})\n    if not isinstance(mapping,\
    \ dict):\n        st.warning(\"No valid mapping data found for this entry.\")\n\
    \        return\n\n    st.subheader(f\"✅ Chosen Term: {mapping.get('label', 'N/A')}\"\
    )\n    st.markdown(f\"**CURIE:** `{mapping.get('id', 'N/A')}` \U0001F4D1\")\n\
    \    \n    # *** CHANGE: Replaced broken emoji with a more standard one ***\n\
    \    if mapping.get('definition'):\n        st.markdown(f\"**ℹ️ Definition:**\
    \ {mapping.get('definition')}\")\n    \n    if mapping.get('synonyms'):\n    \
    \    st.markdown(f\"**Synonyms:** *{', '.join(mapping.get('synonyms'))}*\")\n\
    \    \n    st.divider()\n\n    st.subheader(\"Model Explanation\")\n    st.info(mapping.get('explanation',\
    \ 'No explanation provided.'))\n\n    st.subheader(\"Candidates Considered\")\n\
    \    candidates = selected_row_data.get('candidates', [])\n    if candidates:\n\
    \        sort_key = 'rerank_score' if any('rerank_score' in c for c in candidates)\
    \ else 'score'\n        \n        cand_df = pd.DataFrame(candidates)\n       \
    \ cand_df['Is Chosen'] = cand_df['id'].apply(lambda x: '⭐' if x == mapping.get('id')\
    \ else '')\n        \n        display_cols = {'Is Chosen': 'Chosen', 'label':\
    \ 'Label', 'id': 'CURIE', sort_key: 'Score', 'source_ontology': 'Source'}\n  \
    \      existing_cols = [col for col in display_cols.keys() if col in cand_df.columns]\n\
    \        cand_df_display = cand_df[existing_cols].rename(columns=display_cols)\n\
    \n        st.dataframe(cand_df_display, use_container_width=True, hide_index=True,\
    \ column_config={\"Score\": st.column_config.NumberColumn(format=\"%.3f\")})\n\
    \    else:\n        st.info(\"No other candidates were provided for this mapping.\"\
    )\n\n    with st.expander(\"Show Raw JSON Data\"):\n        st.json(selected_row_data)\n\
    \n# JsCode for cell styling in AgGrid\ncellsytle_jscode = JsCode(\"\"\"\nfunction(params)\
    \ {\n    function interpolateColor(color1, color2, factor) {\n        let result\
    \ = color1.slice();\n        for (let i = 0; i < 3; i++) {\n            result[i]\
    \ = Math.round(result[i] + factor * (color2[i] - color1[i]));\n        }\n   \
    \     return result;\n    }\n\n    let confidence = params.data.Confidence;\n\
    \    if (confidence === null || confidence === undefined) {\n        return {\
    \ backgroundColor: '#f0f0f0' };\n    }\n    \n    const red_pastel = [255, 224,\
    \ 224];\n    const green_pastel = [224, 255, 224];\n    \n    let color = interpolateColor(red_pastel,\
    \ green_pastel, confidence);\n    \n    return {\n        'backgroundColor': `rgb(${color[0]},\
    \ ${color[1]}, ${color[2]})`\n    };\n};\n\"\"\")\n\n# --- DataFrame Preparation\
    \ ---\ndef prepare_dataframe(mapped_ingredients: List[Dict]) -> pd.DataFrame:\n\
    \    \"\"\"Transforms the list of mapping results into a DataFrame for AgGrid.\"\
    \"\"\n    records = []\n    for i, item in enumerate(mapped_ingredients):\n  \
    \      item['_id'] = i\n        mapping = item.get('mapping_result', {})\n   \
    \     is_mapped = isinstance(mapping, dict) and mapping.get('id')\n        \n\
    \        record = {\n            \"Token\": item['original_ingredient'],\n   \
    \         \"Ontology Term\": mapping.get('label', '⚠️ UNMAPPED') if is_mapped\
    \ else '⚠️ UNMAPPED',\n            \"Ontology ID\": mapping.get('id', 'N/A') if\
    \ is_mapped else 'N/A',\n            \"Confidence\": mapping.get('confidence_score',\
    \ 0.0) if is_mapped else 0.0,\n            \"Explanation\": mapping.get('explanation',\
    \ 'No explanation provided.'),\n            \"_id\": i\n        }\n        records.append(record)\n\
    \    \n    return pd.DataFrame(records)\n\n# --- Main Application ---\nst.set_page_config(page_title=\"\
    Mapping Reviewer\", page_icon=\"\U0001F50E\", layout=\"wide\")\n\nst.title(\"\U0001F50E\
    \ Ontology Mapping Review\")\nst.markdown(\"An interactive interface to review,\
    \ validate, and correct model-generated ontology mappings.\")\n\ndata = load_data(MAPPED_DATA_PATH)\n\
    \nif data:\n    if 'product_keys' not in st.session_state:\n        st.session_state.product_keys\
    \ = list(data.keys())\n        st.session_state.current_index = 0\n        \n\
    \    with st.sidebar:\n        st.header(\"Product Navigation\")\n        selected_product_key\
    \ = st.selectbox(\n            \"Select Product to Review:\",\n            options=st.session_state.product_keys,\n\
    \            index=st.session_state.current_index,\n            key=\"product_selector\"\
    \n        )\n        new_index = st.session_state.product_keys.index(selected_product_key)\n\
    \        if new_index != st.session_state.current_index:\n            st.session_state.current_index\
    \ = new_index\n            st.rerun()\n\n        col1, col2 = st.columns(2)\n\
    \        if col1.button(\"⬅️ Previous\", use_container_width=True, disabled=(st.session_state.current_index\
    \ == 0)):\n            st.session_state.current_index -= 1\n            st.rerun()\n\
    \        if col2.button(\"Next ➡️\", use_container_width=True, disabled=(st.session_state.current_index\
    \ >= len(st.session_state.product_keys) - 1)):\n            st.session_state.current_index\
    \ += 1\n            st.rerun()\n        \n        st.divider()\n        st.caption(f\"\
    Progress: Product {st.session_state.current_index + 1} of {len(st.session_state.product_keys)}\"\
    )\n        st.progress((st.session_state.current_index + 1) / len(st.session_state.product_keys))\n\
    \n    product_id = st.session_state.product_keys[st.session_state.current_index]\n\
    \    product_data = data[product_id]\n    mapped_ingredients_list = product_data.get('mapped_ingredients',\
    \ [])\n\n    st.header(f\"Reviewing Product: `{product_id}`\", divider=\"rainbow\"\
    )\n    \n    if mapped_ingredients_list:\n        df_for_metrics = prepare_dataframe(mapped_ingredients_list)\n\
    \        total_ingredients = len(df_for_metrics)\n        mapped_count = len(df_for_metrics[df_for_metrics['Confidence']\
    \ > 0])\n        avg_confidence = df_for_metrics['Confidence'][df_for_metrics['Confidence']\
    \ > 0].mean()\n        low_confidence_count = len(df_for_metrics[df_for_metrics['Confidence'].between(0.01,\
    \ 0.5, inclusive=\"right\")])\n\n        c1, c2, c3, c4 = st.columns(4)\n    \
    \    c1.metric(\"Total Tokens\", f\"{total_ingredients}\")\n        c2.metric(\"\
    Mapped\", f\"{mapped_count}/{total_ingredients}\")\n        c3.metric(\"Avg. Confidence\"\
    , f\"{avg_confidence:.2f}\" if pd.notna(avg_confidence) else \"N/A\")\n      \
    \  c4.metric(\"Low Confidence (<0.5)\", f\"{low_confidence_count}\")\n       \
    \ \n    full_ingredients = product_data.get(\"original_ingredients\", \"N/A\"\
    )\n    \n    # *** CHANGE: Used st.code for better visibility of the ingredients\
    \ string ***\n    st.markdown(\"**Original Ingredients String:**\")\n    st.code(full_ingredients,\
    \ language=None)\n    st.markdown(\"---\")\n    \n    st.subheader(\"Ingredient\
    \ Mappings\")\n    st.info(\"\U0001F3A8 Row color indicates confidence.  Hover\
    \ over 'Ontology Term' or 'ID' for the model's explanation. Click a row to see\
    \ full details below.\")\n    \n    if not mapped_ingredients_list:\n        st.warning(\"\
    No mapped ingredients found for this product.\")\n    else:\n        df_for_display\
    \ = prepare_dataframe(mapped_ingredients_list)\n\n        gb = GridOptionsBuilder.from_dataframe(df_for_display)\n\
    \        \n        gb.configure_column(\"Ontology Term\", tooltipField=\"Explanation\"\
    )\n        gb.configure_column(\"Ontology ID\", tooltipField=\"Explanation\")\n\
    \        gb.configure_default_column(cellStyle=cellsytle_jscode)\n        \n \
    \       gb.configure_selection(selection_mode=\"single\", use_checkbox=False)\n\
    \        gb.configure_grid_options(domLayout='normal')\n        \n        gb.configure_column(\"\
    _id\", hide=True)\n        gb.configure_column(\"Confidence\", hide=True)\n  \
    \      gb.configure_column(\"Explanation\", hide=True)\n        \n        grid_options\
    \ = gb.build()\n\n        grid_response = AgGrid(\n            df_for_display,\n\
    \            gridOptions=grid_options,\n            height=400,\n            width='100%',\n\
    \            fit_columns_on_grid_load=True,\n            allow_unsafe_jscode=True,\n\
    \            key=f\"aggrid_{product_id}\"\n        )\n\n        # --- Selection\
    \ Handling and Details Display for AgGrid ---\n        selected_row_data = None\n\
    \        \n        # *** CHANGE: New logic to default to the first row if nothing\
    \ is selected ***\n        # First, prioritize the user's click\n        if grid_response['selected_rows']\
    \ is not None and not grid_response['selected_rows'].empty:\n            selected_id\
    \ = grid_response['selected_rows'].iloc[0]['_id']\n            selected_row_data\
    \ = next((item for item in mapped_ingredients_list if item.get('_id') == selected_id),\
    \ None)\n        # If no row is clicked and the list isn't empty, default to the\
    \ first ingredient\n        elif mapped_ingredients_list:\n            selected_row_data\
    \ = mapped_ingredients_list[0]\n\n        # If we have data (either from a click\
    \ or the default), display the details\n        if selected_row_data:\n      \
    \      display_details_drawer(selected_row_data)\n\nelse:\n    st.warning(\"Could\
    \ not load data. Please check the file path and format.\")"
  "prompts":
    "confidence_assessment.tpl": |-
      You are a rigorous AI confidence assessor for ontology mapping. Your task is to evaluate a proposed match between a user entity and a chosen ontology term, and assign a confidence score based on a strict rubric.

      User Entity:
      [USER_ENTITY]

      Chosen Ontology Term:
      [CHOSEN_TERM_DETAILS]

      Other Top Candidates (for context):
      [OTHER_CANDIDATES]

      INSTRUCTIONS:

      1.  **Analyze and Score:** Compare the user entity against the chosen term's Label, Definition, and Synonyms. Use the other candidates for context on why this specific term was chosen over others.
      2.  **Apply the Rubric:** Use the following rubric to determine the confidence score. You must follow this hierarchy.
      3.  **Format Output:** Provide your response in a valid JSON format only. Do not add any text, comments, or markdown fences before or after the JSON block.

      ---
      ### **Confidence Score Rubric**

      *   **1.0 (Certain Match):** The user entity is an exact, case-insensitive match for the chosen term's `Label` or one of its `Synonyms`.
      *   **0.9 (High-Confidence Match):** The user entity is a well-known alternative name, abbreviation, or a normalized form (e.g., handles plurals, spacing) of the chosen term's `Label` or `Synonyms`.
      *   **0.6 (Plausible Match):** The user entity is a substring of the chosen term's label/synonym, or describes a very specific instance of it. The connection is strong and highly likely.
      *   **0.4 (Speculative Match):** The entities are only related by broad category or context. The explanation must state the speculative nature.
      *   **0.0 (No Confidence):** The match is incorrect or highly tenuous. For regulated substances, any difference in name or number (e.g., "Blue 1" vs. "Blue 2") means the match is wrong.

      ---
      **JSON OUTPUT FORMAT**:

      The JSON object must contain two keys:
      *   `"confidence_score"`: A float between 0.0 and 1.0, determined strictly by the rubric.
      *   `"explanation"`: A brief, clear explanation for the assigned score, referencing the rubric and justifying the choice. This may refine the initial explanation.

      **EXAMPLE**:
      *User Entity*: 'organic apple'
      *Chosen Term*: Label: 'Apple'
      *Generated JSON*:
      {
        "confidence_score": 0.6,
        "explanation": "Plausible Match. The entity 'organic apple' is a specific instance of the chosen term 'Apple'. The 'organic' quality is not captured in the ontology term, which prevents a higher score. Score based on the 0.6 rubric criteria."
      }
    "final_selection.tpl": |-
      You are an expert ontologist specializing in food science. Your task is to analyze a list of candidate ontology terms and select the single most appropriate term that matches the user's provided entity.

      **User Entity:**
      [USER_ENTITY]

      **Candidate Ontology Terms:**
      [CANDIDATE_LIST]

      **Instructions:**
      1.  Carefully review the user's entity and each candidate's details (ID, Label, Definition, Synonyms).
      2.  Select the single best match. Consider exact matches of labels or synonyms as strong signals. If there are multiple good matches, prefer the more specific term over a general one.
      3.  Provide your response in a valid JSON format only. Do not add any text before or after the JSON block.
      4.  The JSON object must contain two keys:
          - "chosen_id": The CURIE (ID) of the single best matching term (e.g., "FOODON:00001290").
          - "explanation": A brief, clear explanation for your choice, justifying why it is the best fit compared to other options.

      **Example Response Format:**
      {
        "chosen_id": "FOODON:00001290",
        "explanation": "I chose 'garlic' because its label is an exact match for the user entity. Candidate 'allium sativum' is the scientific name but 'garlic' is the common term and therefore a better fit."
      }
    "final_selection2.tpl": "SYSTEM:\nYou are an expert ontologist specializing in\
      \ food science.  \nYour objective is to map a given *user_entity* to one—and\
      \ only one—candidate term drawn from FoodOn.  \nOperate deterministically: set\
      \ **temperature = 0** (greedy decoding).  \nReturn **only** a valid JSON object\
      \ as output—no prose before or after.\n\nINPUT (exact JSON structure):\n{\n\
      \  \"user_entity\": \"<string>\",\n  \"candidate_terms\": [\n    {\n      \"\
      id\": \"<CURIE>\",\n      \"label\": \"<string>\",\n      \"definition\": \"\
      <string>\",\n      \"synonyms\": [\"<string>\", ...]\n    }\n    // … additional\
      \ candidates in the same shape\n  ]\n}\n\nTASK INSTRUCTIONS\n1. Compare *user_entity*\
      \ against each candidate term’s label, definition, and synonyms.\n2. Identify\
      \ the single best match.  \n   • Exact lexical matches in *label* or *synonyms*\
      \ are strong signals.  \n   • If multiple candidates are plausible, **prefer\
      \ the most specific term** over broader ones.\n3. Produce a JSON object with\
      \ exactly two keys:\n   • **\"chosen_id\"** – the CURIE of the selected term.\
      \  \n   • **\"explanation\"** – a concise, step-by-step justification of why\
      \ this term outranks the others (implicit chain-of-thought).\n\nOUTPUT FORMAT\
      \ (nothing else):\n{\n  \"chosen_id\": \"<CURIE>\",\n  \"explanation\": \"<brief\
      \ but complete reasoning>\"\n}\n\nEDGE-CASE EXAMPLE (for the model’s patterning\
      \ only):\nInput:\n{\n  \"user_entity\": \"apple\",\n  \"candidate_terms\": [\n\
      \    {\"id\":\"FOODON:00002403\",\"label\":\"apple (fruit)\",\"definition\"\
      :\"The edible fruit of Malus domestica.\",\"synonyms\":[\"apple fruit\",\"fresh\
      \ apple\"]},\n    {\"id\":\"FOODON:03311015\",\"label\":\"fruit (plant product)\"\
      ,\"definition\":\"A botanical fruit.\",\"synonyms\":[\"plant fruit\"]},\n  \
      \  {\"id\":\"FOODON:00002405\",\"label\":\"apple pie\",\"definition\":\"A pie\
      \ made with apples.\",\"synonyms\":[\"apple tart\"]}\n  ]\n}\n\nExpected Output:\n\
      {\n  \"chosen_id\": \"FOODON:00002403\",\n  \"explanation\": \"Exact label match\
      \ with 'apple (fruit)'. While 'fruit (plant product)' matches only generically\
      \ and 'apple pie' is a derivative product, 'apple (fruit)' is the most specific,\
      \ conceptually precise fit.\"\n}\n\n\n\n\n\n\nYou are an expert ontologist specializing\
      \ in food science.\nRespond deterministically (the caller will invoke the model\
      \ with temperature 0).\n\n────────────────────────────────────────────────────────\n\
      USER ENTITY\n[USER_ENTITY]\n\nCANDIDATE ONTOLOGY TERMS\n[As numbered list —\
      \ each item has “ID”, “Label”, “Definition”, “Synonyms”.]\n[CANDIDATE_LIST]\n\
      ────────────────────────────────────────────────────────\n\nTASK\n1. Examine\
      \ the user entity against each candidate’s **Label, Definition, and Synonyms**.\n\
      2. Pick the **single** best-matching term.\n   • Exact lexical matches in Label\
      \ or Synonyms are strong signals.  \n   • If several terms could work, choose\
      \ the **most specific** one.\n3. Output **only** a valid JSON object – nothing\
      \ before or after it.\n\nOUTPUT SHAPE\n{\n  \"chosen_id\": \"<CURIE>\",\n  \"\
      explanation\": \"<concise step-by-step rationale comparing the chosen term to\
      \ close alternatives>\"\n}\n\n\U0001F4CC Do not wrap the JSON in markdown fences.\
      \ Do not emit any other text.\n\nEXAMPLE (shows specificity rule; do NOT repeat\
      \ in your answer)\nInput block (abbreviated):\n  USER_ENTITY: apple\n  CANDIDATE_LIST:\n\
      \    1. ID: FOODON:00002403 … Label: apple (fruit) …\n    2. ID: FOODON:03311015\
      \ … Label: fruit (plant product) …\n    3. ID: FOODON:00002405 … Label: apple\
      \ pie …\nExpected JSON:\n{\n  \"chosen_id\": \"FOODON:00002403\",\n  \"explanation\"\
      : \"Exact label match with 'apple (fruit)'. 'Fruit (plant product)' is generic\
      \ and 'apple pie' is a derivative food; therefore 'apple (fruit)' is the most\
      \ specific fit.\"\n}\n"
    "final_selection_ingredients.tpl": |-
      You are an expert ontologist specializing in food, ingredient, and chemical substance classification. Your task is to analyze a user-provided entity and select the single most appropriate term from a list of candidate ontology terms, based on a strict set of rules.

      You will be given a user entity and a numbered list of candidate terms. Each candidate in the list includes an ID, Label, Definition, and a list of Synonyms.

      **User Entity:**
      [USER_ENTITY]

      **Candidate List:**
      [CANDIDATE_LIST]

      **Instructions:**
      1.  Carefully evaluate the User Entity against the Label, Synonyms, and Definition of each candidate in the Candidate List.
      2.  Select the single best match. An exact match between the User Entity and a candidate's Label or one of its Synonyms is the strongest signal for selection.
      3.  **Specificity Rule:** If multiple candidates are good matches, you must choose the most specific term over the more general one.
      4.  Your response must be a single, valid JSON object only. Do not add any text, explanations, or comments before or after the JSON block.

      **Output Format:**
      The JSON object you return must contain two keys:
      * `"chosen_id"`: The ID of the single best matching term.
      * `"explanation"`: A brief justification for your choice. This explanation must clarify why the chosen term is the best fit and, if relevant, why it was chosen over other plausible candidates by applying the specificity rule.

      **Example:**
      ---
      **User Entity:**
      citric acid

      **Candidate List:**
      1. ID: FOODON:03301503
         Label: acidulant
         Definition: A food additive which increases the acidity or enhances the sour taste of a food.
         Synonyms: food acid

      2. ID: CHEBI:30769
         Label: citric acid
         Definition: A tricarboxylic acid that is propane-1,2,3-tricarboxylic acid bearing a hydroxy substituent at position 2.
         Synonyms: 2-hydroxypropane-1,2,3-tricarboxylic acid

      3. ID: FOODON:03301072
         Label: lemon juice
         Definition: The juice obtained from lemons, a common source of citric acid.
         Synonyms: None
      ---

      Your Response:
      ```json
      {
        "chosen_id": "CHEBI:30769",
        "explanation": "I chose 'citric acid' because its label is an exact match for the user entity. While 'acidulant' describes its function, 'citric acid' is the specific chemical entity and therefore the most precise match, adhering to the specificity rule. 'lemon juice' is a product that contains the entity, not the entity itself."
      }
    "parse_ontology.tpl": ""
    "selection_with_confidence.tpl": |-
      You are an expert ontologist specializing in food, ingredient, and chemical substance classification. Your task is to analyze a user-provided entity and select the single most appropriate term from a list of candidate ontology terms, based on a strict set of rules.

      You will be given a user entity and a numbered list of candidate terms. Each candidate in the list includes an ID, Label, Definition, and a list of Synonyms.

      **User Entity**:
      [USER_ENTITY]

      **Candidate Ontology Terms**:
      [CANDIDATE_LIST]

      **INSTRUCTIONS**:

      Carefully review the user's entity and each candidate's details (ID, Label, Definition, Synonyms).
      Select the single best match. Consider exact matches of labels or synonyms as strong signals. If there are multiple good matches, prefer the more specific term over a general one.
      Provide your response in a valid JSON format only. Do not add any text, comments, or markdown fences before or after the JSON block.
      The JSON object must contain three keys:
      "chosen_id": The CURIE (ID) of the single best matching term (e.g., "FOODON:00001290").
      "confidence_score": A float between 0.0 and 1.0 indicating your confidence in the correct term being in the candidate list. If the candidate includes a similar and not exact chemical give low score.
      "explanation": A brief, clear explanation for your choice, justifying why it is the best fit. If confidence is low, explain the ambiguity.
      **EXAMPLE RESPONSE FORMAT**:

      Generated json
      {
        "chosen_id": "FOODON:00001290",
        "confidence_score": 1.0,
        "explanation": "The user entity 'garlic' is an exact match for the label of candidate FOODON:00001290. This is the most direct and specific match available."
      }
    "strict_final_selection.tpl": |-
      You are a rigorous and precise ontologist specializing in food, ingredient, and chemical substance classification. Your task is to analyze a user-provided entity and find the single most appropriate term from a list of candidate ontology terms. You must operate based on a strict, evidence-based rubric and avoid making speculative or purely semantic connections.

      You will be given a user entity and a numbered list of candidate terms. Each candidate in the list includes an ID, Label, Definition, and a list of Synonyms.

      **User Entity**:
      [USER_ENTITY]

      **Candidate Ontology Terms**:
      [CANDIDATE_LIST]

      **INSTRUCTIONS**:

      1.  **Analyze and Match:** Carefully compare the user entity against each candidate's Label and Synonyms list. Your goal is to find a match based on verifiable evidence.
      2.  **Apply the Rubric:** Use the following **Matching and Confidence Score Rubric** to determine the single best match and its corresponding confidence score. You must follow this hierarchy. Start at the top (1.0) and work your way down.
      3.  **Handle No Match:** If no candidate meets at least the criteria for a 0.4 confidence score, or if all potential matches have clear contradictory information, you must select "no match".
      4.  **Format Output:** Provide your response in a valid JSON format only. Do not add any text, comments, or markdown fences before or after the JSON block.

      ---
      ### **Matching and Confidence Score Rubric**

      *   **1.0 (Certain Match):**
          *   **Criteria:** The user entity is an exact, case-insensitive match for the candidate's `Label` or one of its `Synonyms`.
          *   **Example:** User entity `garlic` matches `Label: 'Garlic'`.

      *   **0.9 (High-Confidence Match):**
          *   **Criteria:** The user entity is a well-known alternative name, abbreviation, or a normalized form of the candidate's `Label` or `Synonyms` (e.g., handles plurals, spacing, or common initialisms like "MSG" for "monosodium glutamate").
          *   **Example:** User entity `powdered sugar` matches `Label: 'Icing Sugar'`.

      *   **0.6 (Plausible Match):**
          *   **Criteria:** The user entity is a substring of a label/synonym, or describes a very specific instance of the candidate, but is not a direct synonym. The connection is strong and highly likely, but not formally verified in the synonyms list.
          *   **Example:** User entity `organic apple` could plausibly match `Label: 'Apple'`. The explanation must note that the 'organic' quality is not captured in the ontology term.

      *   **0.4 (Speculative Match):**
          *   **Criteria:** The entities are only related by broad category or context, but there is no direct lexical overlap. **This should be used rarely.** The user entity and the candidate belong to the same specific class, but are clearly not the same thing.
          *   **Example:** User entity `lemonade` and a candidate `Label: 'Citrus Drink'`. The explanation must clearly state the speculative nature and the lack of a direct match.

      *   **NO MATCH (Confidence 0.0):**
          *   **Criteria:** No candidate meets the 0.4 criteria. Crucially, use this if the best potential match is a different, distinct entity within the same category.
          *   **Crucial Rule:** For chemicals, food colorings, or regulated substances, any difference in naming or numbering (e.g., "Blue 1" vs. "Blue 2") means they are **distinct entities and not a match**. Your example `blue 2 lake` vs. `copper(II) phthalocyanine` (Pigment Blue 15) is a classic case for **NO MATCH**.

      ---
      **JSON OUTPUT FORMAT**:

      The JSON object must contain three keys:
      *   `"chosen_id"`: The CURIE (ID) of the single best matching term. If no suitable match is found according to the rubric, this value **must be '-1'**.
      *   `"confidence_score"`: A float between 0.0 and 1.0, determined strictly by the rubric above. If `chosen_id` is `-1`, this **must be `0.0`**.
      *   `"explanation"`: A brief, clear explanation for the choice and score, referencing the rubric. If confidence is low or it's a "no match," explain the ambiguity or the reason for rejection.

      **EXAMPLE 1 (Perfect Match)**:
      *User Entity*: 'garlic'
      *Generated JSON*:

      {
        "chosen_id": "FOODON:00001290",
        "confidence_score": 1.0,
        "explanation": "The user entity 'garlic' is an exact match for the label of candidate FOODON:00001290, meeting the criteria for a 1.0 confidence score."
      }

      EXAMPLE 2 (No Match):
      User Entity: 'blue 2 lake'
      Candidate List containing CHEBI:155903 (copper(II) phthalocyanine)
      Generated JSON:
      {
        "chosen_id": "-1",
        "confidence_score": 0.0,
        "explanation": "No suitable match found. The user entity 'blue 2 lake' refers to FD&C Blue No. 2, a specific food dye. The closest candidate, CHEBI:155903 (copper(II) phthalocyanine), is a different chemical substance (Pigment Blue 15). Although both are blue pigments, they are distinct entities."
      }
    "strict_selection_minimal.tpl": |-
      You are a precise AI for food ontology mapping. Your task is to analyze a user entity and select the single best matching term from a list of candidates based on a strict rubric. Avoid speculation.

      User Entity:
      [USER_ENTITY]

      Candidate Ontology Terms:
      [CANDIDATE_LIST]

      Instructions & Rubric
      Follow this rubric hierarchically. If no candidate scores at least 0.4, it is a "no match".

      1.0 (Certain): Exact, case-insensitive match of the user entity to the candidate's Label or a Synonym.
      Example: User garlic -> Label Garlic.
      0.9 (High-Confidence): The user entity is a normalized form (plural, spacing) or a common abbreviation of the Label or a Synonym.
      Example: User powdered sugar -> Label Icing Sugar.
      0.6 (Plausible): The user entity is a specific instance or substring of the candidate. The connection is strong but not a direct synonym. The explanation must note what is not captured.
      Example: User organic apple -> Label Apple.
      0.4 (Speculative): Related by broad category only, with no direct lexical match. Use rarely.
      Example: User lemonade -> Label Citrus Drink.
      0.0 (No Match):
      Criteria: No candidate meets the 0.4 threshold.
      Crucial Rule: For regulated substances like chemicals or food dyes, any difference in name or number (e.g., "Blue 1" vs. "Blue 2") means they are distinct entities and NOT a match.
      Output Format
      Your response must be a valid JSON object only, with no other text or markdown.
      {
        "chosen_id": "string",
        "confidence_score": "float",
        "explanation": "string"
      }
      "chosen_id": The ID of the best match. Must be "-1" if no match.
      "confidence_score": The float score from the rubric. Must be 0.0 if no match.
      "explanation": A brief justification referencing the rubric. Explain why if there is no match or the confidence is low.
      Example Output (for 'blue 2 lake' vs a candidate for 'Pigment Blue 15')
      {
        "chosen_id": "-1",
        "confidence_score": 0.0,
        "explanation": "No match found. The user entity 'blue 2 lake' (FD&C Blue No. 2) and the candidate 'copper(II) phthalocyanine' (Pigment Blue 15) are distinct chemical substances, which is a non-negotiable mismatch according to the rubric."
      }
  "scripts":
    "find_namespaces.py": |-
      # scripts/find_namespaces.py
      import rdflib
      from rdflib import URIRef
      from collections import Counter
      import re
      import sys
      from pathlib import Path
      from typing import Optional

      project_root = Path(__file__).resolve().parent.parent
      if project_root not in sys.path:
          sys.path.insert(0, str(project_root))

      from src.config import ONTOLOGIES_CONFIG

      def get_base_uri(uri_str: str) -> Optional[str]:
          """
          Tries to extract a base URI from a full URI.
          e.g., http://purl.obolibrary.org/obo/FOODON_0000123 -> http://purl.obolibrary.org/obo/FOODON_
          e.g., http://www.w3.org/2000/01/rdf-schema#label -> http://www.w3.org/2000/01/rdf-schema#
          """
          if '#' in uri_str:
              return uri_str.rsplit('#', 1)[0] + '#'
          # Common OBO pattern: ends with an underscore followed by digits
          match_obo = re.match(r'(.+[_A-Z]+_)(\d+)$', uri_str)
          if match_obo:
              return match_obo.group(1)
          # General pattern: up to the last slash
          if '/' in uri_str:
              return uri_str.rsplit('/', 1)[0] + '/'
          return None

      def analyze_ontology(ontology_path: Path):
          """Loads a single ontology and prints a report of its namespaces."""
          print(f"\nLoading ontology from: {ontology_path}...")
          g = rdflib.Graph()
          try:
              # rdflib's parse method can handle Path objects directly
              g.parse(ontology_path) # format will be auto-detected
              print(f"Successfully parsed. Found {len(g)} triples.")
          except Exception as e:
              print(f"ERROR: Could not parse ontology: {e}")
              return

          uris = set()
          for s, p, o in g:
              if isinstance(s, URIRef): uris.add(str(s))
              if isinstance(p, URIRef): uris.add(str(p))
              if isinstance(o, URIRef): uris.add(str(o))

          print(f"Found {len(uris)} unique URIs.")

          base_uri_counts = Counter()
          for uri in uris:
              base = get_base_uri(uri)
              if base:
                  base_uri_counts[base] += 1

          print("\n--- Potential Base URIs (with usage counts) ---")
          if not base_uri_counts:
              print("No common base URIs found.")
          else:
              # Sort by count descending and print
              for base, count in base_uri_counts.most_common():
                  print(f"- \"{base}\": (used in {count} distinct URIs)")

          print("\n--- rdflib's known prefixes from file ---")
          namespaces = list(g.namespaces())
          if not namespaces:
              print("No prefixes were explicitly defined in the ontology file.")
          else:
              for prefix, namespace in namespaces:
                  print(f"  {prefix}: {namespace}")

      def main():
          """
          Loops through all ontologies defined in the project configuration
          and runs the namespace analysis on each one.
          """
          print("--- Finding Namespaces in All Configured Ontologies ---")

          if not ONTOLOGIES_CONFIG:
              print("No ontologies found in the ONTOLOGIES_CONFIG dictionary in your config file. Exiting.")
              return

          # Loop through each configured ontology
          for name, config_data in ONTOLOGIES_CONFIG.items():
              print(f"\n==================================================")
              print(f"Analyzing Ontology: '{name}'")
              print(f"==================================================")

              ontology_path = config_data.get('path')

              # Check if the path is defined in the config
              if not ontology_path:
                  print(f"WARNING: No 'path' key found in configuration for '{name}'. Skipping.")
                  continue

              # Check if the file actually exists using pathlib
              if not ontology_path.exists():
                  print(f"ERROR: Ontology file for '{name}' not found at the configured path: '{ontology_path}'. Skipping.")
                  continue

              # Call the analysis function for the valid path
              analyze_ontology(ontology_path)

          print("\n--- Analysis Complete ---")

      if __name__ == "__main__":
          main()
    "format_evaluation_results.py": "# scripts/format_evaluation_results.py\n\nimport\
      \ json\nimport sys\nimport argparse\nfrom pathlib import Path\nfrom typing import\
      \ Dict, Any, List, Optional\n\nproject_root = Path(__file__).resolve().parent.parent\n\
      if project_root not in sys.path:\n    sys.path.insert(0, str(project_root))\n\
      \ntry:\n    from src import config\nexcept ImportError:\n    print(\"Error:\
      \ Could not import from 'src'. Make sure the script is run from the project\
      \ root\"\n          \" or the project structure is correct.\")\n    # Provide\
      \ fallback paths if config fails to import\n    class config:\n        ONTOLOGY_DUMP_JSON\
      \ = project_root / \"data\" / \"ontology_dump.json\"\n\ndef load_json_data(file_path:\
      \ Path) -> Any:\n    \"\"\"Loads data from a JSON file.\"\"\"\n    if not file_path.exists():\n\
      \        print(f\"Error: File not found at {file_path}\")\n        sys.exit(1)\n\
      \    print(f\"Loading data from {file_path}...\")\n    with open(file_path,\
      \ 'r', encoding='utf-8') as f:\n        return json.load(f)\n\ndef get_enriched_details(curie:\
      \ str, ontology_data: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Retrieves\
      \ detailed information for a given CURIE from the ontology dump.\n    \n   \
      \ Args:\n        curie: The CURIE to look up (e.g., \"FOODON:03316347\").\n\
      \        ontology_data: The loaded ontology_dump.json data.\n\n    Returns:\n\
      \        A dictionary with enriched details.\n    \"\"\"\n    term_data = ontology_data.get(curie)\n\
      \    if not term_data:\n        return {\n            \"curie\": curie,\n  \
      \          \"label\": \"--- CURIE NOT FOUND ---\",\n            \"definition\"\
      : \"N/A\",\n            \"synonyms\": []\n        }\n    \n    return {\n  \
      \      \"curie\": curie,\n        \"label\": term_data.get(\"label\"),\n   \
      \     \"definition\": term_data.get(\"definition\"),\n        \"synonyms\":\
      \ term_data.get(\"synonyms\", [])\n    }\n\ndef process_evaluation_file(\n \
      \   eval_results: List[Dict[str, Any]],\n    ontology_data: Dict[str, Any]\n\
      ) -> List[Dict[str, Any]]:\n    \"\"\"\n    Processes the evaluation results\
      \ to create a human-readable version.\n    \"\"\"\n    print(\"Enriching evaluation\
      \ results...\")\n    enriched_output = []\n\n    for item in eval_results:\n\
      \        query = item.get(\"query\")\n        chosen_curie = item.get(\"chosen_curie\"\
      )\n        true_curies = item.get(\"true_curies\", [])\n        candidates_provided\
      \ = item.get(\"candidates_provided\", [])\n        \n        # Check if the\
      \ chosen CURIE is in the list of true CURIEs\n        is_correct = chosen_curie\
      \ in true_curies\n\n        # Enrich all relevant CURIEs\n        chosen_details\
      \ = get_enriched_details(chosen_curie, ontology_data)\n        true_details\
      \ = [get_enriched_details(tc, ontology_data) for tc in true_curies]\n      \
      \  candidates_details = [get_enriched_details(cc, ontology_data) for cc in candidates_provided]\n\
      \n        enriched_item = {\n            \"query\": query,\n            \"is_correct\"\
      : is_correct,\n            \"explanation\": item.get(\"explanation\"),\n   \
      \         \"chosen_term\": chosen_details,\n            \"ground_truth_terms\"\
      : true_details,\n            \"candidate_terms_provided\": candidates_details\n\
      \        }\n        enriched_output.append(enriched_item)\n        \n    return\
      \ enriched_output\n\ndef main():\n    \"\"\"Main function to run the script.\"\
      \"\"\n    parser = argparse.ArgumentParser(\n        description=\"Enrich LLM\
      \ evaluation results with ontology details for human review.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n\
      \    )\n    parser.add_argument(\n        \"--input-file\",\n        type=Path,\n\
      \        default=project_root / \"evaluation_results_gemini_1.5-flash.json\"\
      ,\n        help=\"Path to the raw evaluation results JSON file.\"\n    )\n \
      \   parser.add_argument(\n        \"--output-file\",\n        type=Path,\n \
      \       default=project_root / \"data\" / \"readable_evaluation_results.json\"\
      ,\n        help=\"Path to save the enriched, human-readable JSON file.\"\n \
      \   )\n    parser.add_argument(\n        \"--ontology-dump\",\n        type=Path,\n\
      \        default=Path(config.ONTOLOGY_DUMP_JSON),\n        help=\"Path to the\
      \ ontology_dump.json file.\"\n    )\n    args = parser.parse_args()\n\n    #\
      \ Create output directory if it doesn't exist\n    args.output_file.parent.mkdir(parents=True,\
      \ exist_ok=True)\n\n    # Load data\n    ontology_data = load_json_data(args.ontology_dump)\n\
      \    eval_results = load_json_data(args.input_file)\n\n    # Process and enrich\
      \ the data\n    readable_results = process_evaluation_file(eval_results, ontology_data)\n\
      \n    # Save the new file\n    print(f\"Saving enriched results to {args.output_file}...\"\
      )\n    with open(args.output_file, 'w', encoding='utf-8') as f:\n        json.dump(readable_results,\
      \ f, indent=2, ensure_ascii=False)\n    \n    print(\"Done!\")\n\nif __name__\
      \ == \"__main__\":\n    main()"
    "rebuild_base.bash": |-
      #!/usr/bin/env bash
      #
      # Rebuilds all data and indexes for the Onto-RAG pipeline.
      # Loops through all ontologies defined in src/config.py.
      #
      # Usage: ./scripts/rebuild_base.bash            # runs with defaults
      #        ./scripts/rebuild_base.bash --skip-eval   # skip final recall test
      #
      set -euo pipefail

      ROOT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )/.." && pwd )"
      cd "${ROOT_DIR}"

      DATA_DIR="data"

      echo "🧹  Cleaning old data and index artifacts..."
      # Use wildcards to clean all ontology-specific files and directories
      rm -f "${DATA_DIR}"/ontology_dump_*.json
      rm -f "${DATA_DIR}"/enriched_documents_*.json
      rm -f "${DATA_DIR}"/embeddings_*.json
      rm -f "${DATA_DIR}"/faiss_index_*.bin
      rm -f "${DATA_DIR}"/faiss_metadata_*.json
      rm -rf "${DATA_DIR}"/whoosh_index_*
      echo "✅  Cleanup complete."
      echo

      # Each script now handles looping internally based on src/config.py
      echo "①  Parsing all configured ontologies..."
      python -m src.ingestion.parse_ontology
      echo

      echo "②  Creating all enriched document files..."
      python -m src.ingestion.enrich_documents
      echo

      echo "③  Embedding all enriched document sets..."
      python -m src.embeddings.embed_docs
      echo

      echo "④  Building all FAISS vector stores..."
      python -m src.vector_store.faiss_store
      echo

      echo "⑤  Building all Whoosh lexical indexes..."
      python -m src.ingestion.build_lexical_index
      echo

      # The evaluation script may need updates to work with the new retriever
      if [[ "${1-}" != "--skip-eval" ]]; then
        echo "⑥  Running evaluator (Note: may need updates for multi-ontology setup)..."
        python -m src.evaluation.evaluate_retriever_recall
      fi

      echo "✅  Pipeline rebuild finished successfully."
    "reduce_evaluation_set.py": "# scripts/reduce_evaluation_set.py\nimport xml.etree.ElementTree\
      \ as ET\nimport sys\nfrom collections import defaultdict\nfrom pathlib import\
      \ Path\n\n# Add project root to Python path to allow direct imports if needed\
      \ in the future\nproject_root = Path(__file__).resolve().parent.parent\nif project_root\
      \ not in sys.path:\n    sys.path.insert(0, str(project_root))\n\n# --- Configuration\
      \ ---\n# Assumes the data folder is at the project root\nINPUT_XML_PATH = project_root\
      \ / \"data\" / \"CafeteriaFCD_foodon.xml\"\nOUTPUT_XML_PATH = project_root /\
      \ \"data\" / \"CafeteriaFCD_foodon_unique.xml\"\n\ndef create_unique_dataset(input_file:\
      \ Path, output_file: Path) -> None:\n    \"\"\"\n    Parses an XML annotation\
      \ file and creates a new, smaller XML file\n    containing only one instance\
      \ of each unique (text, semantic_tags) pair.\n    \"\"\"\n    if not input_file.exists():\n\
      \        print(f\"Error: Input file not found at {input_file}\")\n        return\n\
      \n    try:\n        tree = ET.parse(input_file)\n        root = tree.getroot()\n\
      \    except ET.ParseError as e:\n        print(f\"Error parsing XML file {input_file}:\
      \ {e}\")\n        return\n\n    # A set to store the unique identifiers of annotations\
      \ we've already added.\n    # The identifier will be a tuple: (UPPERCASE_TEXT,\
      \ sorted_tuple_of_tags)\n    seen_annotations = set()\n    \n    # These will\
      \ be the root and single document of our new XML file\n    new_root = ET.Element(\"\
      collection\")\n    new_doc = ET.SubElement(new_root, \"document\")\n    new_doc.set(\"\
      id\", \"unique_annotations_document\")\n\n    total_annotations_processed =\
      \ 0\n    unique_annotations_found = 0\n\n    # Find all annotation tags anywhere\
      \ in the document\n    for annotation_node in root.findall('.//annotation'):\n\
      \        total_annotations_processed += 1\n        \n        text_node = annotation_node.find('text')\n\
      \        tags_node = annotation_node.find('infon[@key=\"semantic_tags\"]')\n\
      \n        # Ensure both text and tags exist to form a valid entry\n        if\
      \ (text_node is not None and text_node.text and\n                tags_node is\
      \ not None and tags_node.text):\n            \n            # 1. Normalize the\
      \ text to be case-insensitive\n            text = text_node.text.strip().upper()\n\
      \n            # 2. Normalize the tags to be order-insensitive\n            raw_tags\
      \ = tags_node.text.strip()\n            # Split by ';', strip whitespace, filter\
      \ out any empty strings, and sort\n            sorted_tags = sorted([tag.strip()\
      \ for tag in raw_tags.split(';') if tag.strip()])\n\n            # Create a\
      \ unique, hashable key for this annotation\n            # We convert the list\
      \ of tags to a tuple to make it hashable for the set\n            annotation_key\
      \ = (text, tuple(sorted_tags))\n\n            # 3. If we haven't seen this unique\
      \ combination before, add it\n            if annotation_key not in seen_annotations:\n\
      \                seen_annotations.add(annotation_key)\n                # Append\
      \ the original annotation_node to our new document\n                new_doc.append(annotation_node)\n\
      \                unique_annotations_found += 1\n\n    print(f\"Processed {total_annotations_processed}\
      \ total annotations.\")\n    print(f\"Found {unique_annotations_found} unique\
      \ (text, tags) pairs.\")\n\n    # Write the new, smaller XML tree to the output\
      \ file\n    new_tree = ET.ElementTree(new_root)\n    # The indent function (Python\
      \ 3.9+) makes the XML output readable\n    if sys.version_info >= (3, 9):\n\
      \        ET.indent(new_tree)\n        \n    new_tree.write(output_file, encoding='utf-8',\
      \ xml_declaration=True)\n    print(f\"Successfully saved unique dataset to:\
      \ {output_file}\")\n\n\nif __name__ == \"__main__\":\n    create_unique_dataset(INPUT_XML_PATH,\
      \ OUTPUT_XML_PATH)"
  "src":
    "__init__.py": ""
    "config.py": "# src/config.py\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\
      from rdflib import Namespace\nfrom os import getenv\n\n# --- Path Configuration\
      \ (using pathlib) ---\n\n# Project Root Directory\nPROJECT_ROOT = Path(__file__).resolve().parent.parent\n\
      \n# Load environment variables from .env file in the project root\nload_dotenv(PROJECT_ROOT\
      \ / \".env\")\n\n# Data Directory (for ontology dump, indexes, etc.)\nDATA_DIR\
      \ = PROJECT_ROOT / \"data\"\n\n# Ontologies Directory\nONTOLOGIES_DIR = PROJECT_ROOT\
      \ / \"ontologies\"\n\n# --- Ontology Configuration ---\nONTOLOGIES_CONFIG =\
      \ {\n    'foodon': {\n        'path': ONTOLOGIES_DIR / \"foodon.owl\",\n   \
      \     'prefix': 'FOODON:',\n        'dump_json_path': DATA_DIR / \"ontology_dump_foodon.json\"\
      ,\n        'enriched_docs_path': DATA_DIR / \"enriched_documents_foodon.json\"\
      ,\n        'embeddings_path': DATA_DIR / \"embeddings_foodon.json\",\n     \
      \   'whoosh_index_dir': DATA_DIR / \"whoosh_index_foodon\",\n        'faiss_index_path':\
      \ DATA_DIR / \"faiss_index_foodon.bin\",\n        'faiss_metadata_path': DATA_DIR\
      \ / \"faiss_metadata_foodon.json\",\n    },\n    'chebi': {\n        'path':\
      \ ONTOLOGIES_DIR / \"chebi.owl\",\n        'prefix': 'CHEBI:',\n        'dump_json_path':\
      \ DATA_DIR / \"ontology_dump_chebi.json\",\n        'enriched_docs_path': DATA_DIR\
      \ / \"enriched_documents_chebi.json\",\n        'embeddings_path': DATA_DIR\
      \ / \"embeddings_chebi.json\",\n        'whoosh_index_dir': DATA_DIR / \"whoosh_index_chebi\"\
      ,\n        'faiss_index_path': DATA_DIR / \"faiss_index_chebi.bin\",\n     \
      \   'faiss_metadata_path': DATA_DIR / \"faiss_metadata_chebi.json\",\n    }\n\
      }\n# NOTE: The loop that created Whoosh directories has been removed.\n# The\
      \ script responsible for building the Whoosh index should create its own directory.\n\
      \n# --- Model Configuration ---\nEMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\
      \nRERANKER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\nEMBEDDING_BATCH_SIZE\
      \ = 32\nEMBEDDING_DEVICE = 'cpu'\n\n# --- Retrieval and Reranking Configuration\
      \ ---\nDEFAULT_K_LEXICAL = 20\nDEFAULT_K_VECTOR = 20\nDEFAULT_RERANK_K = DEFAULT_K_LEXICAL\
      \ + DEFAULT_K_VECTOR\n\n# --- Namespace Configuration ---\nRDFS_NS_STR = \"\
      http://www.w3.org/2000/01/rdf-schema#\"\nRDF_NS_STR = \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\
      \nOWL_NS_STR = \"http://www.w3.org/2002/07/owl#\"\nSKOS_NS_STR = \"http://www.w3.org/2004/02/skos/core#\"\
      \nOBO_NS_STR = \"http://purl.obolibrary.org/obo/\"\nOBOINOWL_NS_STR = \"http://www.geneontology.org/formats/oboInOwl#\"\
      \nIAO_NS_STR = \"http://purl.obolibrary.org/obo/IAO_\"\n\nRDFS_NS = Namespace(RDFS_NS_STR)\n\
      RDF_NS = Namespace(RDF_NS_STR)\nOWL_NS = Namespace(OWL_NS_STR)\nSKOS_NS = Namespace(SKOS_NS_STR)\n\
      OBO_NS = Namespace(OBO_NS_STR)\nOBOINOWL_NS = Namespace(OBOINOWL_NS_STR)\nIAO_NS\
      \ = Namespace(IAO_NS_STR)\n\n\n# Mapping of common relation URIs/CURIEs to human-readable\
      \ names and default prefixes, used by parse_ontology.py and potentially enrich_documents.py\n\
      RELATION_CONFIG = {\n    \"obo:BFO_0000050\": {\"label\": \"part of\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0001000\": {\"label\": \"derives from\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0002200\": {\"label\": \"has phenotype\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0002451\": {\"label\": \"has part\", \"prefix\": \"\
      obo\"},\n    \"obo:FOODON_0000246\": {\"label\": \"has ingredient\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000056\": {\"label\": \"participates in\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000057\": {\"label\": \"has participant\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000085\": {\"label\": \"functionally related to\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002090\": {\"label\": \"adjacent to\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002131\": {\"label\": \"overlaps\", \"\
      prefix\": \"obo\"},\n    \"obo:RO_0002150\": {\"label\": \"connected to\", \"\
      prefix\": \"obo\"},\n    \"obo:RO_0002202\": {\"label\": \"develops from\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002350\": {\"label\": \"member of\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002351\": {\"label\": \"has member\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002353\": {\"label\": \"output of\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002440\": {\"label\": \"causally related\
      \ to\", \"prefix\": \"obo\"},\n    \"obo:RO_0002450\": {\"label\": \"contains\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002500\": {\"label\": \"surrounds\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002573\": {\"label\": \"has quality\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002606\": {\"label\": \"is transformation\
      \ of\", \"prefix\": \"obo\"},\n    \"obo:RO_0002607\": {\"label\": \"has input\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0003000\": {\"label\": \"used in\", \"\
      prefix\": \"obo\"},\n    \"obo:FOODON_0000380\": {\"label\": \"has preparation\
      \ method\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000226\": {\"label\": \"\
      has food source\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000388\": {\"label\"\
      : \"has maturity state\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000286\"\
      : {\"label\": \"has packaging\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000240\"\
      : {\"label\": \"has preservation method\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000440\"\
      : {\"label\": \"has physical state\", \"prefix\": \"obo\"},\n    \"obo:ERO_0000039\"\
      : {\"label\": \"has nutrient\", \"prefix\": \"obo\"},\n    \"obo:ERO_0000589\"\
      : {\"label\": \"dietary context of\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C25277\"\
      : {\"label\": \"is allergen of\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C48670\"\
      : {\"label\": \"has active ingredient\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C64548\"\
      : {\"label\": \"part of human diet\", \"prefix\": \"obo\"},\n    \"obo:PATO_0000001\"\
      : {\"label\": \"has quality PATO\", \"prefix\": \"obo\"},\n}\n\nTARGET_RELATIONS_CURIES\
      \ = list(RELATION_CONFIG.keys())\n\n# CURIE Prefix Map for uri_to_curie and\
      \ curie_to_uri conversions\nCURIE_PREFIX_MAP = {\n    \"http://purl.obolibrary.org/obo/FOODON_\"\
      : \"FOODON\",\n    \"http://purl.obolibrary.org/obo/BFO_\": \"BFO\",\n    \"\
      http://purl.obolibrary.org/obo/RO_\": \"RO\",\n    \"http://purl.obolibrary.org/obo/ERO_\"\
      : \"ERO\",\n    \"http://purl.obolibrary.org/obo/NCIT_\": \"NCIT\",\n    \"\
      http://purl.obolibrary.org/obo/PATO_\": \"PATO\",\n    \"http://purl.obolibrary.org/obo/IAO_\"\
      : \"IAO\",\n    \"http://www.w3.org/2000/01/rdf-schema#\": \"rdfs\",\n    \"\
      http://www.w3.org/1999/02/22-rdf-syntax-ns#\": \"rdf\",\n    \"http://www.w3.org/2002/07/owl#\"\
      : \"owl\",\n    \"http://www.w3.org/2004/02/skos/core#\": \"skos\",\n    \"\
      http://www.geneontology.org/formats/oboInOwl#\": \"oboInOwl\",\n    \"http://purl.obolibrary.org/obo/HANCESTRO_\"\
      : \"HANCESTRO\",  \n    \"http://purl.obolibrary.org/obo/GAZ_\": \"GAZ\",\n\
      \    \"http://purl.obolibrary.org/obo/CHEBI_\": \"CHEBI\",\n    \"http://purl.obolibrary.org/obo/NCBITaxon_\"\
      : \"NCBITaxon\",\n    \"http://purl.obolibrary.org/obo/UBERON_\": \"UBERON\"\
      ,\n    \"http://purl.obolibrary.org/obo/ENVO_\": \"ENVO\",\n    \"http://purl.obolibrary.org/obo/HP_\"\
      : \"HP\",\n    \"http://purl.obolibrary.org/obo/GO_\": \"GO\",\n    \"http://purl.obolibrary.org/obo/\"\
      : \"obo\",\n}\n\n# LLM API Key (placeholders)\nGEMINI_API_KEY = getenv(\"GEMINI_API_KEY\"\
      )\n#GEMINI_SELECTOR_MODEL_NAME = \"gemini-1.5-flash-latest\"\nGEMINI_SELECTOR_MODEL_NAME\
      \ = \"gemini-2.5-flash-lite-preview-06-17\"\nGEMINI_SCORER_MODEL_NAME = \"gemini-2.5-flash-lite-preview-06-17\"\
      \n\nOLLAMA_SELECTOR_MODEL_NAME = 'llama3.1:8b'\nOLLAMA_SCORER_MODEL_NAME = 'llama3.1:8b'\n\
      # OPENAI_API_KEY = getenv.OPENAI_API_KEY\n\n# Path to the prompt template for\
      \ the selector\nSELECTOR_PROMPT_TEMPLATE_PATH = PROJECT_ROOT / \"prompts\" /\
      \ \"strict_selection_minimal.tpl\"\nCONFIDENCE_PROMPT_TEMPLATE_PATH = PROJECT_ROOT\
      \ / \"prompts\" / \"confidence_assessment.tpl\" \n\nPIPELINE = \"ollama\" #\
      \ \"gemini\" or \"ollama\" \n\n# Logging configuration\nLOG_LEVEL = \"INFO\"\
      \nLOG_FILE = PROJECT_ROOT / \"app.log\""
    "main.py": "# src/main.py\nimport argparse\nimport json\nimport sys\nfrom pathlib\
      \ import Path\n\n# --- Add project root to sys.path ---\nPROJECT_ROOT = Path(__file__).resolve().parent.parent\n\
      if str(PROJECT_ROOT) not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\n\
      \nfrom src.pipeline.pipeline_factory import get_pipeline\nfrom src.config import\
      \ DEFAULT_K_LEXICAL, DEFAULT_K_VECTOR, PIPELINE\n\ndef main():\n    parser =\
      \ argparse.ArgumentParser(description=\"Run the Onto-RAG pipeline with LLM selection.\"\
      )\n    parser.add_argument(\"query\", type=str, help=\"The entity or text to\
      \ search for (e.g., 'raw milk').\")\n    parser.add_argument(\"--lexical_k\"\
      , type=int, default=DEFAULT_K_LEXICAL, help=f\"Initial candidates from lexical\
      \ search (default: {DEFAULT_K_LEXICAL}).\")\n    parser.add_argument(\"--vector_k\"\
      , type=int, default=DEFAULT_K_VECTOR, help=f\"Initial candidates from vector\
      \ search (default: {DEFAULT_K_VECTOR}).\")\n    parser.add_argument(\"--top_n_rerank\"\
      , type=int, default=100, help=\"Number of candidates to rerank and pass to the\
      \ LLM (default: 10).\")\n    parser.add_argument(\"--show_candidates\", action=\"\
      store_true\", help=\"Show the list of candidates provided to the LLM.\")\n \
      \   args = parser.parse_args()\n\n    pipeline = None\n    try:\n        pipeline\
      \ = get_pipeline(PIPELINE)\n        \n        # --- MODIFIED: Unpack the tuple\
      \ returned by pipeline.run ---\n        result_tuple = pipeline.run(\n     \
      \       query=args.query,\n            lexical_k=args.lexical_k,\n         \
      \   vector_k=args.vector_k,\n            rerank_top_n=args.top_n_rerank\n  \
      \      )\n        \n        # Handle case where pipeline returns None\n    \
      \    if not result_tuple:\n            final_result, candidates = None, []\n\
      \        else:\n            final_result, candidates = result_tuple\n\n    \
      \    # --- Print the final selection (no changes here) ---\n        print(\"\
      \\n--- Final LLM Selection ---\")\n        if not final_result:\n          \
      \  print(\"Could not determine a matching ontology term.\")\n        else:\n\
      \            print(f\"Query: '{args.query}'\")\n            print(\"---------------------------\"\
      )\n            print(f\"Chosen Term ID: {final_result.get('id', 'N/A')}\")\n\
      \            print(f\"Label:          {final_result.get('label', 'N/A')}\")\n\
      \            print(f\"Confidence:     {final_result.get('confidence_score',\
      \ 0.0):.1%}\")\n            print(f\"Definition:     {final_result.get('definition',\
      \ 'N/A')}\")\n            print(f\"Synonyms:       {'; '.join(final_result.get('synonyms',\
      \ [])) or 'None'}\")\n            print(\"\\nLLM Explanation:\")\n         \
      \   print(f\"  > {final_result.get('explanation', 'No explanation provided.')}\"\
      )\n        print(\"---------------------------\\n\")\n\n        # --- NEW: Print\
      \ the candidates if requested ---\n        if args.show_candidates and candidates:\n\
      \            print(f\"--- Top {len(candidates)} Candidates Provided to LLM ---\"\
      )\n            chosen_id = final_result.get('id') if final_result else None\n\
      \            \n            for i, candidate in enumerate(candidates):\n    \
      \            # Fetch full details for printing\n                details = pipeline.retriever.get_term_details(candidate.get('id'))\n\
      \                if not details: continue\n\n                marker = \"⭐️\"\
      \ if details.get('id') == chosen_id else \"  \"\n                rerank_score\
      \ = candidate.get('rerank_score')\n                score_str = f\"(Score: {rerank_score:.4f})\"\
      \ if rerank_score is not None else \"\"\n\n                print(f\"{i+1}. {marker}\
      \ {details.get('label', 'N/A')} `{details.get('id', 'N/A')}` {score_str}\")\n\
      \                definition = details.get('definition')\n                if\
      \ definition:\n                    print(f\"       Def: {definition[:150]}...\"\
      )  # Print first 150 chars of definition\n                else:\n          \
      \          print(f\"       Def: No definition available.\")\n\n            \
      \    if details.get('synonyms'):\n                    print(f\"       Syns:\
      \ {'; '.join(details.get('synonyms', []))}\")\n                print(\"-\" *\
      \ 20)\n            print(\"-------------------------------------------\\n\"\
      )\n        elif args.show_candidates:\n            print(\"--- No Candidates\
      \ to Display ---\")\n\n\n    except Exception as e:\n        print(f\"\\nAn\
      \ error occurred during the pipeline execution: {e}\", file=sys.stderr)\n  \
      \      import traceback\n        traceback.print_exc()\n    finally:\n     \
      \   if pipeline:\n            pipeline.close()\n\nif __name__ == \"__main__\"\
      :\n    main()"
    "run_on_off.py": "# scripts/run_on_off.py\n\"\"\"\nScript to run the RAG-based\
      \ ontology mapping pipeline on a batch of products\nfrom an input file (e.g.,\
      \ parsed Open Food Facts data).\n\nIt processes each product's ingredients,\
      \ links them to ontology terms, and\nsaves the detailed structured results,\
      \ including candidates considered by the LLM,\nto an output JSON file. The final\
      \ mapping result is simplified to improve readability.\n\"\"\"\n\nimport sys\n\
      import json\nimport logging\nfrom tqdm import tqdm\nfrom pathlib import Path\n\
      from src.pipeline.pipeline_factory import get_pipeline\n\n# --- Add project\
      \ root to sys.path ---\nPROJECT_ROOT = Path(__file__).resolve().parent.parent\n\
      if str(PROJECT_ROOT) not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\n\
      \nfrom src import config\n\n# --- Configuration ---\nLOGGING_LEVEL = logging.INFO\n\
      PRODUCT_LIMIT = 100\n\n# --- Paths Configuration (using pathlib) ---\nDATA_DIR\
      \ = PROJECT_ROOT / 'data' / 'outputs'\nINPUT_FILE = DATA_DIR / 'parsed_ingredients_output.json'\n\
      OUTPUT_FILE = DATA_DIR / 'mapped_ingredients_output.json'\n\n\n# --- Setup Logging\
      \ ---\nlogging.basicConfig(level=LOGGING_LEVEL, format='%(asctime)s - %(levelname)s\
      \ - %(name)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n\n# ---\
      \ HELPER FUNCTION (Unchanged) ---\ndef simplify_mapping_result(result):\n  \
      \  \"\"\"\n    Removes verbose keys from a mapping result dictionary to keep\
      \ the output clean.\n    Returns the simplified dictionary or the original input\
      \ if it's not a dict.\n    \"\"\"\n    if not isinstance(result, dict):\n  \
      \      return result\n    keys_to_remove = {'ancestors', 'parents', 'relations'}\n\
      \    simplified_dict = {\n        key: value for key, value in result.items()\n\
      \        if key not in keys_to_remove\n    }\n    return simplified_dict\n\n\
      \ndef main():\n    \"\"\"\n    Main function to run the batch ingredient mapping\
      \ process.\n    \"\"\"\n    logger.info(\"Starting batch ontology mapping process...\"\
      )\n    logger.info(f\"Using pipeline: {config.PIPELINE}\")\n\n    OUTPUT_FILE.parent.mkdir(parents=True,\
      \ exist_ok=True)\n\n    pipeline = None\n    try:\n        # --- 1. Load Input\
      \ Data ---\n        logger.info(f\"Loading ingredients from: {INPUT_FILE}\"\
      )\n        if not INPUT_FILE.exists():\n            logger.error(f\"Input file\
      \ not found: {INPUT_FILE}\")\n            logger.error(\"Please ensure you have\
      \ run the parsing script first.\")\n            return\n\n        with INPUT_FILE.open('r',\
      \ encoding='utf-8') as f:\n            ## MODIFIED ##: Renamed variable to reflect\
      \ new data structure\n            all_product_data = json.load(f)\n\n      \
      \  # --- 2. Initialize RAG Pipeline ---\n        logger.info(\"Initializing\
      \ RAG pipeline...\")\n        pipeline = get_pipeline(config.PIPELINE)\n   \
      \     logger.info(\"RAG pipeline initialized successfully.\")\n\n        # ---\
      \ 3. Process Ingredients ---\n        all_mappings = {}\n\n        ## MODIFIED\
      \ ##: Use the new data variable\n        items_to_process = list(all_product_data.items())\n\
      \        if PRODUCT_LIMIT is not None:\n            items_to_process = items_to_process[:PRODUCT_LIMIT]\n\
      \            logger.warning(f\"Processing a limited set of {PRODUCT_LIMIT} products.\"\
      )\n\n        ## MODIFIED ##: Loop now unpacks the product data dictionary\n\
      \        for product_id, product_data in tqdm(items_to_process, desc=\"Processing\
      \ Products\"):\n            logger.debug(f\"--- Processing Product ID: {product_id}\
      \ ---\")\n\n            ## ADDED ##: Extract original and parsed ingredients\
      \ from the product data\n            original_ingredients_text = product_data.get(\"\
      original_ingredients\", \"\")\n            parsed_ingredients = product_data.get(\"\
      parsed_ingredients\", [])\n\n            mapped_ingredients = [] # Will store\
      \ the list of mapping results for this product\n            \n            #\
      \ Continue to process only unique ingredients to avoid redundant work\n    \
      \        unique_ingredients = sorted(list(set(parsed_ingredients)))\n\n    \
      \        for ingredient_query in unique_ingredients:\n                logger.debug(f\"\
      Querying for: '{ingredient_query}'\")\n\n                result_tuple = pipeline.run(query=ingredient_query)\n\
      \                mapping_result, candidates = (None, []) if not result_tuple\
      \ else result_tuple\n\n                simplified_mapping = simplify_mapping_result(mapping_result)\n\
      \n                if mapping_result:\n                    logger.info(f\"Query:\
      \ '{ingredient_query}' -> Mapped to: '{mapping_result.get('label')}' (ID: {mapping_result.get('id')})\"\
      )\n                else:\n                    logger.warning(f\"Query: '{ingredient_query}'\
      \ -> No mapping found\")\n\n                # Store the simplified result and\
      \ candidates in the final JSON structure\n                mapped_ingredients.append({\n\
      \                    \"original_ingredient\": ingredient_query,\n          \
      \          \"mapping_result\": simplified_mapping if simplified_mapping else\
      \ \"No mapping found\",\n                    \"candidates\": candidates if candidates\
      \ else []\n                })\n\n            ## MODIFIED ##: Structure the output\
      \ to include original ingredients text\n            all_mappings[product_id]\
      \ = {\n                \"original_ingredients\": original_ingredients_text,\n\
      \                \"mapped_ingredients\": mapped_ingredients\n            }\n\
      \n        # --- 4. Save Results ---\n        logger.info(f\"Saving mapped results\
      \ to: {OUTPUT_FILE}\")\n        with OUTPUT_FILE.open('w', encoding='utf-8')\
      \ as f:\n            json.dump(all_mappings, f, indent=4)\n\n        logger.info(\"\
      Mapping process completed successfully!\")\n\n    except Exception as e:\n \
      \       logger.error(f\"An unexpected error occurred during the mapping process:\
      \ {e}\", exc_info=True)\n    finally:\n        # --- 5. Clean up ---\n     \
      \   if pipeline:\n            logger.info(\"Closing pipeline resources.\")\n\
      \            pipeline.close()\n\nif __name__ == \"__main__\":\n    main()"
    "tempCodeRunnerFile.py": |-
      f
    "confidence_scorers":
      "base_confidence_scorer.py": "# src/confidence_scorers/base_confidence_scorer.py\n\
        import logging\nfrom abc import ABC, abstractmethod\nfrom typing import List,\
        \ Dict, Any, Optional\nimport json\n\nfrom src import config\n\nlogger = logging.getLogger(__name__)\n\
        \nclass BaseConfidenceScorer(ABC):\n    \"\"\"Abstract base class for LLM-based\
        \ confidence scorers.\"\"\"\n\n    def __init__(self, model_name: str):\n\
        \        self.model_name = model_name\n        self.prompt_template = self._load_prompt_template()\n\
        \        logger.info(f\"{self.__class__.__name__} initialized for model: {self.model_name}\"\
        )\n\n    def _load_prompt_template(self) -> str:\n        template_path =\
        \ config.CONFIDENCE_PROMPT_TEMPLATE_PATH\n        try:\n            with template_path.open('r',\
        \ encoding='utf-8') as f:\n                return f.read()\n        except\
        \ FileNotFoundError:\n            logger.error(f\"Confidence prompt template\
        \ not found at {template_path}\")\n            raise\n\n    def _format_term_details(self,\
        \ term_details: Dict[str, Any]) -> str:\n        label = term_details.get(\"\
        label\", \"N/A\")\n        definition = term_details.get(\"definition\", \"\
        No definition available.\")\n        synonyms = \"; \".join(term_details.get(\"\
        synonyms\", [])) or \"None\"\n        return f\"ID: {term_details.get('id',\
        \ 'N/A')}\\nLabel: {label}\\nDefinition: {definition}\\nSynonyms: {synonyms}\"\
        \n\n    def _format_other_candidates(self, candidates: List[Dict[str, Any]],\
        \ chosen_id: str, limit: int = 5) -> str:\n        other_candidates = [c for\
        \ c in candidates if c.get('id') != chosen_id][:limit]\n        if not other_candidates:\n\
        \            return \"None provided.\"\n        \n        formatted_list =\
        \ []\n        for cand in other_candidates:\n            formatted_list.append(f\"\
        - {cand.get('label', 'N/A')} (ID: {cand.get('id', 'N/A')})\")\n        return\
        \ \"\\n\".join(formatted_list)\n    \n    def _parse_response(self, response_text:\
        \ str) -> Optional[Dict[str, Any]]:\n        try:\n            cleaned_response\
        \ = response_text.strip().lstrip(\"```json\").rstrip(\"```\").strip()\n  \
        \          result = json.loads(cleaned_response)\n            \n         \
        \   if 'confidence_score' not in result or 'explanation' not in result:\n\
        \                logger.error(\"Confidence scorer response missing required\
        \ keys. Response: %s\", result)\n                return None\n           \
        \ \n            return {\n                'confidence_score': float(result['confidence_score']),\n\
        \                'explanation': str(result['explanation'])\n            }\n\
        \        except (json.JSONDecodeError, ValueError, TypeError) as e:\n    \
        \        logger.error(f\"Failed to decode or parse confidence scorer response:\
        \ {response_text}. Error: {e}\")\n            return None\n\n    @abstractmethod\n\
        \    def _call_llm(self, prompt: str) -> Optional[str]:\n        \"\"\"Makes\
        \ the actual API call to the specific LLM provider.\"\"\"\n        pass\n\
        \    \n    def score_confidence(self, query: str, chosen_term_details: Dict[str,\
        \ Any], all_candidates: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:\n\
        \        \"\"\"Formats the prompt, calls the LLM, and parses the response\
        \ for confidence scoring.\"\"\"\n        chosen_details_str = self._format_term_details(chosen_term_details)\n\
        \        other_candidates_str = self._format_other_candidates(all_candidates,\
        \ chosen_term_details.get('id', ''))\n\n        prompt = self.prompt_template.replace(\"\
        [USER_ENTITY]\", query)\n        prompt = prompt.replace(\"[CHOSEN_TERM_DETAILS]\"\
        , chosen_details_str)\n        prompt = prompt.replace(\"[OTHER_CANDIDATES]\"\
        , other_candidates_str)\n\n        response_text = self._call_llm(prompt)\n\
        \        if response_text is None:\n            return None\n        \n  \
        \      return self._parse_response(response_text)"
      "gemini_confidence_scorer.py": "# src/confidence_scorers/gemini_confidence_scorer.py\n\
        import logging\nfrom typing import Optional\n\nfrom google import genai\n\
        from google.api_core import exceptions\n\nfrom src.confidence_scorers.base_confidence_scorer\
        \ import BaseConfidenceScorer\nfrom src import config\n\nlogger = logging.getLogger(__name__)\n\
        \nclass GeminiConfidenceScorer(BaseConfidenceScorer):\n    \"\"\"Uses Google\
        \ Gemini to assess the confidence of an ontology mapping.\"\"\"\n    def __init__(self):\n\
        \        model_name = config.GEMINI_SCORER_MODEL_NAME # Use the same model\
        \ for now\n        super().__init__(model_name=model_name)\n        \n   \
        \     if not config.GEMINI_API_KEY:\n            raise ValueError(\"GEMINI_API_KEY\
        \ not found in environment variables.\")\n        \n        self.client =\
        \ genai.Client(api_key=config.GEMINI_API_KEY)\n\n    def _call_llm(self, prompt:\
        \ str) -> Optional[str]:\n        logger.info(f\"Sending confidence scoring\
        \ request to Gemini...\")\n        try:\n            generation_config = {'temperature':\
        \ 0}\n            response = self.client.models.generate_content(\n      \
        \          model=self.model_name,\n                contents=prompt,\n    \
        \            config=generation_config\n            )\n\n            feedback\
        \ = getattr(response, 'prompt_feedback', None)\n            if feedback and\
        \ any(r.blocked for r in feedback.safety_ratings or []):\n               \
        \ logger.warning(f\"Confidence scoring request was blocked by safety filters.\"\
        )\n                return None\n\n            return response.text\n     \
        \   except exceptions.GoogleAPIError as e:\n            logger.error(f\"A\
        \ Google API error occurred with the Gemini call for confidence scoring: {e}\"\
        , exc_info=True)\n            return None\n        except Exception as e:\n\
        \            logger.error(f\"An unexpected error occurred with the Gemini\
        \ confidence API call: {e}\", exc_info=True)\n            return None"
      "ollama_confidence_scorer.py": "# src/rag_selectors/ollama_confidence_scorer.py\n\
        import logging\nfrom typing import Optional\n\nimport ollama\n\nfrom src.confidence_scorers.base_confidence_scorer\
        \ import BaseConfidenceScorer\nfrom src import config\n\nlogger = logging.getLogger(__name__)\n\
        \nclass OllamaConfidenceScorer(BaseConfidenceScorer):\n    \"\"\"Uses a local\
        \ LLM via Ollama to assess the confidence of an ontology mapping.\"\"\"\n\
        \    \n    def __init__(self):\n        # Use a specific model for confidence\
        \ scoring if needed, or reuse the selector model\n        model_name = config.OLLAMA_SCORER_MODEL_NAME\n\
        \        super().__init__(model_name=model_name)\n        \n        try:\n\
        \            ollama.ps()\n            logger.info(\"Ollama service is running\
        \ for confidence scorer.\")\n        except Exception as exc:\n          \
        \  logger.error(\"Ollama service not detected. Please ensure Ollama is running.\"\
        )\n            raise ConnectionError(\"Ollama service not available.\") from\
        \ exc\n\n    def _call_llm(self, prompt: str) -> Optional[str]:\n        logger.info(f\"\
        Sending confidence scoring request to Ollama with model '{self.model_name}'\"\
        )\n        try:\n            response = ollama.chat(\n                model=self.model_name,\n\
        \                messages=[\n                    {\n                     \
        \   'role': 'user',\n                        'content': prompt,\n        \
        \            },\n                ],\n                format='json',\n    \
        \            options={'temperature': 0.0}\n            )\n            return\
        \ response['message']['content']\n        except ollama.ResponseError as e:\n\
        \            logger.error(f\"An error occurred with the Ollama API call for\
        \ confidence scoring: {e.status_code} - {e.error}\")\n            return None\n\
        \        except Exception as e:\n            logger.error(f\"An unexpected\
        \ error occurred during the Ollama confidence scoring call: {e}\", exc_info=True)\n\
        \            return None"
    "embeddings":
      "embed_docs.py": "# src/embeddings/embed_docs.py\nimport json\nimport os\nimport\
        \ logging\nimport time\nfrom typing import List, Dict, Any\n\n# Ensure src\
        \ is in path for imports if run directly\nimport sys\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..')))\n\ntry:\n    from sentence_transformers import SentenceTransformer\n\
        \    import torch \nexcept ModuleNotFoundError:\n    logging.error(\"Modules\
        \ sentence_transformers or torch not found. Please install them: pip install\
        \ sentence-transformers torch\")\n    sys.exit(1)\n\n# Changed: Use central\
        \ config and specific constants\nfrom src.config import ONTOLOGIES_CONFIG,\
        \ EMBEDDING_MODEL_NAME, EMBEDDING_BATCH_SIZE, EMBEDDING_DEVICE\n\nlogging.basicConfig(level=logging.INFO,\
        \ format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef load_enriched_documents(file_path:\
        \ str) -> List[Dict[str, Any]]:\n    \"\"\"Loads enriched documents from a\
        \ JSON file.\"\"\"\n    logging.info(f\"Loading enriched documents from {file_path}...\"\
        )\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n \
        \           documents = json.load(f)\n        logging.info(f\"Successfully\
        \ loaded {len(documents)} documents.\")\n        return documents\n    except\
        \ FileNotFoundError:\n        logging.error(f\"Error: Enriched documents file\
        \ not found at {file_path}\")\n        return []\n    except json.JSONDecodeError:\n\
        \        logging.error(f\"Error: Could not decode JSON from {file_path}\"\
        )\n        return []\n\ndef batch_embed_documents(\n    documents: List[Dict[str,\
        \ Any]], \n    model: SentenceTransformer,\n    batch_size: int = 32\n) ->\
        \ List[Dict[str, Any]]:\n    \"\"\"\n    Generates embeddings for a list of\
        \ documents using a pre-loaded SentenceTransformer model.\n\n    Args:\n \
        \       documents: A list of dictionaries, each with \"id\", \"label\", and\
        \ \"text\" keys.\n        model: The pre-loaded SentenceTransformer model\
        \ instance.\n        batch_size: The number of documents to process in each\
        \ batch.\n\n    Returns:\n        A list of dictionaries, each containing\
        \ \"id\", \"label\", and \"embedding\".\n    \"\"\"\n    if not documents:\n\
        \        logging.warning(\"No documents provided for embedding.\")\n     \
        \   return []\n\n    logging.info(\"Starting embedding process...\")\n   \
        \ texts_to_embed = [doc.get(\"text\", \"\") for doc in documents]\n    ids\
        \ = [doc.get(\"id\") for doc in documents]\n    labels = [doc.get(\"label\"\
        ) for doc in documents]\n\n    all_embeddings_data = []\n    start_time =\
        \ time.time()\n\n    for i in range(0, len(texts_to_embed), batch_size):\n\
        \        batch_texts = texts_to_embed[i:i+batch_size]\n        batch_ids =\
        \ ids[i:i+batch_size]\n        batch_labels = labels[i:i+batch_size]\n\n \
        \       logging.info(f\"  Processing batch {i//batch_size + 1}/{(len(texts_to_embed)\
        \ - 1)//batch_size + 1} (size: {len(batch_texts)})\")\n        \n        try:\n\
        \            embeddings = model.encode(batch_texts, convert_to_tensor=False,\
        \ show_progress_bar=False)\n            \n            for idx, embedding in\
        \ enumerate(embeddings):\n                all_embeddings_data.append({\n \
        \                   \"id\": batch_ids[idx],\n                    \"label\"\
        : batch_labels[idx],\n                    \"embedding\": embedding.tolist()\n\
        \                })\n        except Exception as e:\n            logging.error(f\"\
        Error embedding batch starting at index {i}: {e}\")\n\n    end_time = time.time()\n\
        \    logging.info(f\"Embedding process completed for {len(all_embeddings_data)}\
        \ documents in {end_time - start_time:.2f} seconds.\")\n    return all_embeddings_data\n\
        \ndef save_embeddings(embeddings_data: List[Dict[str, Any]], output_path:\
        \ str):\n    \"\"\"Saves the embeddings data to a JSON file.\"\"\"\n    if\
        \ not embeddings_data:\n        logging.warning(\"No embeddings data to save.\"\
        )\n        return\n\n    # Ensure output directory exists\n    os.makedirs(os.path.dirname(output_path),\
        \ exist_ok=True)\n    \n    logging.info(f\"Saving {len(embeddings_data)}\
        \ embeddings to {output_path}...\")\n    try:\n        with open(output_path,\
        \ 'w', encoding='utf-8') as f:\n            json.dump(embeddings_data, f)\
        \ # No indent for smaller file size\n        logging.info(\"Successfully saved\
        \ embeddings.\")\n    except IOError as e:\n        logging.error(f\"Error:\
        \ Could not write embeddings to {output_path}: {e}\")\n    except TypeError\
        \ as e:\n        logging.error(f\"TypeError during JSON serialization: {e}.\
        \ Check embedding data format.\")\n\ndef main():\n    \"\"\"Main function\
        \ to generate and save document embeddings for all configured ontologies.\"\
        \"\"\n    logging.info(\"--- Starting Embedding Generation for All Ontologies\
        \ ---\")\n\n    # 1. Load the model once to be reused for all ontologies\n\
        \    logging.info(f\"Loading SentenceTransformer model: {EMBEDDING_MODEL_NAME}\"\
        )\n    try:\n        model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=EMBEDDING_DEVICE,\
        \ trust_remote_code=True)\n        logging.info(f\"Model loaded successfully\
        \ on device: '{EMBEDDING_DEVICE}'\")\n        if hasattr(model, 'max_seq_length'):\n\
        \             logging.info(f\"Model max sequence length: {model.max_seq_length}\"\
        )\n    except Exception as e:\n        logging.error(f\"Fatal error loading\
        \ model: {e}\")\n        return\n\n    # 2. Loop over all configured ontologies\n\
        \    for name, config_data in ONTOLOGIES_CONFIG.items():\n        enriched_docs_path\
        \ = config_data.get('enriched_docs_path')\n        embeddings_output_path\
        \ = config_data.get('embeddings_path')\n        \n        logging.info(f\"\
        \\n--- Processing Ontology: {name} ---\")\n\n        if not enriched_docs_path\
        \ or not embeddings_output_path:\n            logging.warning(f\"Config for\
        \ '{name}' is missing 'enriched_docs_path' or 'embeddings_path'. Skipping.\"\
        )\n            continue\n        \n        logging.info(f\"Input: {enriched_docs_path}\"\
        )\n        logging.info(f\"Output: {embeddings_output_path}\")\n\n       \
        \ enriched_docs = load_enriched_documents(enriched_docs_path)\n        if\
        \ not enriched_docs:\n            logging.error(f\"Failed to load or empty\
        \ enriched documents file. Skipping '{name}'.\")\n            continue\n\n\
        \        embeddings_data = batch_embed_documents(\n            documents=enriched_docs,\n\
        \            model=model, # Pass the single, pre-loaded model\n          \
        \  batch_size=EMBEDDING_BATCH_SIZE\n        )\n\n        if embeddings_data:\n\
        \            save_embeddings(embeddings_data, embeddings_output_path)\n  \
        \          logging.info(f\"Generated and saved {len(embeddings_data)} embeddings\
        \ for '{name}'.\")\n        else:\n            logging.warning(f\"No embeddings\
        \ were generated for '{name}'.\")\n    \n    logging.info(\"\\n--- All Embedding\
        \ Generation Complete ---\")\n\nif __name__ == \"__main__\":\n    main()"
    "evaluation":
      "evaluate_pipeline.py": "# src/evaluation/evaluate_pipeline.py\n\nimport xml.etree.ElementTree\
        \ as ET\nimport os\nimport sys\nimport logging\nimport json\nfrom typing import\
        \ Tuple, List, Dict\n\n# Add project root to Python path to allow direct imports\
        \ from src\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..'))\nsys.path.append(PROJECT_ROOT)\n\nfrom src.retriever.hybrid_retriever\
        \ import HybridRetriever\nfrom src.rag_selectors.ollama_selector import OllamaSelector\
        \ # Import the new selector\nfrom src.rag_selectors.gemini_selector import\
        \ GeminiSelector # Import the old selector for reference\n\nfrom src.config\
        \ import (\n    ONTOLOGY_DUMP_JSON,\n    WHOOSH_INDEX_DIR,\n    FAISS_INDEX_PATH,\n\
        \    FAISS_METADATA_PATH,\n    EMBEDDING_MODEL_NAME,\n    CURIE_PREFIX_MAP,\n\
        \    DEFAULT_K_LEXICAL,\n    DEFAULT_K_VECTOR,\n    OLLAMA_SELECTOR_MODEL_NAME\
        \ # Import the model name\n)\nfrom src.utils.ontology_utils import uri_to_curie\n\
        \n# --- Configuration for this specific evaluation script ---\nEVALUATION_XML_FILE\
        \ = os.path.join(PROJECT_ROOT, \"data\", \"CafeteriaFCD_foodon_unique.xml\"\
        )\n# File to save detailed results of incorrect selections\nEVALUATION_OUTPUT_FILE\
        \ = os.path.join(PROJECT_ROOT, \"evaluation_results.json\")\n\n# --- Logging\
        \ Setup ---\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s -\
        \ %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n\
        # --- Re-used from evaluate_retriever_recall.py (no changes needed) ---\n\
        def parse_evaluation_xml(xml_file_path: str) -> list:\n    \"\"\"\n    Parses\
        \ the evaluation XML file to extract entities and their ground truth semantic\
        \ tags.\n    (This function is identical to the one in evaluate_retriever_recall.py)\n\
        \    \"\"\"\n    if not os.path.exists(xml_file_path):\n        logger.error(f\"\
        Evaluation XML file not found: {xml_file_path}\")\n        return []\n\n \
        \   gold_standard_data = []\n    try:\n        tree = ET.parse(xml_file_path)\n\
        \        root = tree.getroot()\n        for doc_idx, document_node in enumerate(root.findall('.//document')):\n\
        \            doc_id_node = document_node.find('id')\n            doc_id =\
        \ doc_id_node.text if doc_id_node is not None else f\"doc_{doc_idx}\"\n  \
        \          \n            for ann_idx, annotation_node in enumerate(document_node.findall('annotation')):\n\
        \                entity_text_node = annotation_node.find('text')\n       \
        \         semantic_tags_node = annotation_node.find('infon[@key=\"semantic_tags\"\
        ]')\n                ann_id_val = annotation_node.get('id', f\"ann_{doc_idx}_{ann_idx}\"\
        )\n\n                if entity_text_node is not None and semantic_tags_node\
        \ is not None and entity_text_node.text is not None and semantic_tags_node.text\
        \ is not None:\n                    entity_text = entity_text_node.text.strip()\n\
        \                    raw_tags = semantic_tags_node.text.strip()\n        \
        \            true_uris = {tag.strip() for tag in raw_tags.split(';') if tag.strip()}\n\
        \                    true_curies = {uri_to_curie(uri, CURIE_PREFIX_MAP) for\
        \ uri in true_uris} - {None}\n                    \n                    if\
        \ entity_text and true_curies:\n                        gold_standard_data.append({\n\
        \                            'text': entity_text,\n                      \
        \      'true_curies': true_curies,\n                            'doc_id':\
        \ doc_id,\n                            'ann_id': ann_id_val\n            \
        \            })\n    except ET.ParseError as e:\n        logger.error(f\"\
        Error parsing XML file {xml_file_path}: {e}\")\n        return []\n    \n\
        \    logger.info(f\"Successfully parsed {len(gold_standard_data)} entities\
        \ from {xml_file_path}\")\n    return gold_standard_data\n\n# --- New Evaluation\
        \ Logic for the Full Pipeline ---\ndef evaluate_full_pipeline(\n    retriever:\
        \ HybridRetriever, \n    selector: GeminiSelector,  #\n    gold_standard_data:\
        \ list, \n    lexical_k: int, \n    vector_k: int\n) -> Tuple[float, int,\
        \ int, int, int, List[Dict]]:\n    \"\"\"\n    Evaluates the full retrieval\
        \ and selection pipeline against the gold standard data.\n\n    Returns:\n\
        \        A tuple containing:\n        - accuracy_score (float)\n        -\
        \ total_entities_processed (int)\n        - correct_selections (int)\n   \
        \     - retrieval_failures (int): Count of queries that returned no candidates.\n\
        \        - selection_failures (int): Count of queries where the selector failed.\n\
        \        - incorrect_selections (list): A list of dictionaries detailing the\
        \ misses.\n    \"\"\"\n    total_entities_processed = 0\n    correct_selections\
        \ = 0\n    retrieval_failures = 0\n    selection_failures = 0\n    incorrect_selections\
        \ = []\n\n    if not gold_standard_data:\n        logger.warning(\"No gold\
        \ standard data provided for evaluation.\")\n        return 0.0, 0, 0, 0,\
        \ 0, []\n\n    for i, item in enumerate(gold_standard_data):\n        query_text\
        \ = item['text']\n        true_curies = item['true_curies']\n        \n  \
        \      total_entities_processed += 1\n        logger.info(f\"--- Processing\
        \ ({i+1}/{len(gold_standard_data)}): '{query_text}' (True: {true_curies})\
        \ ---\")\n\n        # 1. RETRIEVAL STEP: Get candidates\n        try:\n  \
        \          # Note: The HybridRetriever's search method should return a combined,\
        \ reranked list of candidates.\n            # We adapt to the provided retriever's\
        \ output format.\n            retriever_output_dict = retriever.search(\n\
        \                query_string=query_text, lexical_limit=lexical_k, vector_k=vector_k\n\
        \            )\n            lexical_results = retriever_output_dict.get(\"\
        lexical_results\", [])\n            vector_results = retriever_output_dict.get(\"\
        vector_results\", [])\n\n            # Combine and deduplicate\n         \
        \   candidates = []\n            seen_ids = set()\n            for doc in\
        \ lexical_results + vector_results:\n                doc_id = doc.get('id')\n\
        \                if doc_id and doc_id not in seen_ids:\n                 \
        \   candidates.append(doc)\n                    seen_ids.add(doc_id)\n   \
        \         \n            if not candidates:\n                logger.warning(f\"\
        Retrieval Failure: No candidates found for '{query_text}'.\")\n          \
        \      retrieval_failures += 1\n                continue\n\n        except\
        \ Exception as e:\n            logger.error(f\"Error during retrieval for\
        \ query '{query_text}': {e}\", exc_info=True)\n            retrieval_failures\
        \ += 1\n            continue\n\n        # 2. SELECTION STEP: Use OllamaSelector\n\
        \        try:\n            selection_result = selector.select_best_term(query=query_text,\
        \ candidates=candidates)\n\n            if not selection_result or \"chosen_id\"\
        \ not in selection_result:\n                logger.warning(f\"Selection Failure:\
        \ Selector did not return a valid choice for '{query_text}'.\")\n        \
        \        selection_failures += 1\n                continue\n            \n\
        \            chosen_curie = selection_result[\"chosen_id\"]\n\n        except\
        \ Exception as e:\n            logger.error(f\"Error during selection for\
        \ query '{query_text}': {e}\", exc_info=True)\n            selection_failures\
        \ += 1\n            continue\n        \n        # 3. COMPARISON STEP\n   \
        \     if chosen_curie in true_curies:\n            correct_selections += 1\n\
        \            logger.info(f\"✅ HIT! Query: '{query_text}'. Chosen: '{chosen_curie}'.\
        \ Correct.\")\n        else:\n            logger.info(f\"❌ MISS! Query: '{query_text}'.\
        \ Chosen: '{chosen_curie}', Expected: {true_curies}.\")\n            incorrect_selections.append({\n\
        \                \"query\": query_text,\n                \"chosen_curie\"\
        : chosen_curie,\n                \"true_curies\": list(true_curies),\n   \
        \             \"explanation\": selection_result.get(\"explanation\", \"N/A\"\
        ),\n                \"candidates_provided\": [cand['id'] for cand in candidates]\n\
        \            })\n\n    if (total_entities_processed - retrieval_failures -\
        \ selection_failures) == 0:\n        accuracy_score = 0.0\n    else:\n   \
        \     # Accuracy is based on the number of times the selector could make a\
        \ choice\n        accuracy_score = correct_selections / (total_entities_processed\
        \ - retrieval_failures - selection_failures)\n\n    return accuracy_score,\
        \ total_entities_processed, correct_selections, retrieval_failures, selection_failures,\
        \ incorrect_selections\n\ndef main():\n    logger.info(\"Starting Full Pipeline\
        \ Evaluation Script...\")\n    \n    # 1. Check for necessary model name in\
        \ config\n    if not OLLAMA_SELECTOR_MODEL_NAME:\n        logger.error(\"\
        OLLAMA_SELECTOR_MODEL_NAME is not set in config.py. Exiting.\")\n        return\n\
        \n    # 2. Parse Gold Standard XML\n    logger.info(f\"Loading gold standard\
        \ data from: {EVALUATION_XML_FILE}\")\n    gold_standard_data = parse_evaluation_xml(EVALUATION_XML_FILE)\n\
        \    if not gold_standard_data:\n        logger.error(\"Failed to load or\
        \ parse gold standard data. Exiting.\")\n        return\n\n    # 3. Initialize\
        \ Pipeline Components\n    try:\n        logger.info(\"Initializing HybridRetriever...\"\
        )\n        retriever = HybridRetriever(\n            ontology_data_path=ONTOLOGY_DUMP_JSON,\n\
        \            whoosh_index_dir=WHOOSH_INDEX_DIR,\n            faiss_index_path=FAISS_INDEX_PATH,\n\
        \            faiss_metadata_path=FAISS_METADATA_PATH,\n            embedding_model_name=EMBEDDING_MODEL_NAME\n\
        \        )\n        logger.info(\"HybridRetriever initialized successfully.\"\
        )\n\n        logger.info(f\"Initializing OllamaSelector with model '{OLLAMA_SELECTOR_MODEL_NAME}'...\"\
        )\n        selector = GeminiSelector(retriever=retriever)\n        logger.info(\"\
        OllamaSelector initialized successfully.\")\n\n    except Exception as e:\n\
        \        logger.error(f\"Failed to initialize pipeline components: {e}\",\
        \ exc_info=True)\n        return\n\n    # 4. Perform Evaluation\n    logger.info(\n\
        \        f\"Starting evaluation with Retriever(lexical_k={DEFAULT_K_LEXICAL},\
        \ vector_k={DEFAULT_K_VECTOR}) \"\n        f\"and Selector(model={OLLAMA_SELECTOR_MODEL_NAME})\"\
        \n    )\n    \n    accuracy, total, correct, ret_fails, sel_fails, misses\
        \ = evaluate_full_pipeline(\n        retriever, \n        selector, \n   \
        \     gold_standard_data, \n        lexical_k=DEFAULT_K_LEXICAL,\n       \
        \ vector_k=DEFAULT_K_VECTOR\n    )\n\n    # 5. Print and Save Results\n  \
        \  logger.info(\"--- Evaluation Complete ---\")\n    logger.info(f\"Total\
        \ entities evaluated: {total}\")\n    logger.info(f\"Retrieval Failures (no\
        \ candidates): {ret_fails}\")\n    logger.info(f\"Selection Failures (LLM\
        \ error): {sel_fails}\")\n    logger.info(\"-\" * 27)\n    valid_attempts\
        \ = total - ret_fails - sel_fails\n    logger.info(f\"Valid attempts for selector:\
        \ {valid_attempts}\")\n    logger.info(f\"Correct selections (Hits): {correct}\"\
        )\n    if valid_attempts > 0:\n        logger.info(f\"Accuracy: {accuracy:.4f}\
        \ ({correct}/{valid_attempts})\")\n    else:\n        logger.info(\"Accuracy:\
        \ N/A (no valid attempts were made)\")\n    \n    logger.info(f\"Saving {len(misses)}\
        \ incorrect selections to {EVALUATION_OUTPUT_FILE}\")\n    with open(EVALUATION_OUTPUT_FILE,\
        \ 'w', encoding='utf-8') as f:\n        json.dump(misses, f, indent=4)\n \
        \       \n    logger.info(\"Evaluation finished.\")\n\nif __name__ == \"__main__\"\
        :\n    if not os.path.exists(EVALUATION_XML_FILE):\n        logger.error(f\"\
        Evaluation XML file '{EVALUATION_XML_FILE}' not found.\")\n    else:\n   \
        \     main()"
      "evaluate_retriever_recall.py": "# src/evaluation/evaluate_retriever_recall.py\n\
        \nimport xml.etree.ElementTree as ET\nimport os\nimport sys\nimport logging\n\
        from collections import defaultdict\n\n# Add project root to Python path to\
        \ allow direct imports from src\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..'))\nsys.path.append(PROJECT_ROOT)\n\nfrom src.retriever.hybrid_retriever\
        \ import HybridRetriever\nfrom src.config import (\n    ONTOLOGY_DUMP_JSON,\n\
        \    WHOOSH_INDEX_DIR,\n    FAISS_INDEX_PATH,\n    FAISS_METADATA_PATH,\n\
        \    EMBEDDING_MODEL_NAME,\n    CURIE_PREFIX_MAP,\n    DEFAULT_K_LEXICAL,\n\
        \    DEFAULT_K_VECTOR,\n    DEFAULT_RERANK_K\n)\nfrom src.utils.ontology_utils\
        \ import uri_to_curie\n\n# --- Configuration for this specific evaluation\
        \ script ---\n# Path to your evaluation XML file\nEVALUATION_XML_FILE = os.path.join(PROJECT_ROOT,\
        \ \"data\", \"CafeteriaFCD_foodon_unique.xml\")\n# Top K results from the\
        \ combined retriever output to consider for a match\nRECALL_AT_K = DEFAULT_RERANK_K\
        \ # How many retrieved items to check for a match\n\n# --- Logging Setup ---\n\
        # Stays with basicConfig as src.utils.logging.get_logger is \"to be developed\"\
        \nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s\
        \ - %(message)s')\nlogger = logging.getLogger(__name__)\n\ndef parse_evaluation_xml(xml_file_path:\
        \ str) -> list:\n    \"\"\"\n    Parses the evaluation XML file to extract\
        \ entities and their ground truth semantic tags.\n\n    Args:\n        xml_file_path\
        \ (str): Path to the XML file.\n\n    Returns:\n        list: A list of dictionaries,\
        \ each containing:\n              {'text': str, 'true_curies': set_of_str,\
        \ 'doc_id': str, 'ann_id': str}\n              Returns an empty list if parsing\
        \ fails.\n    \"\"\"\n    if not os.path.exists(xml_file_path):\n        logger.error(f\"\
        Evaluation XML file not found: {xml_file_path}\")\n        return []\n\n \
        \   gold_standard_data = []\n    try:\n        tree = ET.parse(xml_file_path)\n\
        \        root = tree.getroot()\n        for doc_idx, document_node in enumerate(root.findall('.//document')):\n\
        \            doc_id_node = document_node.find('id')\n            doc_id =\
        \ doc_id_node.text if doc_id_node is not None else f\"doc_{doc_idx}\"\n  \
        \          \n            annotations = document_node.findall('annotation')\n\
        \            for ann_idx, annotation_node in enumerate(annotations):\n   \
        \             entity_text_node = annotation_node.find('text')\n          \
        \      semantic_tags_node = annotation_node.find('infon[@key=\"semantic_tags\"\
        ]')\n                ann_id_val = annotation_node.get('id', f\"ann_{doc_idx}_{ann_idx}\"\
        )\n\n\n                if entity_text_node is not None and semantic_tags_node\
        \ is not None and entity_text_node.text is not None and semantic_tags_node.text\
        \ is not None:\n                    entity_text = entity_text_node.text.strip()\n\
        \                    \n                    raw_tags = semantic_tags_node.text.strip()\n\
        \                    true_uris = {tag.strip() for tag in raw_tags.split(';')\
        \ if tag.strip()}\n                    \n                    true_curies =\
        \ set()\n                    for uri in true_uris:\n                     \
        \   try:\n                            # Adjusted: Use CURIE_PREFIX_MAP\n \
        \                           curie = uri_to_curie(uri, CURIE_PREFIX_MAP)\n\
        \                            if curie: \n                                true_curies.add(curie)\n\
        \                            else:\n                                logger.warning(f\"\
        Could not convert URI to CURIE: {uri} for entity '{entity_text}' in {doc_id}\
        \ (ann: {ann_id_val})\")\n                        except Exception as e:\n\
        \                            logger.error(f\"Error converting URI {uri} to\
        \ CURIE: {e}\")\n                    \n                    if entity_text\
        \ and true_curies:\n                        gold_standard_data.append({\n\
        \                            'text': entity_text,\n                      \
        \      'true_curies': true_curies,\n                            'doc_id':\
        \ doc_id,\n                            'ann_id': ann_id_val\n            \
        \            })\n                    elif entity_text: # Has text but no convertible\
        \ true CURIEs\n                        logger.warning(f\"Entity '{entity_text}'\
        \ in {doc_id} (ann: {ann_id_val}) had no convertible true CURIEs from URIs:\
        \ {true_uris}\")\n\n                # else: # This can be too verbose if many\
        \ annotations don't have these specific fields\n                #     logger.debug(f\"\
        Annotation missing text or semantic_tags in {doc_id}, ann_id={ann_id_val}\"\
        )\n\n\n    except ET.ParseError as e:\n        logger.error(f\"Error parsing\
        \ XML file {xml_file_path}: {e}\")\n        return []\n    except Exception\
        \ as e:\n        logger.error(f\"An unexpected error occurred during XML parsing:\
        \ {e}\", exc_info=True)\n        return []\n        \n    logger.info(f\"\
        Successfully parsed {len(gold_standard_data)} entities with text and true\
        \ CURIEs from {xml_file_path}\")\n    return gold_standard_data\n\ndef evaluate_retriever(retriever:\
        \ HybridRetriever, gold_standard_data: list, recall_at_k: int, lexical_k:\
        \ int, vector_k: int) -> tuple[float, int, int]:\n    \"\"\"\n    Evaluates\
        \ the retriever against the gold standard data.\n\n    Args:\n        retriever\
        \ (HybridRetriever): The initialized hybrid retriever.\n        gold_standard_data\
        \ (list): List of gold standard entities and their CURIEs.\n        recall_at_k\
        \ (int): The K value for Recall@K (slice of combined results).\n        lexical_k\
        \ (int): Number of results to fetch from lexical search.\n        vector_k\
        \ (int): Number of results to fetch from vector search.\n\n    Returns:\n\
        \        tuple: (recall_score, total_entities_processed, hits)\n    \"\"\"\
        \n    total_entities_processed = 0\n    hits = 0\n    \n    if not gold_standard_data:\n\
        \        logger.warning(\"No gold standard data provided for evaluation.\"\
        )\n        return 0.0, 0, 0\n\n    for i, item in enumerate(gold_standard_data):\n\
        \        query_text = item['text']\n        true_curies = item['true_curies']\n\
        \        \n        if not query_text or not true_curies:\n            # This\
        \ should ideally be filtered by parse_evaluation_xml already\n           \
        \ logger.warning(f\"Skipping item with empty query text or true_curies: {item}\"\
        )\n            continue\n            \n        total_entities_processed +=\
        \ 1\n        \n        logger.debug(f\"({i+1}/{len(gold_standard_data)}) Querying\
        \ for: '{query_text}', True CURIEs: {true_curies}\")\n\n        try:\n   \
        \         # Adjusted: HybridRetriever.search returns a dict: {\"lexical_results\"\
        : [], \"vector_results\": []}\n            retriever_output_dict = retriever.search(\n\
        \                query_string=query_text,\n                lexical_limit=lexical_k,\n\
        \                vector_k=vector_k,\n                target_ontologies=[\"\
        foodon\", \"chebi\"], # Adjusted: Use target_ontologies to limit search\n\
        \            )\n            \n            lexical_results = retriever_output_dict.get(\"\
        lexical_results\", [])\n            vector_results = retriever_output_dict.get(\"\
        vector_results\", [])\n\n            # Combine and deduplicate results, lexical\
        \ first then vector.\n            # Scores are not comparable, so this is\
        \ a simple merge strategy.\n            # The 'id' field from result dicts\
        \ contains the CURIE.\n            combined_ordered_results = []\n       \
        \     seen_ids = set()\n\n            for doc in lexical_results:\n      \
        \          doc_id = doc.get('id') # 'id' is the CURIE\n                if\
        \ doc_id and doc_id not in seen_ids:\n                    combined_ordered_results.append(doc)\
        \ # doc contains 'id', 'label', 'score', etc.\n                    seen_ids.add(doc_id)\n\
        \            \n            for doc in vector_results:\n                doc_id\
        \ = doc.get('id') # 'id' is the CURIE\n                if doc_id and doc_id\
        \ not in seen_ids:\n                    combined_ordered_results.append(doc)\n\
        \                    seen_ids.add(doc_id)\n            \n            # Extract\
        \ the CURIEs from the top `recall_at_k` combined documents\n            #\
        \ Adjusted: use doc['id'] as it stores the CURIE\n            retrieved_curies_set\
        \ = {doc['id'] for doc in combined_ordered_results[:recall_at_k]}\n      \
        \      logger.debug(f\"Retrieved CURIEs (top {recall_at_k} from combined):\
        \ {retrieved_curies_set}\")\n\n            if not true_curies.isdisjoint(retrieved_curies_set):\n\
        \                hits += 1\n                logger.info(f\"HIT! Query: '{query_text}'.\
        \ True: {true_curies}. Found in top {recall_at_k}: {true_curies.intersection(retrieved_curies_set)}\"\
        )\n            else:\n                logger.info(f\"MISS. Query: '{query_text}'.\
        \ True: {true_curies}. Top {recall_at_k} (combined) CURIEs: {retrieved_curies_set}\"\
        )\n                # For misses, log more details if needed, e.g. full combined_ordered_results\n\
        \                # logger.debug(f\"Full combined/ordered results for miss:\
        \ {combined_ordered_results}\")\n\n\n        except Exception as e:\n    \
        \        logger.error(f\"Error during retrieval or processing for query '{query_text}':\
        \ {e}\", exc_info=True)\n            \n    if total_entities_processed ==\
        \ 0:\n        logger.warning(\"No valid entities were processed for evaluation.\"\
        )\n        return 0.0, 0, 0\n        \n    recall_score = hits / total_entities_processed\n\
        \    return recall_score, total_entities_processed, hits\n\ndef main():\n\
        \    logger.info(\"Starting Retriever Evaluation Script...\")\n\n    # 1.\
        \ Parse the Gold Standard XML\n    logger.info(f\"Loading gold standard data\
        \ from: {EVALUATION_XML_FILE}\")\n    gold_standard_data = parse_evaluation_xml(EVALUATION_XML_FILE)\n\
        \    if not gold_standard_data:\n        logger.error(\"Failed to load or\
        \ parse gold standard data. Exiting.\")\n        return\n\n    # 2. Initialize\
        \ the HybridRetriever\n    logger.info(\"Initializing HybridRetriever...\"\
        )\n    try:\n        # Ensure all paths are valid and files exist where expected\
        \ by HybridRetriever\n        # Adjusted: Use ONTOLOGY_DUMP_JSON for ontology_data_path\n\
        \        if not os.path.exists(ONTOLOGY_DUMP_JSON):\n            logger.error(f\"\
        Ontology dump not found: {ONTOLOGY_DUMP_JSON}. Run ingestion pipeline.\")\n\
        \            return\n        if not os.path.exists(WHOOSH_INDEX_DIR) or not\
        \ os.listdir(WHOOSH_INDEX_DIR): # Check if dir exists and is not empty\n \
        \            logger.error(f\"Whoosh index directory is empty or not found:\
        \ {WHOOSH_INDEX_DIR}. Run ingestion pipeline.\")\n             return\n  \
        \      if not os.path.exists(FAISS_INDEX_PATH):\n            logger.error(f\"\
        FAISS index not found: {FAISS_INDEX_PATH}. Run ingestion pipeline.\")\n  \
        \          return\n        if not os.path.exists(FAISS_METADATA_PATH):\n \
        \           logger.error(f\"FAISS metadata not found: {FAISS_METADATA_PATH}.\
        \ Run ingestion pipeline.\")\n            return\n\n        retriever = HybridRetriever()\n\
        \        logger.info(\"HybridRetriever initialized successfully.\")\n    except\
        \ Exception as e:\n        logger.error(f\"Failed to initialize HybridRetriever:\
        \ {e}\", exc_info=True)\n        return\n\n    # 3. Perform Evaluation\n \
        \   logger.info(f\"Starting evaluation with Recall@{RECALL_AT_K}...\")\n \
        \   # Adjusted: Use DEFAULT_K_LEXICAL and DEFAULT_K_VECTOR\n    logger.info(f\"\
        HybridRetriever search params: Lexical K={DEFAULT_K_LEXICAL}, Vector K={DEFAULT_K_VECTOR}\"\
        )\n    \n    recall_score, total_entities, hits = evaluate_retriever(\n  \
        \      retriever, \n        gold_standard_data, \n        recall_at_k=RECALL_AT_K,\n\
        \        lexical_k=DEFAULT_K_LEXICAL, # Adjusted\n        vector_k=DEFAULT_K_VECTOR\
        \    # Adjusted\n    )\n\n    # 4. Print Results\n    logger.info(\"--- Evaluation\
        \ Results ---\")\n    logger.info(f\"Total entities processed: {total_entities}\"\
        )\n    logger.info(f\"Number of hits (at least one true CURIE found in top\
        \ {RECALL_AT_K} combined results): {hits}\")\n    if total_entities > 0:\n\
        \        logger.info(f\"Recall@{RECALL_AT_K}: {recall_score:.4f}\")\n    else:\n\
        \        logger.info(\"Recall not calculated as no entities were processed.\"\
        )\n\n    # 5. Clean up\n    try:\n        if hasattr(retriever, 'close') and\
        \ callable(retriever.close):\n            retriever.close()\n            logger.info(\"\
        Retriever resources closed.\")\n    except Exception as e:\n        logger.error(f\"\
        Error closing retriever resources: {e}\")\n\nif __name__ == \"__main__\":\n\
        \    eval_dir = os.path.dirname(EVALUATION_XML_FILE)\n    if not os.path.exists(eval_dir):\n\
        \        try:\n            os.makedirs(eval_dir)\n            logger.info(f\"\
        Created directory: {eval_dir}\")\n            logger.info(f\"Please place\
        \ '{os.path.basename(EVALUATION_XML_FILE)}' in {eval_dir} to run the evaluation.\"\
        )\n        except OSError as e:\n            logger.error(f\"Failed to create\
        \ directory {eval_dir}: {e}\")\n            sys.exit(1) # Exit if cannot create\
        \ data directory for eval file\n    \n    if not os.path.exists(EVALUATION_XML_FILE):\n\
        \        logger.error(f\"Evaluation XML file '{EVALUATION_XML_FILE}' not found.\
        \ Please place it in the correct directory.\")\n    else:\n        main()"
    "ingestion":
      "__init__.py": ""
      "build_lexical_index.py": "# src/ingestion/build_lexical_index.py\nimport sys\n\
        import json\nimport whoosh.index\nfrom whoosh.index import create_in\nfrom\
        \ whoosh.fields import Schema, ID, TEXT\nimport traceback\nfrom pathlib import\
        \ Path \n\n# --- Add project root to sys.path ---\nproject_root = Path(__file__).resolve().parent.parent\n\
        if project_root not in sys.path:\n    sys.path.insert(0, str(project_root))\n\
        \nfrom src.config import ONTOLOGIES_CONFIG\n\ndef build_single_index(json_path:\
        \ Path, index_dir: Path):\n    \"\"\"Builds a single Whoosh index from a given\
        \ JSON dump file.\"\"\"\n    print(f\"Building lexical index from {json_path}\
        \ into {index_dir}\")\n\n    schema = Schema(\n        curie=ID(stored=True,\
        \ unique=True),\n        label=TEXT(stored=True),\n        synonyms=TEXT(stored=True),\n\
        \        definition=TEXT(stored=True),\n        relations_text=TEXT(stored=False)\
        \ # Indexed but not stored to save space\n    )\n\n    # Ensure the target\
        \ directory exists\n    index_dir.mkdir(parents=True, exist_ok=True)\n   \
        \ try:\n        ix = create_in(str(index_dir), schema)\n        print(f\"\
        Whoosh index schema created in {index_dir}\")\n    except (whoosh.index.LockError,\
        \ whoosh.index.EmptyIndexError) as e:\n         print(f\"Error creating Whoosh\
        \ index directory or schema: {e}\")\n         traceback.print_exc()\n    \
        \     return\n         return\n\n    writer = ix.writer()\n    try:\n    \
        \    with json_path.open('r', encoding='utf-8') as f:\n            ontology_data\
        \ = json.load(f)\n\n        print(f\"Indexing {len(ontology_data)} entities...\"\
        )\n        indexed_count = 0\n        for curie, data in ontology_data.items():\n\
        \            label = data.get('label', '') or ''\n            synonyms_list\
        \ = data.get('synonyms', [])\n            synonyms = \" \".join(s for s in\
        \ synonyms_list if s)\n            definition = data.get('definition', '')\
        \ or ''\n            \n            # Create a searchable string from relation\
        \ data\n            relations_text = \"\"\n            relations_dict = data.get('relations',\
        \ {})\n            if relations_dict:\n                 # The 'key' is the\
        \ human-readable relation name (e.g., \"has ingredient\")\n              \
        \   # The 'value' is a list of target CURIEs (which we don't need for this\
        \ text field)\n                 relations_text = \" \".join(relations_dict.keys())\n\
        \            \n            writer.add_document(\n                curie=curie,\n\
        \                label=label,\n                synonyms=synonyms,\n      \
        \          definition=definition,\n                relations_text=relations_text\n\
        \            )\n            indexed_count += 1\n            if indexed_count\
        \ % 1000 == 0:\n                 print(f\"  ...indexed {indexed_count} entities...\"\
        )\n        \n        print(f\"Committing index with {indexed_count} documents.\"\
        )\n        writer.commit()\n        print(\"Lexical index built successfully.\"\
        )\n\n    except FileNotFoundError:\n        print(f\"Error: Ontology dump\
        \ file not found at {json_path}\")\n        traceback.print_exc()\n      \
        \  writer.cancel() \n    except (json.JSONDecodeError, whoosh.writing.IndexingError,\
        \ IOError) as e:\n        print(f\"An error occurred during index building:\
        \ {e}\")\n        traceback.print_exc()\n        writer.cancel()\n\ndef main():\n\
        \    \"\"\"Loops over all configured ontologies and builds a lexical index\
        \ for each.\"\"\"\n    for name, config_data in ONTOLOGIES_CONFIG.items():\n\
        \        dump_path = config_data.get('dump_json_path')\n        index_dir\
        \ = config_data.get('whoosh_index_dir')\n        \n        print(f\"\\n---\
        \ Building Lexical Index for: '{name}' ---\")\n\n        if not dump_path\
        \ or not index_dir:\n            print(f\"Warning: Configuration for '{name}'\
        \ is missing 'dump_json_path' or 'whoosh_index_dir'. Skipping.\")\n      \
        \      continue\n            \n        if not dump_path.exists():\n      \
        \      print(f\"Error: Ontology dump file not found at {dump_path}. Skipping\
        \ '{name}'.\")\n            print(\"Please run 'src/ingestion/parse_ontology.py'\
        \ and 'src/ingestion/enrich_documents.py' first.\")\n            continue\n\
        \            \n        build_single_index(dump_path, index_dir)\n        \n\
        \    print(\"\\n--- All lexical indexes have been built. ---\")\n\nif __name__\
        \ == \"__main__\":\n    main()"
      "enrich_documents.py": "# src/ingestion/enrich_documents.py\nimport json\nimport\
        \ logging\nfrom typing import Dict, Any, List, Optional\nimport sys\n\nfrom\
        \ pathlib import Path\n_PROJECT_ROOT = Path(__file__).resolve().parent.parent\n\
        \n# Add the project root to sys.path if it's not already there\nif _PROJECT_ROOT\
        \ not in sys.path:\n    sys.path.insert(0, str(_PROJECT_ROOT))  # Insert at\
        \ the beginning\n\ntry:\n    # Changed: Import specific configs needed\n \
        \   from src.config import ONTOLOGIES_CONFIG, RELATION_CONFIG\nexcept ModuleNotFoundError\
        \ as e:\n    print(f\"CRITICAL ERROR: Could not import project modules. Exception:\
        \ {e}\")\n    print(f\"This script expects to be run in a way that the 'src'\
        \ package is discoverable.\")\n    print(f\"Attempted to add project root\
        \ '{_PROJECT_ROOT}' to sys.path.\")\n    print(f\"Current sys.path: {sys.path}\"\
        )\n    print(\"Please ensure you are running this script from the project's\
        \ root directory ('onto_rag/'), for example:\")\n    print(\"  python src/ingestion/enrich_documents.py\"\
        )\n    sys.exit(1)\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s\
        \ - %(levelname)s - %(message)s')\n\ndef get_label_for_curie(curie: str, ontology_data:\
        \ Dict[str, Dict[str, Any]], default_value: Optional[str] = None) -> Optional[str]:\n\
        \    \"\"\"\n    Retrieves the label for a given CURIE from the ontology data.\n\
        \    Args:\n        curie: The CURIE string (e.g., \"FOODON:00001234\").\n\
        \        ontology_data: The loaded ontology data dictionary.\n        default_value:\
        \ Value to return if CURIE not found or has no label. Defaults to the CURIE\
        \ itself.\n    Returns:\n        The label string, or the default_value.\n\
        \    \"\"\"\n    if default_value is None:\n        default_value = curie\
        \ # Fallback to CURIE if no specific default\n\n    term_info = ontology_data.get(curie)\n\
        \    if term_info and term_info.get(\"label\"):\n        return term_info[\"\
        label\"]\n    return default_value\n\ndef get_relation_name(relation_curie:\
        \ str) -> str:\n    \"\"\"\n    Gets a human-readable name for a relation\
        \ CURIE using RELATION_CONFIG.\n    \"\"\"\n    # Exact match\n    if relation_curie\
        \ in RELATION_CONFIG:\n        return RELATION_CONFIG[relation_curie][\"label\"\
        ]\n    \n    # Check for generic FoodON prefix if specific one not found\n\
        \    generic_foodon_prefix = \"obo:FOODON_\"\n    if relation_curie.startswith(generic_foodon_prefix)\
        \ and generic_foodon_prefix in RELATION_CONFIG:\n         # Attempt to make\
        \ it slightly more readable if it's like \"obo:FOODON_0000XXXX\"\n       \
        \ relation_suffix = relation_curie.split('_')[-1]\n        return f\"FoodON\
        \ relation {relation_suffix}\"\n\n    # Fallback for other OBO relations\n\
        \    if relation_curie.startswith(\"obo:\"):\n        name_part = relation_curie.split(':')[-1].replace(\"\
        _\", \" \")\n        return name_part\n\n    return relation_curie # Fallback\
        \ to the CURIE itself\n\ndef create_enriched_documents(ontology_data_path:\
        \ Path, output_path: Path) -> List[Dict[str, Any]]:\n    \"\"\"\n    Creates\
        \ enriched text documents for each ontology entry.\n    Args:\n        ontology_data_path:\
        \ Path to the ontology_dump.json file.\n        output_path: Path to save\
        \ the enriched_documents.json file.\n    Returns:\n        A list of enriched\
        \ document dictionaries.\n    \"\"\"\n    logging.info(f\"Loading ontology\
        \ data from {ontology_data_path}...\")\n    try:\n        with open(ontology_data_path,\
        \ 'r', encoding='utf-8') as f:\n            ontology_data = json.load(f)\n\
        \    except FileNotFoundError:\n        logging.error(f\"Error: Ontology data\
        \ file not found at {ontology_data_path}\")\n        return []\n    except\
        \ json.JSONDecodeError:\n        logging.error(f\"Error: Could not decode\
        \ JSON from {ontology_data_path}\")\n        return []\n\n    logging.info(f\"\
        Successfully loaded {len(ontology_data)} terms.\")\n\n    enriched_docs =\
        \ []\n\n    for term_curie, term_data in ontology_data.items():\n        doc_parts\
        \ = []\n\n        # 1. Label\n        label = term_data.get(\"label\")\n \
        \       if not label:\n            logging.warning(f\"Term {term_curie} has\
        \ no label. Using CURIE as label for document generation.\")\n           \
        \ label = term_curie \n        \n        doc_parts.append(f\"{label}.\")\n\
        \n        # 2. Definition\n        definition = term_data.get(\"definition\"\
        )\n        if definition:\n            doc_parts.append(f\"{definition}.\"\
        )\n\n        # 3. Synonyms\n        synonyms = term_data.get(\"synonyms\"\
        )\n        if synonyms:\n            synonyms_text = \"; \".join(synonyms)\n\
        \            doc_parts.append(f\"Also known as: {synonyms_text}.\")\n\n  \
        \      # 4. Parents (direct subclasses)\n        parent_curies = term_data.get(\"\
        parents\", [])\n        if parent_curies:\n            # Since this function\
        \ processes one self-contained ontology dump,\n            # all parent lookups\
        \ are valid within the same ontology_data.\n            parent_labels = [get_label_for_curie(p_curie,\
        \ ontology_data) for p_curie in parent_curies]\n            parent_labels_filtered\
        \ = [l for l in parent_labels if l]\n            if parent_labels_filtered:\n\
        \                if len(parent_labels_filtered) == 1:\n                  \
        \  doc_parts.append(f\"Is a type of: {parent_labels_filtered[0]}.\")\n   \
        \             else:\n                    doc_parts.append(f\"Is a type of:\
        \ {'; '.join(parent_labels_filtered)}.\")\n        \n        # 5. Relations\n\
        \        relations = term_data.get(\"relations\", {})\n        relation_texts\
        \ = []\n        for rel_name, target_curies_list in relations.items():\n \
        \           # In the new structure, rel_name is the human-readable name from\
        \ parse_ontology\n            target_labels = [get_label_for_curie(t_curie,\
        \ ontology_data) for t_curie in target_curies_list]\n            target_labels_filtered\
        \ = [l for l in target_labels if l]\n            if target_labels_filtered:\n\
        \                relation_texts.append(f\"{rel_name}: {', '.join(target_labels_filtered)}\"\
        )\n        \n        if relation_texts:\n            doc_parts.append(\"Key\
        \ characteristics include: \" + \"; \".join(relation_texts) + \".\")\n\n \
        \       # Combine all parts into a single text\n        enriched_text = \"\
        \ \".join(doc_parts).replace(\"..\", \".\").strip()\n\n        enriched_docs.append({\n\
        \            \"id\": term_curie,\n            \"label\": term_data.get(\"\
        label\", term_curie), # Store original label if available, else CURIE\n  \
        \          \"text\": enriched_text\n        })\n\n    logging.info(f\"Created\
        \ {len(enriched_docs)} enriched documents.\")\n\n    logging.info(f\"Saving\
        \ enriched documents to {output_path}...\")\n    try:\n        with open(output_path,\
        \ 'w', encoding='utf-8') as f:\n            json.dump(enriched_docs, f, indent=2)\n\
        \        logging.info(\"Successfully saved enriched documents.\")\n    except\
        \ IOError:\n        logging.error(f\"Error: Could not write enriched documents\
        \ to {output_path}\")\n\n    return enriched_docs\n\ndef main():\n    \"\"\
        \"Main function to create enriched documents for each configured ontology.\"\
        \"\"\n    for name, config_data in ONTOLOGIES_CONFIG.items():\n        logging.info(f\"\
        \\n--- Enriching documents for '{name}' ---\")\n        \n        ontology_dump_path\
        \ = config_data.get('dump_json_path')\n        enriched_docs_output_path =\
        \ config_data.get('enriched_docs_path')\n\n        if not ontology_dump_path\
        \ or not enriched_docs_output_path:\n            logging.warning(f\"Configuration\
        \ for '{name}' is missing 'dump_json_path' or 'enriched_docs_path'. Skipping.\"\
        )\n            continue\n            \n        if not ontology_dump_path.exists():\n\
        \            logging.error(f\"Dump file not found: {ontology_dump_path}. Skipping\
        \ '{name}'.\")\n            continue\n\n        # Ensure output directory\
        \ exists\n        enriched_docs_output_path.parent.mkdir(parents=True, exist_ok=True)\n\
        \            \n        create_enriched_documents(\n            ontology_data_path=ontology_dump_path,\n\
        \            output_path=enriched_docs_output_path\n        )\n        \n\
        \    logging.info(\"\\n--- All enriched document file creation complete. ---\"\
        )\n\n\nif __name__ == \"__main__\":\n    main()"
      "parse_ontology.py": "# src/ingestion/parse_ontology.py\nimport sys\nimport\
        \ logging # Import logging\nimport rdflib\nfrom rdflib import Graph, Namespace,\
        \ URIRef, RDFS, OWL, RDF\nfrom typing import Dict, List, Any\nimport json\n\
        import traceback\nfrom pathlib import Path\n\n# --- Add project root to sys.path\
        \ ---\nproject_root = Path(__file__).resolve().parent.parent\nif project_root\
        \ not in sys.path:\n    sys.path.insert(0, str(project_root))\n# --- End sys.path\
        \ modification ---\n\n# Now import using the 'src' package prefix\nfrom src.config\
        \ import (\n    ONTOLOGIES_CONFIG,       \n    CURIE_PREFIX_MAP,\n    RELATION_CONFIG,\n\
        \    TARGET_RELATIONS_CURIES,\n    IAO_NS_STR,\n    OBOINOWL_NS_STR,\n)\n\
        from src.utils.ontology_utils import uri_to_curie, curie_to_uri\n\n# --- Logging\
        \ Setup ---\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s -\
        \ %(levelname)s - %(module)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\
        \n\n# Define commonly used namespaces (can still use these locally for convenience)\n\
        IAO = Namespace(IAO_NS_STR)\nOBOINOWL = Namespace(OBOINOWL_NS_STR)\n\n\ndef\
        \ load_ontology(path: str) -> rdflib.Graph:\n    g = Graph()\n    try:\n \
        \       logger.info(f\"Loading ontology from: {path}\")\n        try:\n  \
        \          g.parse(path, format=\"application/rdf+xml\")\n        except Exception\
        \ as e_xml:\n            logger.warning(f\"Failed to parse as RDF/XML: {e_xml}.\
        \ Trying Turtle...\")\n            try:\n                g.parse(path, format=\"\
        turtle\")\n            except Exception as e_ttl:\n                logger.warning(f\"\
        Failed to parse as Turtle: {e_ttl}. Trying auto-detection...\")\n        \
        \        g.parse(path)\n\n        logger.info(f\"Ontology loaded successfully.\
        \ Contains {len(g)} triples.\")\n        return g\n    except FileNotFoundError:\n\
        \        logger.error(f\"Error: Ontology file not found at {path}\")\n   \
        \     raise\n    except Exception as e:\n        logger.error(f\"Error parsing\
        \ ontology file {path}: {e}\")\n        traceback.print_exc()\n        raise\n\
        \ndef get_ancestors(g: Graph, term_uri: URIRef, prefix_map: Dict[str, str],\
        \ visited_uris: set = None) -> List[str]:\n    if visited_uris is None:\n\
        \        visited_uris = set()\n\n    ancestor_curies = set()\n    for parent_uri\
        \ in g.objects(subject=term_uri, predicate=RDFS.subClassOf):\n        if isinstance(parent_uri,\
        \ URIRef) and parent_uri != OWL.Thing:\n            if parent_uri not in visited_uris:\n\
        \                visited_uris.add(parent_uri)\n                # Pass the\
        \ prefix_map explicitly\n                parent_curie = uri_to_curie(parent_uri,\
        \ prefix_map)\n                if parent_curie and parent_curie != str(parent_uri):\n\
        \                    ancestor_curies.add(parent_curie)\n                 \
        \   ancestor_curies.update(get_ancestors(g, parent_uri, prefix_map, visited_uris))\n\
        \    return list(ancestor_curies)\n\n\ndef extract_labels_and_synonyms(g:\
        \ Graph, prefix_map: Dict[str, str]) -> Dict[str, Dict[str, Any]]:\n    data\
        \ = {}\n    relevant_predicates = [\n        RDFS.label,\n        OBOINOWL.hasExactSynonym,\
        \ OBOINOWL.hasRelatedSynonym,\n        OBOINOWL.hasNarrowSynonym, OBOINOWL.hasBroadSynonym\n\
        \    ]\n    \n    processed_subjects = set()\n\n    for pred in relevant_predicates:\n\
        \        for s_uri in g.subjects(predicate=pred):\n            if not isinstance(s_uri,\
        \ URIRef) or s_uri in processed_subjects:\n                continue\n    \
        \        \n            # Pass the prefix_map explicitly\n            curie\
        \ = uri_to_curie(s_uri, prefix_map)\n            if not curie or curie ==\
        \ str(s_uri): # Skip if not converted to a CURIE effectively\n           \
        \     continue\n\n            if curie not in data:\n                data[curie]\
        \ = {\"label\": None, \"synonyms\": []}\n\n            # Label\n         \
        \   label_val = g.value(subject=s_uri, predicate=RDFS.label)\n           \
        \ if label_val and isinstance(label_val, rdflib.Literal):\n              \
        \  data[curie][\"label\"] = str(label_val)\n\n            # Synonyms\n   \
        \         current_synonyms = []\n            for syn_prop in [OBOINOWL.hasExactSynonym,\
        \ OBOINOWL.hasRelatedSynonym,\n                             OBOINOWL.hasNarrowSynonym,\
        \ OBOINOWL.hasBroadSynonym]:\n                for syn_obj in g.objects(subject=s_uri,\
        \ predicate=syn_prop):\n                    if isinstance(syn_obj, rdflib.Literal):\n\
        \                        current_synonyms.append(str(syn_obj))\n         \
        \   \n            if \"synonyms\" not in data[curie] or data[curie][\"synonyms\"\
        ] is None:\n                data[curie][\"synonyms\"] = []\n            for\
        \ s in current_synonyms:\n                if s not in data[curie][\"synonyms\"\
        ]:\n                    data[curie][\"synonyms\"].append(s)\n\n          \
        \  processed_subjects.add(s_uri)\n            \n    final_data = {k: v for\
        \ k, v in data.items() if v.get(\"label\") or v.get(\"synonyms\")}\n    logger.info(f\"\
        Extracted labels and synonyms for {len(final_data)} terms.\")\n    return\
        \ final_data\n\n\ndef extract_definitions(g: Graph, prefix_map: Dict[str,\
        \ str]) -> Dict[str, str]:\n    definitions = {}\n    definition_prop_uri\
        \ = IAO['0000115'] # IAO:0000115 is 'definition'\n    for s_uri in g.subjects(predicate=definition_prop_uri):\n\
        \        if not isinstance(s_uri, URIRef):\n            continue\n       \
        \ \n        curie = uri_to_curie(s_uri, prefix_map)\n        if not curie\
        \ or curie == str(s_uri):\n            continue\n        \n        def_obj\
        \ = g.value(subject=s_uri, predicate=definition_prop_uri)\n        if def_obj\
        \ and isinstance(def_obj, rdflib.Literal):\n            definitions[curie]\
        \ = str(def_obj)\n            \n    logger.info(f\"Extracted definitions for\
        \ {len(definitions)} terms.\")\n    return definitions\n\ndef extract_hierarchy(g:\
        \ Graph, prefix_map: Dict[str, str]) -> Dict[str, Dict[str, List[str]]]:\n\
        \    hierarchy_data = {}\n    all_terms_in_hierarchy = set()\n    for s, p,\
        \ o in g.triples((None, RDFS.subClassOf, None)):\n        if isinstance(s,\
        \ URIRef): all_terms_in_hierarchy.add(s)\n        if isinstance(o, URIRef):\
        \ all_terms_in_hierarchy.add(o)\n    \n    for term_uri in all_terms_in_hierarchy:\n\
        \        if term_uri == OWL.Thing:\n            continue\n\n        curie\
        \ = uri_to_curie(term_uri, prefix_map)\n        if not curie or curie == str(term_uri):\n\
        \            continue\n\n        direct_parent_curies = []\n        for parent_uri\
        \ in g.objects(subject=term_uri, predicate=RDFS.subClassOf):\n           \
        \ if isinstance(parent_uri, URIRef) and parent_uri != OWL.Thing:\n       \
        \         parent_curie = uri_to_curie(parent_uri, prefix_map)\n          \
        \      if parent_curie and parent_curie != str(parent_uri):\n            \
        \        direct_parent_curies.append(parent_curie)\n        \n        ancestor_curies\
        \ = get_ancestors(g, term_uri, prefix_map, visited_uris=set())\n        \n\
        \        if direct_parent_curies or ancestor_curies:\n            hierarchy_data[curie]\
        \ = {\n                \"parents\": list(set(direct_parent_curies)),\n   \
        \             \"ancestors\": list(set(ancestor_curies))\n            }\n \
        \           \n    logger.info(f\"Extracted hierarchy data for {len(hierarchy_data)}\
        \ terms.\")\n    return hierarchy_data\n\n\ndef extract_relations(g: Graph,\
        \ props_to_extract: Dict[str, str], prefix_map: Dict[str, str]) -> Dict[str,\
        \ Dict[str, List[str]]]:\n    relations_data = {}\n    \n    for term_uri\
        \ in g.subjects(unique=True):\n        if not isinstance(term_uri, URIRef):\n\
        \            continue\n\n        curie = uri_to_curie(term_uri, prefix_map)\n\
        \        if not curie or curie == str(term_uri):\n            continue\n\n\
        \        term_specific_relations = {}\n        for rel_readable_name, rel_uri_str\
        \ in props_to_extract.items():\n            rel_uri = URIRef(rel_uri_str)\n\
        \            target_curies = []\n            for target_obj in g.objects(subject=term_uri,\
        \ predicate=rel_uri):\n                if isinstance(target_obj, URIRef):\n\
        \                    target_curie = uri_to_curie(target_obj, prefix_map)\n\
        \                    if target_curie and target_curie != str(target_obj):\n\
        \                        target_curies.append(target_curie)\n            \n\
        \            if target_curies:\n                term_specific_relations[rel_readable_name]\
        \ = list(set(target_curies))\n\n        if term_specific_relations:\n    \
        \        relations_data[curie] = term_specific_relations\n            \n \
        \   logger.info(f\"Extracted relations for {len(relations_data)} terms based\
        \ on {len(props_to_extract)} specified properties.\")\n    return relations_data\n\
        \n\ndef main():\n    logger.info(\"--- Starting Ontology Parsing for Each\
        \ Configured Ontology ---\")\n\n    # Prepare relation properties once\n \
        \   relation_properties_for_extraction = {}\n    for rel_curie_str in TARGET_RELATIONS_CURIES:\n\
        \        if rel_curie_str in RELATION_CONFIG:\n            readable_name =\
        \ RELATION_CONFIG[rel_curie_str].get(\"label\", rel_curie_str)\n         \
        \   full_rel_uri = curie_to_uri(rel_curie_str, CURIE_PREFIX_MAP)\n       \
        \     if full_rel_uri:\n                relation_properties_for_extraction[readable_name]\
        \ = str(full_rel_uri)\n            else:\n                logger.warning(f\"\
        Could not convert relation CURIE {rel_curie_str} to URI. Skipping this relation.\"\
        )\n        else:\n            logger.warning(f\"Relation CURIE {rel_curie_str}\
        \ from TARGET_RELATIONS_CURIES not found in RELATION_CONFIG.\")\n\n    # Loop\
        \ through each ontology configured in config.py\n    for name, config_data\
        \ in ONTOLOGIES_CONFIG.items():\n        ontology_path = config_data['path']\n\
        \        dump_path = config_data['dump_json_path']\n        \n        # Ensure\
        \ the output directory exists\n        dump_path.parent.mkdir(parents=True,\
        \ exist_ok=True)\n        \n        logger.info(f\"\\n--- Processing Ontology:\
        \ '{name}' ---\")\n        logger.info(f\"Source: {ontology_path}\")\n   \
        \     logger.info(f\"Destination: {dump_path}\")\n\n        if not ontology_path.exists():\n\
        \            logger.error(f\"Ontology file not found. Skipping '{name}'.\"\
        )\n            continue\n\n        try:\n            # 1. Load the single\
        \ ontology graph\n            g = load_ontology(ontology_path)\n\n       \
        \     # 2. Extract data FROM THIS GRAPH ONLY\n            logger.info(f\"\
        Extracting data for '{name}'...\")\n            labels_synonyms = extract_labels_and_synonyms(g,\
        \ CURIE_PREFIX_MAP)\n            definitions = extract_definitions(g, CURIE_PREFIX_MAP)\n\
        \            hierarchy = extract_hierarchy(g, CURIE_PREFIX_MAP)\n        \
        \    relations = extract_relations(g, relation_properties_for_extraction,\
        \ CURIE_PREFIX_MAP)\n\n            # 3. Merge extracted data for this ontology\n\
        \            logger.info(\"Merging extracted data...\")\n            ontology_specific_data\
        \ = {}\n            all_curies = set(labels_synonyms.keys()) | set(definitions.keys())\
        \ | set(hierarchy.keys()) | set(relations.keys())\n\n            for curie_key\
        \ in all_curies:\n                ontology_specific_data[curie_key] = {\n\
        \                    \"label\": labels_synonyms.get(curie_key, {}).get(\"\
        label\"),\n                    \"synonyms\": labels_synonyms.get(curie_key,\
        \ {}).get(\"synonyms\", []),\n                    \"definition\": definitions.get(curie_key),\n\
        \                    \"parents\": hierarchy.get(curie_key, {}).get(\"parents\"\
        , []),\n                    \"ancestors\": hierarchy.get(curie_key, {}).get(\"\
        ancestors\", []),\n                    \"relations\": relations.get(curie_key,\
        \ {})\n                }\n            \n            final_data = {k: v for\
        \ k, v in ontology_specific_data.items() if any(v.values())}\n           \
        \ \n            # 4. Save the dedicated dump file\n            logger.info(f\"\
        Found {len(final_data)} entities with data in '{name}'.\")\n            logger.info(f\"\
        Writing data to {dump_path}\")\n            with open(dump_path, 'w', encoding='utf-8')\
        \ as f:\n                json.dump(final_data, f, indent=4, ensure_ascii=False)\n\
        \n            logger.info(f\"Successfully processed '{name}'.\")\n\n     \
        \   except Exception as e:\n            logger.error(f\"An error occurred\
        \ while processing '{name}': {e}\")\n            traceback.print_exc()\n\n\
        \    logger.info(\"\\n--- All Ontology Parsing Complete ---\")\n\n\nif __name__\
        \ == \"__main__\":\n    main()"
    "pipeline":
      "base_pipeline.py": "# src/pipeline/base_pipeline.py\nimport sys\nimport logging\n\
        from typing import List, Dict, Any, Optional, Type\nfrom pathlib import Path\n\
        \n\nPROJECT_ROOT = Path(__file__).resolve().parent.parent\nif str(PROJECT_ROOT)\
        \ not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\n\nfrom src.rag_selectors.base_selector\
        \ import BaseSelector\nfrom src.confidence_scorers.base_confidence_scorer\
        \ import BaseConfidenceScorer\nfrom src.retriever.hybrid_retriever import\
        \ HybridRetriever\nfrom src.reranker.llm_reranker import LLMReranker\nfrom\
        \ src import config\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s\
        \ - %(levelname)s - %(name)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\
        \nclass BaseRAGPipeline:\n    \"\"\"\n    A base class for the RAG pipeline,\
        \ abstracting the common logic for\n    retrieving, reranking, and selecting\
        \ ontology terms.\n    \n    Subclasses should specify the selector to use.\n\
        \    \"\"\"\n    # MODIFIED __init__ to accept a confidence scorer\n    def\
        \ __init__(self, selector_class: Type[BaseSelector], confidence_scorer_class:\
        \ Type[BaseConfidenceScorer]):\n        \"\"\"\n        Initializes the RAG\
        \ Pipeline.\n\n        Args:\n            selector_class (Type[BaseSelector]):\
        \ The class of the selector to use.\n            confidence_scorer_class (Type[BaseConfidenceScorer]):\
        \ The class of the confidence scorer to use.\n        \"\"\"\n        logger.info(f\"\
        Initializing RAG Pipeline with {selector_class.__name__} and {confidence_scorer_class.__name__}...\"\
        )\n        try:\n            self.retriever = HybridRetriever()\n\n      \
        \      all_enriched_docs_paths = [\n                data['enriched_docs_path']\
        \ \n                for data in config.ONTOLOGIES_CONFIG.values()\n      \
        \          if data['enriched_docs_path'].exists()\n            ]\n\n     \
        \       if not all_enriched_docs_paths:\n                raise FileNotFoundError(\"\
        No enriched document files found for any configured ontology. Please run the\
        \ ingestion pipeline.\")\n\n            logger.info(f\"Initializing LLMReranker\
        \ with {len(all_enriched_docs_paths)} enriched document file(s).\")\n    \
        \        \n            self.reranker = LLMReranker(\n                model_name=config.RERANKER_MODEL_NAME,\n\
        \                enriched_docs_paths=all_enriched_docs_paths,\n          \
        \      device=config.EMBEDDING_DEVICE\n            )\n            \n     \
        \       self.selector = selector_class(retriever=self.retriever)\n       \
        \     self.confidence_scorer = confidence_scorer_class() # ADDED\n       \
        \     logger.info(\"RAG Pipeline initialized successfully.\")\n          \
        \  \n        except (FileNotFoundError, ValueError) as e:\n            logger.error(f\"\
        Failed to initialize pipeline: {e}\")\n            logger.error(\"Please run\
        \ ingestion scripts (e.g., 'scripts/rebuild_base.bash') and ensure necessary\
        \ API keys are set in your .env file.\")\n            raise\n        except\
        \ Exception as e:\n            logger.error(f\"An unexpected error occurred\
        \ during pipeline initialization: {e}\", exc_info=True)\n            raise\n\
        \n    # MODIFIED run method to incorporate the two-step selection and scoring\n\
        \    def run(self, \n            query: str, \n            lexical_k: int\
        \ = config.DEFAULT_K_LEXICAL, \n            vector_k: int = config.DEFAULT_K_VECTOR,\
        \ \n            rerank_top_n: int = 10\n            ) -> Optional[tuple[Dict[str,\
        \ Any], List[Dict[str, Any]]]]:\n        \"\"\"\n        Executes the full\
        \ pipeline for a given query.\n\n        Returns:\n            A tuple containing\
        \ (final_result_dict, candidates_list), or None.\n            The final_result_dict\
        \ includes the confidence score and reasoning.\n        \"\"\"\n        logger.info(\"\
        Running pipeline for query: '%s'\", query)\n\n        # 1. Retrieve\n    \
        \    retriever_output = self.retriever.search(query, lexical_limit=lexical_k,\
        \ vector_k=vector_k)\n        lexical_results = retriever_output.get(\"lexical_results\"\
        , [])\n        vector_results = retriever_output.get(\"vector_results\", [])\n\
        \        \n        # 2. Merge unique candidates\n        combined_candidates\
        \ = []\n        seen_ids = set()\n        for doc in lexical_results + vector_results:\n\
        \            doc_id = doc.get('id')\n            if doc_id and doc_id not\
        \ in seen_ids:\n                combined_candidates.append(doc)\n        \
        \        seen_ids.add(doc_id)\n        \n        if not combined_candidates:\n\
        \            logger.warning(\"No candidates found for query: '%s'\", query)\n\
        \            return None\n\n        # 3. Rerank (Uncommented as per intended\
        \ logic)\n        reranked_candidates = self.reranker.rerank(query, combined_candidates,\
        \ top_n=rerank_top_n)\n\n        if not reranked_candidates:\n           \
        \ logger.warning(\"No candidates left after reranking for query: '%s'\", query)\n\
        \            return None\n        \n        logger.info(f\"Top {len(reranked_candidates)}\
        \ candidates after reranking passed to LLM selector.\")\n\n        # 4. Select\
        \ with Selector Agent\n        selection = self.selector.select_best_term(query,\
        \ reranked_candidates)\n\n        if not selection:\n            logger.warning(\"\
        LLM selection failed. Cannot proceed to confidence scoring.\")\n         \
        \   return None, reranked_candidates\n\n        chosen_id = selection['chosen_id']\n\
        \        initial_explanation = selection.get('explanation', 'No explanation\
        \ provided.')\n        \n        if chosen_id in ('0', '-1'):\n          \
        \  logger.info(\"Selector determined no suitable match exists for query: '%s'\"\
        , query)\n            no_match_result = {\n                'id': chosen_id,\n\
        \                'label': 'No Match Found',\n                'definition':\
        \ 'The selector model determined that no candidate was a suitable match.',\n\
        \                'synonyms': [], 'parents': [], 'ancestors': [], 'relations':\
        \ {},\n                'confidence_score': 0.0,\n                'explanation':\
        \ initial_explanation\n            }\n            return no_match_result,\
        \ reranked_candidates\n\n        chosen_term_details = self.retriever.get_term_details(chosen_id)\n\
        \        if not chosen_term_details:\n            logger.error(\"Selector\
        \ chose ID '%s', but its details could not be retrieved.\", chosen_id)\n \
        \           return None, reranked_candidates\n        \n        # 5. Score\
        \ Confidence with Confidence Agent\n        logger.info(\"Passing selection\
        \ to confidence scorer for query: '%s'\", query)\n        confidence_result\
        \ = self.confidence_scorer.score_confidence(\n            query=query,\n \
        \           chosen_term_details=chosen_term_details,\n            all_candidates=reranked_candidates\n\
        \        )\n\n        if confidence_result:\n            chosen_term_details['confidence_score']\
        \ = confidence_result.get('confidence_score', 0.0)\n            chosen_term_details['explanation']\
        \ = confidence_result.get('explanation', initial_explanation)\n          \
        \  logger.info(f\"Confidence score for '{chosen_id}': {chosen_term_details['confidence_score']:.2f}\"\
        )\n        else:\n            logger.warning(\"Confidence scoring failed.\
        \ Using default values.\")\n            chosen_term_details['confidence_score']\
        \ = 0.0\n            chosen_term_details['explanation'] = initial_explanation\n\
        \        \n        return chosen_term_details, reranked_candidates\n\n   \
        \ def close(self):\n        \"\"\"Closes any open resources, like database\
        \ connections.\"\"\"\n        if hasattr(self.retriever, 'close'):\n     \
        \       self.retriever.close()\n        logger.info(\"Pipeline resources closed.\"\
        )"
      "gemini_pipeline.py": "# src/pipeline/gemini_pipeline.py\nimport sys\nfrom pathlib\
        \ import Path\nPROJECT_ROOT = Path(__file__).resolve().parent.parent\nif str(PROJECT_ROOT)\
        \ not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\n    \nfrom\
        \ src.pipeline.base_pipeline import BaseRAGPipeline\nfrom src.rag_selectors.gemini_selector\
        \ import GeminiSelector\nfrom src.confidence_scorers.gemini_confidence_scorer\
        \ import GeminiConfidenceScorer # ADDED\n\nclass GeminiRAGPipeline(BaseRAGPipeline):\n\
        \    \"\"\"\n    A RAG pipeline implementation that uses Gemini for both selection\
        \ and confidence scoring.\n    \"\"\"\n    def __init__(self):\n        \"\
        \"\"\n        Initializes the Gemini-specific RAG pipeline.\n        \"\"\"\
        \n        # Pass the Gemini-specific classes to the base constructor\n   \
        \     super().__init__(\n            selector_class=GeminiSelector, \n   \
        \         confidence_scorer_class=GeminiConfidenceScorer\n        )"
      "ollama_pipeline.py": "# src/pipeline/ollama_pipeline.py\nimport sys\nfrom pathlib\
        \ import Path\nPROJECT_ROOT = Path(__file__).resolve().parent.parent\nif str(PROJECT_ROOT)\
        \ not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\n    \n\nfrom\
        \ src.pipeline.base_pipeline import BaseRAGPipeline\nfrom src.rag_selectors.ollama_selector\
        \ import OllamaSelector\n\nclass OllamaRAGPipeline(BaseRAGPipeline):\n   \
        \ \"\"\"\n    A RAG pipeline implementation that uses the OllamaSelector for\
        \ the\n    final term selection step.\n    \"\"\"\n    def __init__(self):\n\
        \        \"\"\"\n        Initializes the Ollama-specific RAG pipeline.\n \
        \       \"\"\"\n        # Pass the OllamaSelector class to the base constructor\n\
        \        super().__init__(selector_class=OllamaSelector)"
      "pipeline_factory.py": |-
        # src/pipeline/pipeline_factory.py
        from src.pipeline.base_pipeline import BaseRAGPipeline
        from src.pipeline.gemini_pipeline import GeminiRAGPipeline
        from src.pipeline.ollama_pipeline import OllamaRAGPipeline

        def get_pipeline(pipeline_name: str) -> BaseRAGPipeline:
            """
            Factory function to get an instance of a RAG pipeline.
            This is the central place to manage pipeline selection.
            """
            if pipeline_name == "gemini":
                return GeminiRAGPipeline()
            elif pipeline_name == "ollama":
                return OllamaRAGPipeline()
            # To add a new pipeline, you would just add another elif here.
            # elif pipeline_name == "huggingface":
            #     from .huggingface_pipeline import HuggingFaceRAGPipeline
            #     return HuggingFaceRAGPipeline()
            else:
                raise ValueError(f"Unknown pipeline name: '{pipeline_name}'. Valid options are 'gemini', 'ollama'.")
    "rag_selectors":
      "__init__.py": ""
      "base_selector.py": "# src/rag_selectors/base_selector.py\nimport logging\n\
        import json\nfrom abc import ABC, abstractmethod\nfrom typing import List,\
        \ Dict, Any, Optional\nfrom pathlib import Path\n\nfrom src.retriever.hybrid_retriever\
        \ import HybridRetriever\nfrom src import config\n\nlogger = logging.getLogger(__name__)\n\
        \nclass BaseSelector(ABC):\n    \"\"\"\n    Abstract base class for LLM-based\
        \ term selectors.\n\n    This class provides the common framework for loading\
        \ prompts, formatting\n    candidate lists, and parsing the final LLM response.\
        \ Subclasses must\n    implement the provider-specific `__init__` setup and\
        \ the `_call_llm` method.\n    \"\"\"\n\n    def __init__(self, retriever:\
        \ HybridRetriever, model_name: str):\n        \"\"\"\n        Initializes\
        \ the BaseSelector.\n\n        Args:\n            retriever (HybridRetriever):\
        \ An initialized retriever instance.\n            model_name (str): The name\
        \ of the LLM model to use.\n        \"\"\"\n        self.retriever = retriever\n\
        \        self.model_name = model_name\n        self.prompt_template = self._load_prompt_template()\n\
        \        logger.info(f\"{self.__class__.__name__} initialized for model: {self.model_name}\"\
        )\n\n    def _load_prompt_template(self) -> str:\n        \"\"\"Loads the\
        \ prompt template from the file.\"\"\"\n        template_path = config.SELECTOR_PROMPT_TEMPLATE_PATH\n\
        \        try:\n            with template_path.open('r', encoding='utf-8')\
        \ as f:\n                return f.read()\n        except FileNotFoundError:\n\
        \            logger.error(f\"Prompt template not found at {template_path}\"\
        )\n            raise\n\n    def _format_candidates_for_prompt(self, candidates:\
        \ List[Dict[str, Any]]) -> str:\n        \"\"\"Formats the list of candidate\
        \ documents into a string for the prompt.\"\"\"\n        formatted_list =\
        \ []\n        for i, candidate in enumerate(candidates):\n            term_id\
        \ = candidate.get(\"id\")\n            if not term_id:\n                continue\n\
        \n            details = self.retriever.get_term_details(term_id)\n       \
        \     if not details:\n                continue\n\n            label = details.get(\"\
        label\", \"N/A\")\n            definition = details.get(\"definition\", \"\
        No definition available.\")\n            synonyms = \"; \".join(details.get(\"\
        synonyms\", [])) or \"None\"\n\n            formatted_list.append(\n     \
        \           f\"{i+1}. ID: {term_id}\\n\"\n                f\"   Label: {label}\\\
        n\"\n                f\"   Definition: {definition}\\n\"\n               \
        \ f\"   Synonyms: {synonyms}\"\n            )\n        return \"\\n\\n\".join(formatted_list)\n\
        \        \n    def _parse_and_validate_response(self, response_text: str)\
        \ -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Parses the JSON string\
        \ from the LLM and validates its structure.\n        \n        Args:\n   \
        \         response_text (str): The raw string content from the LLM, expected\
        \ to be JSON.\n\n        Returns:\n            A validated dictionary or None\
        \ if parsing or validation fails.\n        \"\"\"\n        try:\n        \
        \    # Clean up potential markdown code blocks around the JSON\n         \
        \   cleaned_response = response_text.strip().lstrip(\"```json\").rstrip(\"\
        ```\").strip()\n            result = json.loads(cleaned_response)\n\n    \
        \        # --- Centralized Validation Logic ---\n            if \"chosen_id\"\
        \ not in result or result.get(\"chosen_id\") is None:\n                logger.error(\n\
        \                    \"LLM response is invalid: Missing the mandatory 'chosen_id'\
        \ key. Response: %s\",\n                    result\n                )\n  \
        \              return None\n            \n            validated_result = {'chosen_id':\
        \ str(result['chosen_id'])}\n\n            if 'explanation' in result:\n \
        \               validated_result['explanation'] = result['explanation']\n\
        \            else:\n                logger.warning(\"LLM response missing\
        \ 'explanation' key. Using default value.\")\n                validated_result['explanation']\
        \ = 'No explanation provided.'\n\n            '''if 'confidence_score' in\
        \ result:\n                try:\n                    validated_result['confidence_score']\
        \ = float(result['confidence_score'])\n                except (ValueError,\
        \ TypeError):\n                    logger.warning(\n                     \
        \   f\"Invalid confidence_score value in response: '{result.get('confidence_score')}'.\
        \ Defaulting to 0.0.\"\n                    )\n                    validated_result['confidence_score']\
        \ = 0.0\n            else:\n                logger.warning(\"LLM response\
        \ missing 'confidence_score' key. Defaulting to 0.0.\")\n                validated_result['confidence_score']\
        \ = 0.0'''\n\n            return validated_result\n            \n        except\
        \ json.JSONDecodeError:\n            logger.error(f\"Failed to decode JSON\
        \ from LLM response: {response_text}\")\n            return None\n\n    @abstractmethod\n\
        \    def _call_llm(self, prompt: str, query: str) -> Optional[str]:\n    \
        \    \"\"\"\n        Makes the actual API call to the specific LLM provider.\n\
        \n        This method must be implemented by subclasses. It should handle\n\
        \        provider-specific API calls, authentication, and error handling.\n\
        \n        Args:\n            prompt (str): The fully formatted prompt to send\
        \ to the LLM.\n            query (str): The original user query, for logging\
        \ purposes.\n\n        Returns:\n            The raw string content of the\
        \ LLM's response, or None on failure.\n        \"\"\"\n        pass\n\n  \
        \  def select_best_term(self, query: str, candidates: List[Dict[str, Any]])\
        \ -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Selects the best term\
        \ by executing the full selection logic.\n\n        Args:\n            query\
        \ (str): The original user query.\n            candidates (List[Dict[str,\
        \ Any]]): The list of candidate documents.\n\n        Returns:\n         \
        \   A dictionary with the selection details, or None on failure.\n       \
        \ \"\"\"\n        if not candidates:\n            return None\n\n        candidate_str\
        \ = self._format_candidates_for_prompt(candidates)\n        prompt = self.prompt_template.replace(\"\
        [USER_ENTITY]\", query).replace(\"[CANDIDATE_LIST]\", candidate_str)\n   \
        \     \n        # Delegate the provider-specific call to the subclass\n  \
        \      response_text = self._call_llm(prompt, query)\n        \n        if\
        \ response_text is None:\n            return None\n        \n        # Use\
        \ the centralized parsing and validation method\n        return self._parse_and_validate_response(response_text)"
      "gemini_selector.py": "# src/rag_selectors/gemini_selector.py\nimport logging\n\
        from typing import Optional\n\nfrom google import genai\nfrom google.api_core\
        \ import exceptions\n\nfrom src.rag_selectors.base_selector import BaseSelector\n\
        from src.retriever.hybrid_retriever import HybridRetriever\nfrom src import\
        \ config\n\nlogger = logging.getLogger(__name__)\n\nclass GeminiSelector(BaseSelector):\n\
        \    \"\"\"\n    Uses the Google Gemini model to select the best ontology\
        \ term.\n    \"\"\"\n    def __init__(self, retriever: HybridRetriever):\n\
        \        \"\"\"\n        Initializes the GeminiSelector.\n\n        Args:\n\
        \            retriever (HybridRetriever): An initialized retriever instance.\n\
        \        \"\"\"\n        model_name = config.GEMINI_SELECTOR_MODEL_NAME\n\
        \        super().__init__(retriever=retriever, model_name=model_name)\n  \
        \      \n        if not config.GEMINI_API_KEY:\n            raise ValueError(\"\
        GEMINI_API_KEY not found in environment variables.\")\n        \n        self.client\
        \ = genai.Client(api_key=config.GEMINI_API_KEY)\n\n    def _call_llm(self,\
        \ prompt: str, query: str) -> Optional[str]:\n        \"\"\"\n        Makes\
        \ the API call to the Gemini model.\n        \"\"\"\n        logger.info(f\"\
        Sending request to Gemini for query: '{query}'\")\n        try:\n        \
        \    generation_config = {'temperature': 0}\n            response = self.client.models.generate_content(\n\
        \                model=self.model_name,\n                contents=prompt,\n\
        \                config=generation_config\n            )\n\n            feedback\
        \ = getattr(response, 'prompt_feedback', None)\n            if feedback and\
        \ any(r.blocked for r in feedback.safety_ratings or []):\n               \
        \ logger.warning(f\"Request for query '{query}' was blocked by safety filters.\"\
        )\n                return None\n\n            return response.text\n     \
        \           \n        except exceptions.GoogleAPIError as e:\n           \
        \ logger.error(f\"A Google API error occurred with the Gemini call: {e}\"\
        , exc_info=True)\n            return None\n        except Exception as e:\n\
        \            logger.error(f\"An unexpected error occurred with the Gemini\
        \ API call: {e}\", exc_info=True)\n            return None"
      "ollama_selector.py": "# src/rag_selectors/ollama_selector.py\nimport logging\n\
        from typing import Optional\n\nimport ollama\n\nfrom src.rag_selectors.base_selector\
        \ import BaseSelector\nfrom src.retriever.hybrid_retriever import HybridRetriever\n\
        from src import config\n\nlogger = logging.getLogger(__name__)\n\nclass OllamaSelector(BaseSelector):\n\
        \    \"\"\"\n    Uses a local LLM via Ollama to select the best ontology term.\n\
        \    \"\"\"\n    def __init__(self, retriever: HybridRetriever):\n       \
        \ \"\"\"\n        Initializes the OllamaSelector.\n\n        Args:\n     \
        \       retriever (HybridRetriever): An initialized retriever instance.\n\
        \        \"\"\"\n        model_name = config.OLLAMA_SELECTOR_MODEL_NAME\n\
        \        super().__init__(retriever=retriever, model_name=model_name)\n  \
        \      \n        try:\n            ollama.ps()\n            logger.info(\"\
        Ollama service is running.\")\n        except Exception as exc:\n        \
        \    logger.error(\"Ollama service not detected. Please ensure Ollama is running.\"\
        )\n            raise ConnectionError(\"Ollama service not available.\") from\
        \ exc\n\n    def _call_llm(self, prompt: str, query: str) -> Optional[str]:\n\
        \        \"\"\"\n        Makes the API call to the Ollama service.\n     \
        \   \"\"\"\n        logger.info(f\"Sending request to Ollama for query: '{query}'\
        \ with model '{self.model_name}'\")\n        try:\n            response =\
        \ ollama.chat(\n                model=self.model_name,\n                messages=[\n\
        \                    {\n                        'role': 'user',\n        \
        \                'content': prompt,\n                    },\n            \
        \    ],\n                format='json',\n                # Add the options\
        \ dictionary to control model parameters\n                options={\n    \
        \                'temperature': 0.0\n                }\n            )\n  \
        \          return response['message']['content']\n                \n     \
        \   except ollama.ResponseError as e:\n            logger.error(f\"An error\
        \ occurred with the Ollama API call: {e.status_code} - {e.error}\")\n    \
        \        return None\n        except (ConnectionError, TimeoutError) as e:\n\
        \            logger.error(f\"Network error during the Ollama call: {e}\",\
        \ exc_info=True)\n            return None\n        except Exception as e:\n\
        \            logger.error(f\"An unexpected error occurred during the Ollama\
        \ call: {e}\", exc_info=True)\n            return None"
    "reranker":
      "__init__.py": |-
        # src/reranker/__init__.py
        # This file makes Python treat the directory 'reranker' as a package.

        from .llm_reranker import LLMReranker

        __all__ = ["LLMReranker"]
      "llm_reranker.py": "# src/reranker/llm_reranker.py\n\nimport json\nimport logging\n\
        from typing import List, Dict, Optional, Tuple, Any, Union\n\nfrom sentence_transformers\
        \ import CrossEncoder\nimport torch\n\nlogger = logging.getLogger(__name__)\n\
        if not logger.hasHandlers():\n    logging.basicConfig(level=logging.INFO,\
        \ format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\nclass\
        \ LLMReranker:\n    \"\"\"\n    Reranks documents using a CrossEncoder model.\
        \ It loads enriched documents\n    from one or more files and uses them to\
        \ rerank candidate documents for a given query.\n    \"\"\"\n\n    def __init__(self,\n\
        \                 model_name: str,\n                 enriched_docs_paths:\
        \ Union[str, List[str]],\n                 device: Optional[str] = None):\n\
        \        \"\"\"\n        Initializes the LLMReranker.\n\n        Args:\n \
        \           model_name (str): The name of the CrossEncoder model to use.\n\
        \            enriched_docs_paths (Union[str, List[str]]): A single path or\
        \ a list of paths\n                                                      \
        \   to JSON files containing enriched documents.\n            device (Optional[str]):\
        \ The device to run the model on (\"cuda\", \"cpu\").\n        \"\"\"\n  \
        \      self.model_name = model_name\n        self.enriched_docs_paths = enriched_docs_paths\n\
        \        self.device = device or (\"cuda\" if torch.cuda.is_available() else\
        \ \"cpu\")\n\n        try:\n            logger.info(f\"Loading CrossEncoder\
        \ model: {self.model_name} on {self.device}\")\n            self.model = CrossEncoder(self.model_name,\
        \ device=self.device, trust_remote_code=True)\n            model_max_length\
        \ = getattr(self.model.tokenizer, 'model_max_length', 512)\n            if\
        \ model_max_length > 10000: # Handle unrealistic default values\n        \
        \        model_max_length = 512\n            self.model.max_length = model_max_length\n\
        \            logger.info(f\"Set CrossEncoder max_length to: {self.model.max_length}\"\
        )\n        except Exception as e:\n            logger.error(f\"Failed to load\
        \ CrossEncoder model {self.model_name}: {e}\", exc_info=True)\n          \
        \  raise\n\n        self.doc_texts = self._load_enriched_documents()\n   \
        \     if not self.doc_texts:\n            logger.error(f\"No documents loaded\
        \ from {self.enriched_docs_paths}. Reranker will be ineffective.\")\n\n  \
        \  def _load_enriched_documents(self) -> Dict[str, str]:\n        \"\"\"\n\
        \        Loads enriched documents from one or more files and creates a single\n\
        \        mapping from document ID to text.\n\n        Returns:\n         \
        \   Dict[str, str]: A dictionary mapping document CURIEs to their enriched\
        \ text.\n        \"\"\"\n        doc_map = {}\n        paths_to_load = self.enriched_docs_paths\n\
        \        if isinstance(paths_to_load, str):\n            paths_to_load = [paths_to_load]\n\
        \n        for path in paths_to_load:\n            try:\n                with\
        \ open(path, 'r', encoding='utf-8') as f:\n                    enriched_data\
        \ = json.load(f)\n                \n                for item in enriched_data:\n\
        \                    if \"id\" in item and \"text\" in item:\n           \
        \             doc_map[item[\"id\"]] = item[\"text\"]\n            except FileNotFoundError:\n\
        \                logger.error(f\"Enriched documents file not found: {path}\"\
        )\n            except json.JSONDecodeError:\n                logger.error(f\"\
        Error decoding JSON from {path}\")\n            except Exception as e:\n \
        \               logger.error(f\"An unexpected error occurred while loading\
        \ {path}: {e}\", exc_info=True)\n        \n        logger.info(f\"Loaded a\
        \ total of {len(doc_map)} enriched documents from {len(paths_to_load)} file(s).\"\
        )\n        return doc_map\n\n    def rerank(self, query: str, documents: List[Dict[str,\
        \ Any]], top_n: Optional[int] = None) -> List[Dict[str, Any]]:\n        \"\
        \"\"\n        Reranks a list of documents based on their relevance to a query.\n\
        \        (This method requires no changes as it uses the pre-loaded self.doc_texts\
        \ map).\n        \"\"\"\n        if not query or not documents:\n        \
        \    return documents[:top_n] if top_n is not None else documents\n\n    \
        \    if not self.doc_texts:\n            logger.error(\"No enriched document\
        \ texts loaded. Cannot perform reranking.\")\n            return documents[:top_n]\
        \ if top_n is not None else documents\n\n        sentence_pairs: List[Tuple[str,\
        \ str]] = []\n        valid_documents_for_reranking: List[Dict[str, Any]]\
        \ = []\n\n        for doc in documents:\n            doc_id = doc.get(\"id\"\
        )\n            if doc_id:\n                doc_text = self.doc_texts.get(doc_id)\n\
        \                if doc_text:\n                    sentence_pairs.append((query,\
        \ doc_text))\n                    valid_documents_for_reranking.append(doc)\n\
        \                else:\n                    logger.warning(f\"Could not find\
        \ enriched text for document ID '{doc_id}'. Skipping for reranking.\")\n \
        \       \n        if not sentence_pairs:\n            logger.warning(\"No\
        \ valid document texts found for the given candidates.\")\n            return\
        \ []\n\n        logger.info(f\"Reranking {len(sentence_pairs)} document(s)\
        \ for query: '{query}'\")\n        scores = self.model.predict(sentence_pairs,\
        \ show_progress_bar=False, batch_size=32)\n\n        for i, doc in enumerate(valid_documents_for_reranking):\n\
        \            doc[\"rerank_score\"] = float(scores[i])\n\n        reranked_documents\
        \ = sorted(valid_documents_for_reranking, key=lambda x: x.get(\"rerank_score\"\
        , -float('inf')), reverse=True)\n\n        return reranked_documents[:top_n]\
        \ if top_n is not None else reranked_documents\n\n# --- Updated Example Usage\
        \ ---\nif __name__ == '__main__':\n    from pathlib import Path\n    import\
        \ random\n\n    PROJECT_ROOT_FOR_MAIN = Path(__file__).resolve().parent.parent.parent\n\
        \    if str(PROJECT_ROOT_FOR_MAIN) not in sys.path:\n        sys.path.append(str(PROJECT_ROOT_FOR_MAIN))\n\
        \n    try:\n        from src.config import ONTOLOGIES_CONFIG, RERANKER_MODEL_NAME,\
        \ EMBEDDING_DEVICE, DEFAULT_RERANK_K\n    except ImportError as e:\n     \
        \   logger.error(f\"Error importing from src.config: {e}. Please ensure config\
        \ is set up.\", exc_info=True)\n        sys.exit(1)\n\n    logger.info(\"\
        --- Running LLMReranker Example with Multi-Ontology Setup ---\")\n\n    #\
        \ --- Collect all enriched doc paths from config ---\n    all_doc_paths =\
        \ [\n        data['enriched_docs_path'] \n        for data in ONTOLOGIES_CONFIG.values()\
        \ \n        if \"enriched_docs_path\" in data and os.path.exists(data['enriched_docs_path'])\n\
        \    ]\n\n    if not all_doc_paths:\n        logger.error(\"No existing enriched\
        \ document files found based on ONTOLOGIES_CONFIG. Please run ingestion first.\"\
        )\n        sys.exit(1)\n\n    logger.info(f\"Found {len(all_doc_paths)} enriched\
        \ document file(s) to load.\")\n\n    # --- Load sample documents from the\
        \ first available file for the demo ---\n    sample_documents_from_retriever\
        \ = []\n    try:\n        with open(all_doc_paths[0], 'r', encoding='utf-8')\
        \ as f:\n            sample_data = json.load(f)\n        \n        num_to_sample\
        \ = min(len(sample_data), 5)\n        for i in range(num_to_sample):\n   \
        \         doc = sample_data[i]\n            sample_documents_from_retriever.append({\n\
        \                \"id\": doc.get(\"id\"),\n                \"label\": doc.get(\"\
        label\", \"N/A\"),\n                \"retriever_score\": random.uniform(0.5,\
        \ 1.0)\n            })\n        logger.info(f\"Created {len(sample_documents_from_retriever)}\
        \ sample documents for the demo from '{all_doc_paths[0]}'.\")\n    except\
        \ Exception as e:\n        logger.error(f\"Failed to create sample documents:\
        \ {e}\", exc_info=True)\n        sys.exit(1)\n\n    # --- Reranker Initialization\
        \ and Usage ---\n    try:\n        reranker = LLMReranker(\n            model_name=RERANKER_MODEL_NAME,\n\
        \            enriched_docs_paths=all_doc_paths, # Pass the list of paths\n\
        \            device=EMBEDDING_DEVICE\n        )\n\n        query = \"Garlic\"\
        \n        logger.info(f\"\\nOriginal sample documents for query '{query}':\"\
        )\n        for doc in sample_documents_from_retriever:\n            logger.info(f\"\
        \  ID: {doc.get('id', 'N/A')}, Label: {doc.get('label', 'N/A')}\")\n\n   \
        \     reranked_results = reranker.rerank(query, sample_documents_from_retriever,\
        \ top_n=DEFAULT_RERANK_K)\n\n        logger.info(f\"\\nReranked documents\
        \ (top {DEFAULT_RERANK_K}):\")\n        for doc in reranked_results:\n   \
        \         logger.info(f\"  ID: {doc.get('id')}, Label: {doc.get('label')},\
        \ Rerank Score: {doc.get('rerank_score', 0.0):.4f}\")\n\n    except Exception\
        \ as e:\n        logger.error(f\"An error occurred in the main example: {e}\"\
        , exc_info=True)\n\n    logger.info(\"--- LLMReranker Example Finished ---\"\
        )"
    "retriever":
      "__init__.py": ""
      "hybrid_retriever.py": "# src/retriever/hybrid_retriever.py\nimport json\nfrom\
        \ pathlib import Path\nfrom whoosh.index import open_dir as open_whoosh_index\n\
        from whoosh.qparser import MultifieldParser, OrGroup\nfrom sentence_transformers\
        \ import SentenceTransformer\nimport logging\nfrom typing import List, Optional,\
        \ Dict, Any\n\n# --- Add project root to sys.path if running script directly\
        \ ---\nif __name__ == '__main__':\n    import sys\n    # Use pathlib to determine\
        \ the project root and add it to the path\n    PROJECT_ROOT_FOR_DIRECT_RUN\
        \ = Path(__file__).resolve().parent.parent\n    if str(PROJECT_ROOT_FOR_DIRECT_RUN)\
        \ not in sys.path:\n        sys.path.insert(0, str(PROJECT_ROOT_FOR_DIRECT_RUN))\n\
        # --- End sys.path modification ---\n\nfrom src.vector_store.faiss_store import\
        \ FAISSVectorStore\nfrom src.config import (\n    ONTOLOGIES_CONFIG,\n   \
        \ EMBEDDING_MODEL_NAME,\n    DEFAULT_K_LEXICAL,\n    DEFAULT_K_VECTOR,\n)\n\
        \nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s\
        \ - %(name)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass\
        \ HybridRetriever:\n    def __init__(self):\n        \"\"\"\n        Initializes\
        \ the HybridRetriever to work with multiple, separate ontologies\n       \
        \ defined in ONTOLOGIES_CONFIG.\n        \"\"\"\n        logger.info(\"Initializing\
        \ HybridRetriever for multiple ontologies...\")\n        \n        self.ontology_data_stores:\
        \ Dict[str, Dict[str, Any]] = {}\n        self.whoosh_searchers: Dict[str,\
        \ Any] = {}\n        self.whoosh_parsers: Dict[str, MultifieldParser] = {}\n\
        \        self.faiss_stores: Dict[str, FAISSVectorStore] = {}\n        self.ontology_names\
        \ = list(ONTOLOGIES_CONFIG.keys())\n        self.prefix_to_name_map = {v['prefix']:\
        \ k for k, v in ONTOLOGIES_CONFIG.items()}\n\n        logger.info(f\"Loading\
        \ embedding model: {EMBEDDING_MODEL_NAME}\")\n        self.embedding_model\
        \ = SentenceTransformer(EMBEDDING_MODEL_NAME, trust_remote_code=True)\n  \
        \      \n        for name, config_data in ONTOLOGIES_CONFIG.items():\n   \
        \         logger.info(f\"--- Initializing resources for ontology: '{name}'\
        \ ---\")\n            \n            # Load ontology data dump using pathlib.Path\n\
        \            dump_path = config_data['dump_json_path']\n            logger.info(f\"\
        Loading ontology data from: {dump_path}\")\n            if not dump_path.exists():\n\
        \                raise FileNotFoundError(f\"Ontology dump for '{name}' not\
        \ found: {dump_path}. Please run ingestion scripts.\")\n            with dump_path.open('r',\
        \ encoding='utf-8') as f:\n                self.ontology_data_stores[name]\
        \ = json.load(f)\n            logger.info(f\"Loaded {len(self.ontology_data_stores[name])}\
        \ entries for '{name}'.\")\n\n            # Load Whoosh index using pathlib.Path\n\
        \            whoosh_dir = config_data['whoosh_index_dir']\n            logger.info(f\"\
        Loading Whoosh index from: {whoosh_dir}\")\n            # Check if directory\
        \ exists and is not empty\n            if not whoosh_dir.is_dir() or not any(whoosh_dir.iterdir()):\n\
        \                raise FileNotFoundError(f\"Whoosh index for '{name}' not\
        \ found or empty: {whoosh_dir}. Please run ingestion scripts.\")\n       \
        \     \n            # whoosh.index.open_dir expects a string path\n      \
        \      whoosh_ix = open_whoosh_index(str(whoosh_dir))\n            self.whoosh_searchers[name]\
        \ = whoosh_ix.searcher()\n            whoosh_fields = [\"label\", \"synonyms\"\
        , \"definition\", \"relations_text\"]\n            self.whoosh_parsers[name]\
        \ = MultifieldParser(whoosh_fields, schema=whoosh_ix.schema, group=OrGroup)\n\
        \            logger.info(f\"Whoosh index for '{name}' loaded.\")\n\n     \
        \       # Initialize FAISS store using pathlib.Path\n            faiss_index_path\
        \ = config_data['faiss_index_path']\n            faiss_metadata_path = config_data['faiss_metadata_path']\n\
        \            logger.info(f\"Initializing FAISS store for '{name}' (index:\
        \ {faiss_index_path}, metadata: {faiss_metadata_path})...\")\n           \
        \ \n            # Assuming FAISSVectorStore constructor accepts pathlib.Path\
        \ objects\n            faiss_store = FAISSVectorStore(\n                index_path=faiss_index_path,\n\
        \                metadata_path=faiss_metadata_path,\n                embeddings_file_path=None\
        \ # Not needed for loading\n            )\n            if not faiss_store.index\
        \ or not faiss_store.metadata:\n                 raise FileNotFoundError(f\"\
        FAISS index or metadata for '{name}' not found at {faiss_index_path}. Please\
        \ build it first.\")\n            self.faiss_stores[name] = faiss_store\n\
        \            logger.info(f\"FAISS store for '{name}' initialized.\")\n\n \
        \       logger.info(\"HybridRetriever initialized successfully for all configured\
        \ ontologies.\")\n\n    def _get_stores_to_query(self, store_dict: dict, target_ontologies:\
        \ Optional[List[str]]):\n        \"\"\"Helper to select which stores (Whoosh/FAISS)\
        \ to query.\"\"\"\n        if target_ontologies is None:\n            # If\
        \ no specific targets, use all available stores\n            return store_dict.items()\n\
        \        \n        # Filter to only the targeted stores that actually exist\n\
        \        stores_to_query = []\n        for name in target_ontologies:\n  \
        \          if name in store_dict:\n                stores_to_query.append((name,\
        \ store_dict[name]))\n            else:\n                logger.warning(f\"\
        Requested ontology '{name}' not found in available stores. It will be skipped.\"\
        )\n        return stores_to_query\n\n    def _lexical_search(self, query_string:\
        \ str, limit: int = DEFAULT_K_LEXICAL, target_ontologies: Optional[List[str]]\
        \ = None):\n        \"\"\"Performs lexical search on all or a subset of Whoosh\
        \ indexes.\"\"\"\n        all_results = []\n        if not query_string:\n\
        \            return all_results\n\n        searchers_to_query = self._get_stores_to_query(self.whoosh_searchers,\
        \ target_ontologies)\n        if not searchers_to_query:\n            logger.warning(\"\
        Lexical search: No valid target ontologies specified or found.\")\n      \
        \      return []\n\n        for name, searcher in searchers_to_query:\n  \
        \          try:\n                parser = self.whoosh_parsers[name]\n    \
        \            query = parser.parse(query_string)\n                search_results\
        \ = searcher.search(query, limit=limit)\n                \n              \
        \  for hit in search_results:\n                    hit_fields = hit.fields()\n\
        \                    term_curie = hit_fields.get('id') # Corrected field name\
        \ to 'id' as per typical schema\n                    if not term_curie: continue\n\
        \                    \n                    all_results.append({\n        \
        \                \"id\": term_curie, \"label\": hit_fields.get('label', 'N/A'),\n\
        \                        \"score\": hit.score, \"source\": \"lexical\", \"\
        source_ontology\": name,\n                    })\n            except Exception\
        \ as e:\n                logger.error(f\"Error during lexical search in '{name}'\
        \ for '{query_string}': {e}\", exc_info=True)\n        \n        all_results.sort(key=lambda\
        \ x: x['score'], reverse=True)\n        return all_results[:limit]\n\n   \
        \ def _vector_search(self, query_string: str, k: int = DEFAULT_K_VECTOR, target_ontologies:\
        \ Optional[List[str]] = None):\n        \"\"\"Performs vector search on all\
        \ or a subset of FAISS indexes.\"\"\"\n        all_results = []\n        if\
        \ not query_string:\n            return all_results\n\n        stores_to_query\
        \ = self._get_stores_to_query(self.faiss_stores, target_ontologies)\n    \
        \    if not stores_to_query:\n            logger.warning(\"Vector search:\
        \ No valid target ontologies specified or found.\")\n            return []\n\
        \n        try:\n            query_vector = self.embedding_model.encode([query_string],\
        \ convert_to_numpy=True)\n            \n            for name, store in stores_to_query:\n\
        \                distances, _, metadata_items = store.search(query_vector,\
        \ k=k)\n                for i, item in enumerate(metadata_items):\n      \
        \              all_results.append({\n                        \"id\": item['id'],\
        \ \"label\": item['label'], \"score\": float(distances[i]),\n            \
        \            \"source\": \"vector\", \"source_ontology\": name,\n        \
        \            })\n        except Exception as e:\n            logger.error(f\"\
        Error during vector search for '{query_string}': {e}\", exc_info=True)\n\n\
        \        all_results.sort(key=lambda x: x['score'])\n        return all_results[:k]\n\
        \n    def search(self, query_string: str, lexical_limit: int = DEFAULT_K_LEXICAL,\
        \ vector_k: int = DEFAULT_K_VECTOR, target_ontologies: Optional[List[str]]\
        \ = None):\n        \"\"\"\n        Performs hybrid search on all or a targeted\
        \ subset of ontologies.\n\n        Args:\n            query_string (str):\
        \ The search query.\n            lexical_limit (int): Max number of results\
        \ from lexical search.\n            vector_k (int): Max number of results\
        \ from vector search.\n            target_ontologies (List[str], optional):\
        \ A list of ontology names to search.\n                                  \
        \                   If None, searches all ontologies.\n        \"\"\"\n  \
        \      lexical_results = self._lexical_search(query_string, limit=lexical_limit,\
        \ target_ontologies=target_ontologies)\n        vector_results = self._vector_search(query_string,\
        \ k=vector_k, target_ontologies=target_ontologies)\n        \n        return\
        \ {\n            \"query\": query_string,\n            \"lexical_results\"\
        : lexical_results,\n            \"vector_results\": vector_results,\n    \
        \    }\n\n    def get_term_details(self, term_id: str) -> Optional[Dict[str,\
        \ Any]]:\n        \"\"\"\n        Retrieves the full details for a given term\
        \ ID by finding which ontology it belongs to.\n        \"\"\"\n        matched_prefix\
        \ = None\n        # Find the longest matching prefix to handle overlapping\
        \ namespaces (e.g., 'obo' and 'obo:FOODON_')\n        for prefix in self.prefix_to_name_map.keys():\n\
        \            if term_id.startswith(prefix):\n                if matched_prefix\
        \ is None or len(prefix) > len(matched_prefix):\n                    matched_prefix\
        \ = prefix\n        \n        if not matched_prefix:\n            logger.warning(f\"\
        Could not determine ontology for term_id '{term_id}'.\")\n            return\
        \ None\n            \n        ontology_name = self.prefix_to_name_map[matched_prefix]\n\
        \        term_data = self.ontology_data_stores.get(ontology_name, {}).get(term_id)\n\
        \        \n        if term_data:\n            # Return a copy to prevent modification\
        \ of the original loaded data\n            term_data = dict(term_data)\n \
        \           term_data['id'] = term_id\n        return term_data\n\n    def\
        \ close(self):\n        \"\"\"Closes any open resources, such as Whoosh searchers.\"\
        \"\"\n        for name, searcher in self.whoosh_searchers.items():\n     \
        \       if searcher:\n                searcher.close()\n                logger.info(f\"\
        Whoosh searcher for '{name}' closed.\")\n\n# --- The example usage block remains\
        \ unchanged but now works with the refactored code ---\nif __name__ == '__main__':\n\
        \    logger.info(\"Running HybridRetriever example...\")\n    retriever =\
        \ None\n    try:\n        retriever = HybridRetriever()\n        \n      \
        \  print(\"\\n\\n\" + \"=\"*80)\n        print(\"✅ 1. Searching for 'cheese'\
        \ across ALL ontologies (default behavior)\")\n        print(\"=\"*80)\n \
        \       results = retriever.search(\"cheese\", lexical_limit=2, vector_k=2)\n\
        \        print(json.dumps(results, indent=2))\n\n        print(\"\\n\\n\"\
        \ + \"=\"*80)\n        print(\"✅ 2. Searching for 'cheese' ONLY in the 'foodon'\
        \ ontology\")\n        print(\"=\"*80)\n        results_foodon = retriever.search(\"\
        cheese\", lexical_limit=2, vector_k=2, target_ontologies=[\"foodon\"])\n \
        \       print(json.dumps(results_foodon, indent=2))\n        \n        print(\"\
        \\n\\n\" + \"=\"*80)\n        print(\"✅ 3. Searching for 'chemical entity'\
        \ ONLY in the 'chebi' ontology\")\n        print(\"=\"*80)\n        results_chebi\
        \ = retriever.search(\"chemical entity\", lexical_limit=2, vector_k=2, target_ontologies=[\"\
        chebi\"])\n        print(json.dumps(results_chebi, indent=2))\n\n        print(\"\
        \\n\\n\" + \"=\"*80)\n        print(\"❌ 4. Searching with an invalid ontology\
        \ name (should be skipped gracefully)\")\n        print(\"=\"*80)\n      \
        \  results_invalid = retriever.search(\"cheese\", lexical_limit=2, vector_k=2,\
        \ target_ontologies=[\"non_existent_ontology\"])\n        print(json.dumps(results_invalid,\
        \ indent=2))\n\n    except FileNotFoundError as e:\n        logger.error(f\"\
        \\nERROR: A required file was not found: {e}. Please ensure you have run the\
        \ data ingestion and indexing scripts first.\")\n    except Exception as e:\n\
        \        logger.error(f\"\\nAn unexpected error occurred during example run:\
        \ {e}\", exc_info=True)\n    finally:\n        if retriever:\n           \
        \ retriever.close()"
    "tests":
      "__init__.py": ""
      "test_lexical_index.py": |-
        import pytest
        import os
        import json
        import tempfile
        import shutil
        from whoosh.index import open_dir
        from whoosh.qparser import QueryParser

        # Adjust path to import modules from src
        import sys
        sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

        from ingestion.parse_ontology import main as parse_ontology_main # Need to run parsing first
        from ingestion.build_lexical_index import build_index
        import config # Need config to override paths

        # Define a known CURIE and search terms from the test snippet
        TEST_CURIE = "FOODON:00001100" # apple
        TEST_LABEL = "apple"
        TEST_SYNONYM = "eating apple"
        TEST_DEFINITION_KEYWORD = "fruit" # Keyword from definition "The fruit of a Malus domestica tree."
        TEST_RELATION_KEYWORD = "apple tree" # Keyword from relations_text (target label)


        # Fixture to create a temporary directory for the test index
        @pytest.fixture(scope="module")
        def temp_index_dir():
            """Creates a temporary directory for the Whoosh index."""
            path = tempfile.mkdtemp(prefix="whoosh_test_index_")
            print(f"Created temp index dir: {path}")
            yield path
            print(f"Cleaning up temp index dir: {path}")
            shutil.rmtree(path)

        # Fixture to run the full ingestion pipeline on the test snippet and build index
        @pytest.fixture(scope="module")
        def built_test_index(temp_index_dir):
            """Runs parse_ontology and build_index on the test snippet."""
            # Use temp files/dirs for test data and index
            with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as tmp_json:
                test_json_path = tmp_json.name

            # Temporarily override config paths for the test
            original_ontology_file = config.ONTOLOGY_FILE
            original_dump_path = config.ONTOLOGY_DUMP_PATH
            original_index_dir = config.WHOOSH_INDEX_DIR

            config.ONTOLOGY_FILE = config.TEST_ONTOLOGY_FILE # Use the small test snippet
            config.ONTOLOGY_DUMP_PATH = test_json_path
            config.WHOOSH_INDEX_DIR = temp_index_dir

            try:
                # 1. Run parsing
                print("\nRunning parse_ontology_main for test index build...")
                parse_ontology_main()
                assert os.path.exists(test_json_path), "Ontology dump JSON not created!"

                # 2. Run index building
                print("\nRunning build_index for test index build...")
                build_index(test_json_path, temp_index_dir)
                assert os.path.exists(os.path.join(temp_index_dir, 'SCHEMA')), "Whoosh index not created!"

                # Yield the index directory path
                yield temp_index_dir

            finally:
                # Restore original paths
                config.ONTOLOGY_FILE = original_ontology_file
                config.ONTOLOGY_DUMP_PATH = original_dump_path
                config.WHOOSH_INDEX_DIR = original_index_dir
                # Clean up temporary JSON file
                if os.path.exists(test_json_path):
                    os.unlink(test_json_path)


        def test_index_exists(built_test_index):
            """Tests if the index directory was created and contains index files."""
            assert os.path.exists(built_test_index)
            assert os.path.exists(os.path.join(built_test_index, 'SCHEMA'))
            assert os.path.exists(os.path.join(built_test_index, 'MAIN')) # Or other index files

        def test_query_by_label_exact(built_test_index):
            """Tests exact search on the label field."""
            ix = open_dir(built_test_index)
            searcher = ix.searcher()
            # Query the 'label' field
            parser = QueryParser("label", ix.schema)
            query = parser.parse(TEST_LABEL) # "apple"
            results = searcher.search(query)

            print(f"\nSearch results for '{TEST_LABEL}': {results}")

            assert len(results) > 0, f"No results found for '{TEST_LABEL}'"
            # Check if the expected CURIE is among the results, preferably the top one
            assert results[0]['curie'] == TEST_CURIE
            assert results[0]['label'] == TEST_LABEL # Verify stored label

            searcher.close()
            ix.close() # Close index readers/writers

        def test_query_by_label_fuzzy(built_test_index):
            """Tests fuzzy search on the label field."""
            ix = open_dir(built_test_index)
            searcher = ix.searcher()
            parser = QueryParser("label", ix.schema)
            # Fuzzy search for "appl" (e.g. "appl~")
            query = parser.parse("appl~") # Whoosh fuzzy syntax
            results = searcher.search(query)

            print(f"\nFuzzy search results for 'appl~': {results}")

            assert len(results) > 0, "No results found for 'appl~'"
            assert results[0]['curie'] == TEST_CURIE
            assert results[0]['label'] == TEST_LABEL

            searcher.close()
            ix.close()

        def test_query_by_synonym(built_test_index):
            """Tests search on the synonyms field."""
            ix = open_dir(built_test_index)
            searcher = ix.searcher()
            # Query the 'synonyms' field
            parser = QueryParser("synonyms", ix.schema)
            query = parser.parse(TEST_SYNONYM) # "eating apple"
            results = searcher.search(query)

            print(f"\nSearch results for synonym '{TEST_SYNONYM}': {results}")

            assert len(results) > 0, f"No results found for synonym '{TEST_SYNONYM}'"
            assert results[0]['curie'] == TEST_CURIE

            searcher.close()
            ix.close()

        def test_query_by_definition_keyword(built_test_index):
            """Tests search on the definition field."""
            ix = open_dir(built_test_index)
            searcher = ix.searcher()
            # Query the 'definition' field
            parser = QueryParser("definition", ix.schema)
            query = parser.parse(TEST_DEFINITION_KEYWORD) # "fruit"
            results = searcher.search(query)

            print(f"\nSearch results for definition keyword '{TEST_DEFINITION_KEYWORD}': {results}")

            assert len(results) > 0, f"No results found for definition keyword '{TEST_DEFINITION_KEYWORD}'"
            assert results[0]['curie'] == TEST_CURIE

            searcher.close()
            ix.close()

        def test_query_by_relations_keyword(built_test_index):
            """Tests search on the flattened relations_text field."""
            ix = open_dir(built_test_index)
            searcher = ix.searcher()
            # Query the 'relations_text' field
            parser = QueryParser("relations_text", ix.schema)
            # Search for a keyword expected in the relations_text (e.g., target's label or part of target's CURIE)
            # Note: Indexing targets as just CURIEs requires searching for 'FOODON:00001101'
            # Indexing target labels requires adding that logic in build_lexical_index
            # Let's assume for now we search for the CURIE or part of it if indexed as text
            # Or if we enhance build_index to add target labels to relations_text:
            # relations_text = " ".join([f"{prop}: {' '.join(targets)} {' '.join(target_labels)}" for ...])
            # For simplicity with current build_index, let's search the target CURIE part
            query = parser.parse("00001101") # Search part of the target CURIE FOODON:00001101
            results = searcher.search(query)

            print(f"\nSearch results for relations keyword '00001101': {results}")

            assert len(results) > 0, f"No results found for relations keyword '00001101'"
            assert results[0]['curie'] == TEST_CURIE

            searcher.close()
            ix.close()
      "test_parse_ontology.py": |-
        import pytest
        import os
        import json
        import rdflib
        from rdflib import Graph

        # Adjust path to import modules from src
        import sys
        sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

        from ingestion.parse_ontology import (
            load_ontology,
            extract_labels_and_synonyms,
            extract_definitions,
            extract_hierarchy,
            extract_relations,
        )
        from config import TEST_ONTOLOGY_FILE, NAMESPACE_MAP, RELATION_PROPERTIES
        from utils.ontology_utils import uri_to_curie # Import if needed for assertions

        # Define a fixture to load the test graph once for all tests
        @pytest.fixture(scope="module")
        def test_graph():
            """Loads the test ontology snippet into a graph."""
            if not os.path.exists(TEST_ONTOLOGY_FILE):
                pytest.skip(f"Test ontology snippet not found at {TEST_ONTOLOGY_FILE}")
            try:
                return load_ontology(TEST_ONTOLOGY_FILE)
            except Exception as e:
                pytest.fail(f"Failed to load test ontology: {e}")

        # Define a known CURIE from the test snippet
        TEST_CURIE = "FOODON:00001100" # apple
        TEST_CURIE_PARENT1 = "FOODON:00001000" # plant-based food
        TEST_CURIE_PARENT2 = "FOODON:00002000" # Pome fruit
        TEST_CURIE_RELATION_TARGET = "FOODON:00001101" # apple tree (example target)
        TEST_RELATION_NAME = "part_of" # example relation name

        def test_load_ontology(test_graph):
            """Tests if the ontology loads and is an RDFLib Graph."""
            assert isinstance(test_graph, Graph)
            assert len(test_graph) > 0, "Test graph is empty!"
            print(f"Loaded test graph with {len(test_graph)} triples.")


        def test_extract_labels_and_synonyms(test_graph):
            """Tests extraction of labels and synonyms."""
            labels_synonyms = extract_labels_and_synonyms(test_graph)
            print(f"Extracted labels/synonyms: {labels_synonyms}")

            assert TEST_CURIE in labels_synonyms
            apple_data = labels_synonyms[TEST_CURIE]
            assert apple_data['label'] == "apple"
            assert "eating apple" in apple_data['synonyms']
            assert "apple fruit" in apple_data['synonyms']
            assert TEST_CURIE_PARENT1 in labels_synonyms # Should also extract its label/synonyms
            assert labels_synonyms[TEST_CURIE_PARENT1]['label'] == "plant-based food"


        def test_extract_definitions(test_graph):
            """Tests extraction of definitions."""
            definitions = extract_definitions(test_graph)
            print(f"Extracted definitions: {definitions}")

            assert TEST_CURIE in definitions
            assert "The fruit of a Malus domestica tree." in definitions[TEST_CURIE] # Use 'in' for substring check or exact match


        def test_extract_hierarchy(test_graph):
            """Tests extraction of hierarchy (parents and ancestors)."""
            hierarchy = extract_hierarchy(test_graph)
            print(f"Extracted hierarchy: {hierarchy}")

            assert TEST_CURIE in hierarchy
            apple_hierarchy = hierarchy[TEST_CURIE]

            # Check direct parents
            assert TEST_CURIE_PARENT1 in apple_hierarchy['parents']
            assert TEST_CURIE_PARENT2 in apple_hierarchy['parents']
            assert len(apple_hierarchy['parents']) >= 2 # At least 2 parents from the snippet

            # Check ancestors (should include parents and parents' ancestors)
            assert TEST_CURIE_PARENT1 in apple_hierarchy['ancestors']
            assert TEST_CURIE_PARENT2 in apple_hierarchy['ancestors']
            # Assuming plant-based food has no ancestors in this snippet, ancestors == parents here
            # If plant-based food had parents, they should appear in apple's ancestors.
            # For this snippet, ancestors should be the same as parents.
            assert set(apple_hierarchy['ancestors']) == set(apple_hierarchy['parents'])


        def test_extract_relations(test_graph):
            """Tests extraction of specific relations."""
            relations = extract_relations(test_graph, RELATION_PROPERTIES)
            print(f"Extracted relations: {relations}")

            assert TEST_CURIE in relations
            apple_relations = relations[TEST_CURIE]

            # Check if the specific relation from the snippet was found
            assert TEST_RELATION_NAME in apple_relations
            assert TEST_CURIE_RELATION_TARGET in apple_relations[TEST_RELATION_NAME]
            assert len(apple_relations[TEST_RELATION_NAME]) >= 1


        # You could add a test that runs the full main parsing flow using the test snippet
        # and checks the structure/content of the resulting JSON file.
        # Example (requires a temporary file):
        # import tempfile
        # @pytest.fixture
        # def temp_json_path():
        #     with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as tmp:
        #         yield tmp.name
        #     os.unlink(tmp.name)
        #
        # def test_main_parsing_flow(temp_json_path):
        #      # Temporarily override config paths for the test
        #      original_ontology_file = config.ONTOLOGY_FILE
        #      original_dump_path = config.ONTOLOGY_DUMP_PATH
        #      config.ONTOLOGY_FILE = TEST_ONTOLOGY_FILE
        #      config.ONTOLOGY_DUMP_PATH = temp_json_path
        #
        #      try:
        #          parse_ontology.main()
        #          assert os.path.exists(temp_json_path)
        #          with open(temp_json_path, 'r', encoding='utf-8') as f:
        #              data = json.load(f)
        #
        #          assert TEST_CURIE in data
        #          apple_data = data[TEST_CURIE]
        #          assert apple_data['label'] == 'apple'
        #          assert 'synonyms' in apple_data and len(apple_data['synonyms']) > 0
        #          assert 'definition' in apple_data and apple_data['definition'] is not None
        #          assert 'parents' in apple_data and len(apple_data['parents']) > 0
        #          assert 'ancestors' in apple_data and len(apple_data['ancestors']) > 0
        #          assert 'relations' in apple_data and len(apple_data['relations']) > 0
        #
        #      finally:
        #          # Restore original paths
        #          config.ONTOLOGY_FILE = original_ontology_file
        #          config.ONTOLOGY_DUMP_PATH = original_dump_path
    "utils":
      "__init__.py": ""
      "ontology_utils.py": "# src/utils/ontology_utils.py\nimport rdflib\nfrom rdflib\
        \ import URIRef, Namespace\nfrom typing import Optional, Dict, Union\n\n#\
        \ Adjusted: Import CURIE_PREFIX_MAP and use it as the default.\n# This assumes\
        \ that the script/module *importing* ontology_utils\n# has already ensured\
        \ that the project root (e.g., 'onto_rag') is on sys.path,\n# so that 'src'\
        \ is resolvable as a top-level package.\nfrom src.config import CURIE_PREFIX_MAP\
        \ # Corrected import\n\ndef uri_to_curie(uri: Union[str, URIRef], namespace_map:\
        \ Dict[str, str] = CURIE_PREFIX_MAP) -> str:\n    \"\"\"\n    Converts a full\
        \ URI to a CURIE (e.g., http://...#term -> PREFIX:term).\n    Assumes namespace_map\
        \ is structured as {base_uri_str: prefix_str}.\n    \"\"\"\n    uri_str =\
        \ str(uri) # Ensure it's a string\n\n    # Iterate through the provided namespace_map\
        \ (base_uri: prefix)\n    # Sort by length of base_uri descending to match\
        \ longest first (more specific)\n    # This helps avoid issues where one base_uri\
        \ is a prefix of another.\n    # e.g., \"http://purl.obolibrary.org/obo/\"\
        \ and \"http://purl.obolibrary.org/obo/FOODON_\"\n    sorted_namespace_map_items\
        \ = sorted(namespace_map.items(), key=lambda item: len(item[0]), reverse=True)\n\
        \n    for base_uri, prefix in sorted_namespace_map_items:\n        if uri_str.startswith(base_uri):\n\
        \            return f\"{prefix}:{uri_str[len(base_uri):]}\"\n\n    # Fallback\
        \ for common RDF/RDFS/OWL/XSD prefixes if not found in the main map\n    #\
        \ This local map is prefix: base_uri_str\n    common_rdf_prefixes = {\n  \
        \      \"rdf\": str(rdflib.RDF),\n        \"rdfs\": str(rdflib.RDFS),\n  \
        \      \"owl\": str(rdflib.OWL),\n        \"xsd\": str(rdflib.XSD),\n    }\n\
        \    for prefix, base_uri in common_rdf_prefixes.items():\n        if uri_str.startswith(base_uri):\n\
        \            return f\"{prefix}:{uri_str[len(base_uri):]}\"\n            \n\
        \    # If rdflib can make a qname (usually for registered namespaces)\n  \
        \  try:\n        g = rdflib.Graph()\n        # Bind known namespaces to help\
        \ compute_qname\n        # For namespace_map (base_uri: prefix), we need to\
        \ iterate as base_uri, prefix\n        for ns_uri_str_from_map, prefix_from_map\
        \ in namespace_map.items():\n             g.bind(prefix_from_map, Namespace(ns_uri_str_from_map))\n\
        \n        # Bind common RDF ones too, in case they weren't in namespace_map\
        \ or to ensure standard prefixes\n        g.bind(\"owl\", rdflib.OWL)\n  \
        \      g.bind(\"rdf\", rdflib.RDF)\n        g.bind(\"rdfs\", rdflib.RDFS)\n\
        \        g.bind(\"xsd\", rdflib.XSD)\n\n        # compute_qname might fail\
        \ if the URI doesn't match any bound namespace's base\n        # It returns\
        \ (prefix, namespace_uri, local_name)\n        qname_tuple = g.compute_qname(URIRef(uri_str))\n\
        \        return f\"{qname_tuple[0]}:{qname_tuple[2]}\"\n    except Exception:\
        \ # Broad except as compute_qname can raise various things or return unexpected\
        \ tuples\n        pass # If rdflib fails, just return the original URI string\n\
        \n    return uri_str # If no CURIE conversion possible, return original URI\
        \ string\n\ndef curie_to_uri(curie: str, namespace_map: Dict[str, str] = CURIE_PREFIX_MAP)\
        \ -> Optional[URIRef]:\n    \"\"\"\n    Converts a CURIE (e.g., PREFIX:term)\
        \ to a full rdflib.URIRef.\n    Assumes namespace_map is structured as {base_uri_str:\
        \ prefix_str}.\n    \"\"\"\n    if ':' not in curie:\n        # Try to see\
        \ if it's a default rdflib qname like \"rdf:type\" that rdflib can expand\n\
        \        # This part might be less common if CURIEs are always expected with\
        \ user-defined prefixes\n        try:\n            g = rdflib.Graph()\n  \
        \          # Bind namespaces from the map (base_uri: prefix)\n           \
        \ for ns_uri_str_from_map, prefix_from_map in namespace_map.items():\n   \
        \             g.bind(prefix_from_map, Namespace(ns_uri_str_from_map))\n  \
        \          # Bind common RDF ones\n            g.bind(\"owl\", rdflib.OWL)\n\
        \            g.bind(\"rdf\", rdflib.RDF)\n            g.bind(\"rdfs\", rdflib.RDFS)\n\
        \            g.bind(\"xsd\", rdflib.XSD)\n            \n            # If it's\
        \ something like \"owl:Class\", g.namespace_manager.expand_curie will work\n\
        \            expanded_uri = g.namespace_manager.expand_curie(curie)\n    \
        \        if str(expanded_uri) != curie: # Check if expansion actually happened\n\
        \                 return URIRef(expanded_uri)\n        except Exception:\n\
        \            pass # If expansion fails, proceed to manual lookup\n       \
        \ return None # Not a valid CURIE format for our map, and rdflib couldn't\
        \ expand\n\n    prefix_part, local_name = curie.split(':', 1)\n\n    # Iterate\
        \ through namespace_map (base_uri: prefix) to find the matching prefix\n \
        \   found_base_uri = None\n    for base_uri_key, prefix_val in namespace_map.items():\n\
        \        if prefix_val == prefix_part:\n            found_base_uri = base_uri_key\n\
        \            break\n    \n    if found_base_uri:\n        return URIRef(found_base_uri\
        \ + local_name)\n    else:\n        # Fallback: Try common RDF prefixes if\
        \ not in the custom map\n        # This local map is prefix: rdflib.Namespace\
        \ object\n        common_rdf_namespaces = {\n            \"rdf\": rdflib.RDF,\n\
        \            \"rdfs\": rdflib.RDFS,\n            \"owl\": rdflib.OWL,\n  \
        \          \"xsd\": rdflib.XSD,\n        }\n        if prefix_part in common_rdf_namespaces:\n\
        \            # Access items in the namespace like attributes: common_rdf_namespaces[prefix_part].type\n\
        \            # or by string concatenation: URIRef(str(common_rdf_namespaces[prefix_part])\
        \ + local_name)\n            return URIRef(str(common_rdf_namespaces[prefix_part])\
        \ + local_name)\n        return None # Prefix not found in custom map or common\
        \ RDF prefixes"
    "vector_store":
      "__init__.py": ""
      "faiss_store.py": "# src/vector_store/faiss_store.py\nimport faiss\nimport json\n\
        import numpy as np\nimport logging\nfrom pathlib import Path\nfrom typing\
        \ import Optional, List, Dict, Any, Union, Tuple\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO,\
        \ format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')\nlogger =\
        \ logging.getLogger(__name__)\n\nclass FAISSVectorStore:\n    def __init__(self,\
        \ index_path: Path, metadata_path: Path, embeddings_file_path: Optional[Path]\
        \ = None, dimension: Optional[int] = None):\n        \"\"\"\n        Initializes\
        \ the FAISSVectorStore.\n        Tries to load an existing index and metadata.\
        \ If not found, and an\n        embeddings_file_path is provided, it will\
        \ attempt to build them.\n        \"\"\"\n        self.index_path = index_path\n\
        \        self.metadata_path = metadata_path\n        self.embeddings_file_path\
        \ = embeddings_file_path\n        self.dimension = dimension\n\n        self.index:\
        \ Optional[faiss.Index] = None\n        self.metadata: List[Dict[str, Any]]\
        \ = []\n\n        if self.index_path.exists() and self.metadata_path.exists():\n\
        \            logger.info(f\"Loading existing FAISS index from {self.index_path}\
        \ and metadata from {self.metadata_path}\")\n            try:\n          \
        \      self.load_store()\n            except Exception as e:\n           \
        \     logger.error(f\"Error loading existing FAISS store: {e}. Will attempt\
        \ to rebuild.\", exc_info=True)\n                self._try_build_store()\n\
        \        elif self.embeddings_file_path and self.embeddings_file_path.exists():\n\
        \            logger.info(f\"FAISS index/metadata not found. Attempting to\
        \ build from {self.embeddings_file_path}\")\n            self._try_build_store()\n\
        \        else:\n            message = \"FAISS store cannot be loaded or built.\
        \ \"\n            if self.embeddings_file_path:\n                message +=\
        \ f\"Embeddings file '{self.embeddings_file_path}' not found.\"\n        \
        \    else:\n                message += \"No embeddings file path provided\
        \ to build a new store.\"\n            logger.warning(message)\n\n    def\
        \ _try_build_store(self):\n        \"\"\"Helper method to attempt building\
        \ the store.\"\"\"\n        embeddings_data = self._load_embeddings_data()\n\
        \        if embeddings_data:\n            # Use the correct field name 'vector'\
        \ from your previous code\n            self.build_index_from_embeddings(embeddings_data,\
        \ embedding_key='vector')\n            if self.index is not None and self.metadata:\n\
        \                 self.save_store()\n            else:\n                logger.warning(\"\
        FAISS index or metadata not built successfully. Store will not be saved.\"\
        )\n        else:\n            logger.warning(f\"No embeddings data loaded\
        \ from {self.embeddings_file_path}, FAISS index not built.\")\n\n    def _load_embeddings_data(self)\
        \ -> Optional[List[Dict[str, Any]]]:\n        \"\"\"Loads and validates embeddings\
        \ data from the specified JSON file.\"\"\"\n        if not self.embeddings_file_path\
        \ or not self.embeddings_file_path.exists():\n            logger.error(f\"\
        Embeddings file not found: {self.embeddings_file_path}\")\n            return\
        \ None\n        try:\n            # Use pathlib's open method\n          \
        \  with self.embeddings_file_path.open('r', encoding='utf-8') as f:\n    \
        \            data = json.load(f)\n            # Your original script used\
        \ 'embedding', but your previous script used 'vector'. Let's check for both\
        \ for robustness.\n            key = 'vector' if data and 'vector' in data[0]\
        \ else 'embedding'\n            if not isinstance(data, list) or not data:\n\
        \                logger.error(f\"Embeddings file {self.embeddings_file_path}\
        \ is empty or not a list.\")\n                return None\n            if\
        \ not all('id' in item and 'label' in item and key in item for item in data):\n\
        \                logger.error(f\"Embeddings data in {self.embeddings_file_path}\
        \ has an incorrect format.\")\n                return None\n            logger.info(f\"\
        Loaded {len(data)} embeddings from {self.embeddings_file_path}\")\n      \
        \      return data\n        except Exception as e:\n            logger.error(f\"\
        Failed to load embeddings file {self.embeddings_file_path}: {e}\", exc_info=True)\n\
        \            return None\n\n    def build_index_from_embeddings(self, embeddings_data:\
        \ List[Dict[str, Any]], embedding_key: str = 'embedding'):\n        \"\"\"\
        Builds the FAISS index and prepares metadata from loaded embeddings data.\"\
        \"\"\n        if not embeddings_data:\n            logger.warning(\"No embeddings\
        \ data provided to build index.\")\n            return\n\n        try:\n \
        \           vectors = [item[embedding_key] for item in embeddings_data if\
        \ item.get(embedding_key)]\n            self.metadata = [{'id': item['id'],\
        \ 'label': item['label']} for item in embeddings_data if item.get(embedding_key)]\n\
        \            \n            if not vectors:\n                logger.warning(f\"\
        No valid vectors found in embeddings_data with key '{embedding_key}'.\")\n\
        \                return\n                \n            vectors_np = np.array(vectors,\
        \ dtype='float32')\n\n            if not self.dimension:\n               \
        \ self.dimension = vectors_np.shape[1]\n            elif self.dimension !=\
        \ vectors_np.shape[1]:\n                raise ValueError(f\"Provided dimension\
        \ {self.dimension} does not match embeddings dimension {vectors_np.shape[1]}\"\
        )\n\n            self.index = faiss.IndexFlatL2(self.dimension)\n        \
        \    self.index.add(vectors_np)\n            logger.info(f\"FAISS index built\
        \ successfully with {self.index.ntotal} vectors of dimension {self.dimension}.\"\
        )\n        except Exception as e:\n            logger.error(f\"Error building\
        \ FAISS index: {e}\", exc_info=True)\n            self.index = None\n    \
        \        self.metadata = []\n\n    def save_store(self):\n        \"\"\"Saves\
        \ the FAISS index and metadata to their respective files.\"\"\"\n        if\
        \ self.index is None:\n            logger.warning(\"No FAISS index to save.\"\
        )\n            return\n\n        if not self.metadata:\n            logger.warning(\"\
        FAISS index exists but metadata is empty. Aborting save to prevent inconsistent\
        \ state.\")\n            return\n\n        # Use pathlib to ensure parent\
        \ directory exists\n        self.index_path.parent.mkdir(parents=True, exist_ok=True)\n\
        \        # FIX: Convert Path to string for faiss function\n        faiss.write_index(self.index,\
        \ str(self.index_path))\n        logger.info(f\"FAISS index saved to {self.index_path}\"\
        )\n\n        self.metadata_path.parent.mkdir(parents=True, exist_ok=True)\n\
        \        with self.metadata_path.open('w', encoding='utf-8') as f:\n     \
        \       # Using indent for readability, like in my other suggestion\n    \
        \        json.dump(self.metadata, f, indent=2)\n        logger.info(f\"FAISS\
        \ metadata saved to {self.metadata_path}\")\n\n    def load_store(self):\n\
        \        \"\"\"Loads the FAISS index and metadata from files.\"\"\"\n    \
        \    # FIX: Convert Path to string for faiss function\n        self.index\
        \ = faiss.read_index(str(self.index_path))\n        self.dimension = self.index.d\n\
        \        logger.info(f\"FAISS index loaded. Index has {self.index.ntotal}\
        \ vectors of dim {self.index.d}.\")\n\n        with self.metadata_path.open('r',\
        \ encoding='utf-8') as f:\n            self.metadata = json.load(f)\n    \
        \    logger.info(f\"FAISS metadata loaded. {len(self.metadata)} items.\")\n\
        \n        if self.index.ntotal != len(self.metadata):\n            logger.warning(f\"\
        Mismatch between FAISS index size ({self.index.ntotal}) and metadata size\
        \ ({len(self.metadata)}).\")\n\n    def search(self, query_vector: Union[np.ndarray,\
        \ list], k: int = 5) -> Union[Tuple, Tuple[np.ndarray, np.ndarray, List[Dict]]]:\n\
        \        \"\"\"Performs a K-Nearest Neighbors search on the FAISS index.\"\
        \"\"\n        if self.index is None:\n            logger.error(\"FAISS index\
        \ not initialized. Cannot perform search.\")\n            return np.array([]),\
        \ np.array([]), []\n\n        if not isinstance(query_vector, np.ndarray):\n\
        \            query_vector = np.array(query_vector, dtype='float32')\n    \
        \    if query_vector.ndim == 1:\n            query_vector = np.expand_dims(query_vector,\
        \ axis=0)\n        \n        if query_vector.shape[1] != self.index.d:\n \
        \           raise ValueError(f\"Query vector dimension ({query_vector.shape[1]})\
        \ does not match index dimension ({self.index.d}).\")\n\n        distances,\
        \ faiss_indices = self.index.search(query_vector, k)\n        \n        all_results_metadata\
        \ = []\n        for i in range(faiss_indices.shape[0]):\n            query_results_metadata\
        \ = []\n            for j in range(faiss_indices.shape[1]):\n            \
        \    idx = faiss_indices[i][j]\n                if idx != -1 and idx < len(self.metadata):\n\
        \                    query_results_metadata.append(self.metadata[idx])\n \
        \           all_results_metadata.append(query_results_metadata)\n        \n\
        \        return (distances[0], faiss_indices[0], all_results_metadata[0])\
        \ if query_vector.shape[0] == 1 else (distances, faiss_indices, all_results_metadata)\n\
        \n# --- Updated build script ---\nif __name__ == '__main__':\n    # Add project\
        \ root to be able to import config\n    import sys\n    # Use pathlib to add\
        \ parent directory\n    project_root = Path(__file__).resolve().parent.parent\n\
        \    sys.path.insert(0, str(project_root))\n    \n    from src.config import\
        \ ONTOLOGIES_CONFIG\n\n    logger.info(\"--- Building FAISS Stores for All\
        \ Configured Ontologies ---\")\n\n    for name, config_data in ONTOLOGIES_CONFIG.items():\n\
        \        # All paths are now pathlib.Path objects from config\n        index_path\
        \ = config_data.get('faiss_index_path')\n        metadata_path = config_data.get('faiss_metadata_path')\n\
        \        embeddings_path = config_data.get('embeddings_path')\n\n        logger.info(f\"\
        \\n--- Processing Ontology: {name} ---\")\n\n        if not all([index_path,\
        \ metadata_path, embeddings_path]):\n            logger.warning(f\"Configuration\
        \ for '{name}' is missing one or more FAISS paths. Skipping.\")\n        \
        \    continue\n\n        # Use pathlib's unlink method to delete old files\n\
        \        index_path.unlink(missing_ok=True)\n        metadata_path.unlink(missing_ok=True)\n\
        \        logger.info(f\"Removed old index files for '{name}' to ensure fresh\
        \ build.\")\n\n        # Use pathlib's exists method\n        if not embeddings_path.exists():\n\
        \            logger.error(f\"ERROR: Embeddings file not found at {embeddings_path}.\
        \ Cannot build FAISS index for '{name}'. Skipping.\")\n            continue\n\
        \n        store = FAISSVectorStore(\n            index_path=index_path,\n\
        \            metadata_path=metadata_path,\n            embeddings_file_path=embeddings_path\n\
        \        )\n\n        if store.index:\n            logger.info(f\"Successfully\
        \ built and saved FAISS index for '{name}' with {store.index.ntotal} vectors.\"\
        )\n        else:\n            logger.error(f\"FAILED to build FAISS index\
        \ for '{name}'. Check logs for errors.\")\n    \n    logger.info(\"\\n---\
        \ All FAISS Store Building Complete ---\")"
