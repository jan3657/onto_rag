"project_summary":
  "project_name": |-
    onto_rag
  "readme_content": |
    # onto_rag
    A Retrieval Augmented Generation system for matching extracted entities to ontologies
  "key_files":
    ".gitignore": |-
      # Byte-compiled / cache
      __pycache__/
      *.py[cod]
      .pytest_cache/

      # Environment & secrets
      .env
      api_key.json

      # VSCode, macOS
      .vscode/
      .DS_Store

      # Large generated data and ontology files
      /data/
      /ontologies/
      /models/
    "requirements.txt": |-
      # onto_rag/requirements.txt

      # Core Ontology Processing
      rdflib>=6.0.0

      # Lexical Indexing
      whoosh>=2.7.0

      # Embeddings & Semantic Search
      sentence-transformers>=2.2.0
      faiss-cpu>=1.7.0   # Or faiss-gpu if you have a CUDA-enabled GPU and want to use it

      # Utilities
      numpy>=1.21.0     # Often a dependency, good to specify
      pandas>=1.3.0     # Dependency for libraries like 'datasets' used by sentence-transformers
      python-dotenv>=0.19.0 # For loading .env files
      tqdm>=4.60.0      # For progress bars in long operations

      # Testing (optional, but good for development)
      pytest>=6.2.0
    "src/config.py": "# src/config.py\nimport os\nfrom dotenv import load_dotenv\n\
      from rdflib import Namespace # This is fine, though rdflib.Namespace is not\
      \ directly used for string constants below.\n\n# Project Root Directory\nPROJECT_ROOT\
      \ = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \".\"))\n\
      # print(f\"Project root directory: {PROJECT_ROOT}\") # Keep for debugging if\
      \ you like\n\n# Load environment variables from .env file\nload_dotenv(os.path.join(PROJECT_ROOT,\
      \ \".env\"))\n\n# Data Directory (for ontology dump, indexes, etc.)\nDATA_DIR\
      \ = os.path.join(PROJECT_ROOT, \"data\")\nos.makedirs(DATA_DIR, exist_ok=True)\n\
      \n# Ontologies Directory\nONTOLOGIES_DIR = os.path.join(PROJECT_ROOT, \"ontologies\"\
      )\nFOODON_PATH = os.path.join(ONTOLOGIES_DIR, \"foodon-with-syns.owl\")\nCHEBI_PATH\
      \ = os.path.join(ONTOLOGIES_DIR, \"chebi.owl\")\nTEST_FOODON_SNIPPET_PATH =\
      \ os.path.join(ONTOLOGIES_DIR, \"test_foodon_snippet.owl\")\n\n# --- NEW: Central\
      \ Ontologies Configuration ---\n# The keys ('foodon', 'chebi') are used as identifiers\
      \ throughout the pipeline.\nONTOLOGIES_CONFIG = {\n    'foodon': {\n       \
      \ 'path': os.path.join(ONTOLOGIES_DIR, \"foodon.owl\"),\n        'prefix': 'FOODON:',\n\
      \        'dump_json_path': os.path.join(DATA_DIR, \"ontology_dump_foodon.json\"\
      ),\n        'enriched_docs_path': os.path.join(DATA_DIR, f\"enriched_documents_foodon.json\"\
      ),\n        'embeddings_path': os.path.join(DATA_DIR, f\"embeddings_foodon.json\"\
      ),\n        'whoosh_index_dir': os.path.join(DATA_DIR, f\"whoosh_index_foodon\"\
      ),\n        'faiss_index_path': os.path.join(DATA_DIR, f\"faiss_index_foodon.bin\"\
      ),\n        'faiss_metadata_path': os.path.join(DATA_DIR, f\"faiss_metadata_foodon.json\"\
      ),\n    },\n    'chebi': {\n        'path': os.path.join(ONTOLOGIES_DIR, \"\
      chebi.owl\"),\n        'prefix': 'CHEBI:',\n        'dump_json_path': os.path.join(DATA_DIR,\
      \ \"ontology_dump_chebi.json\"),\n        'enriched_docs_path': os.path.join(DATA_DIR,\
      \ f\"enriched_documents_chebi.json\"),\n        'embeddings_path': os.path.join(DATA_DIR,\
      \ f\"embeddings_chebi.json\"),\n        'whoosh_index_dir': os.path.join(DATA_DIR,\
      \ f\"whoosh_index_chebi\"),\n        'faiss_index_path': os.path.join(DATA_DIR,\
      \ f\"faiss_index_chebi.bin\"),\n        'faiss_metadata_path': os.path.join(DATA_DIR,\
      \ f\"faiss_metadata_chebi.json\"),\n    }\n}\n# Ensure Whoosh directories exist\n\
      for name, config_data in ONTOLOGIES_CONFIG.items():\n    os.makedirs(config_data['whoosh_index_dir'],\
      \ exist_ok=True)\n\n\n# Output file from parse_ontology.py\nONTOLOGY_DUMP_JSON\
      \ = os.path.join(DATA_DIR, \"ontology_syns_dump.json\")\n\n# Output file for\
      \ enriched documents\nENRICHED_DOCUMENTS_FILE = os.path.join(DATA_DIR, \"enriched_documents.json\"\
      )\n\n# Output file for embeddings (used for building FAISS index)\nEMBEDDINGS_FILE\
      \ = os.path.join(DATA_DIR, \"embeddings.json\")\n\n# Whoosh Index Directory\n\
      WHOOSH_INDEX_DIR = os.path.join(DATA_DIR, \"whoosh_index\")\nos.makedirs(WHOOSH_INDEX_DIR,\
      \ exist_ok=True)\n\n# FAISS Index Paths\nFAISS_INDEX_PATH = os.path.join(DATA_DIR,\
      \ \"faiss_index.bin\")\nFAISS_METADATA_PATH = os.path.join(DATA_DIR, \"faiss_metadata.json\"\
      )\n\n# Embedding Model Configuration\nEMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\
      \n#EMBEDDING_MODEL_NAME = \"Lajavaness/bilingual-embedding-large\" \n# RERANKER_MODEL_NAME\
      \ = \"intfloat/e5-mistral-7b-instruct\"\nRERANKER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\
      \nEMBEDDING_BATCH_SIZE = 32 # For batch embedding script\nEMBEDDING_DEVICE =\
      \ 'cpu'  # or 'cuda' if available, for embedding script\n\n# Default K values\
      \ for retrieval\nDEFAULT_K_LEXICAL = 20\nDEFAULT_K_VECTOR = 20\nDEFAULT_RERANK_K\
      \ = DEFAULT_K_LEXICAL + DEFAULT_K_VECTOR \n\n# Namespaces (using string constants\
      \ for broader compatibility if rdflib not always imported)\nRDFS_NS_STR = \"\
      http://www.w3.org/2000/01/rdf-schema#\"\nRDF_NS_STR = \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\
      \nOWL_NS_STR = \"http://www.w3.org/2002/07/owl#\"\nSKOS_NS_STR = \"http://www.w3.org/2004/02/skos/core#\"\
      \nOBO_NS_STR = \"http://purl.obolibrary.org/obo/\"\nOBOINOWL_NS_STR = \"http://www.geneontology.org/formats/oboInOwl#\"\
      \nIAO_NS_STR = \"http://purl.obolibrary.org/obo/IAO_\"\n\n# For rdflib usage\
      \ where Namespace objects are preferred:\nRDFS_NS = Namespace(RDFS_NS_STR)\n\
      RDF_NS = Namespace(RDF_NS_STR)\nOWL_NS = Namespace(OWL_NS_STR)\nSKOS_NS = Namespace(SKOS_NS_STR)\n\
      OBO_NS = Namespace(OBO_NS_STR)\nOBOINOWL_NS = Namespace(OBOINOWL_NS_STR)\nIAO_NS\
      \ = Namespace(IAO_NS_STR)\n\n\n# Mapping of common relation URIs/CURIEs to human-readable\
      \ names and default prefixes\n# Used by parse_ontology.py and potentially enrich_documents.py\n\
      RELATION_CONFIG = {\n    \"obo:BFO_0000050\": {\"label\": \"part of\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0001000\": {\"label\": \"derives from\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0002200\": {\"label\": \"has phenotype\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0002451\": {\"label\": \"has part\", \"prefix\": \"\
      obo\"},\n    \"obo:FOODON_0000246\": {\"label\": \"has ingredient\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000056\": {\"label\": \"participates in\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000057\": {\"label\": \"has participant\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000085\": {\"label\": \"functionally related to\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002090\": {\"label\": \"adjacent to\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002131\": {\"label\": \"overlaps\", \"\
      prefix\": \"obo\"},\n    \"obo:RO_0002150\": {\"label\": \"connected to\", \"\
      prefix\": \"obo\"},\n    \"obo:RO_0002202\": {\"label\": \"develops from\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002350\": {\"label\": \"member of\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002351\": {\"label\": \"has member\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002353\": {\"label\": \"output of\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002440\": {\"label\": \"causally related\
      \ to\", \"prefix\": \"obo\"},\n    \"obo:RO_0002450\": {\"label\": \"contains\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002500\": {\"label\": \"surrounds\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002573\": {\"label\": \"has quality\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002606\": {\"label\": \"is transformation\
      \ of\", \"prefix\": \"obo\"},\n    \"obo:RO_0002607\": {\"label\": \"has input\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0003000\": {\"label\": \"used in\", \"\
      prefix\": \"obo\"},\n    \"obo:FOODON_0000380\": {\"label\": \"has preparation\
      \ method\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000226\": {\"label\": \"\
      has food source\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000388\": {\"label\"\
      : \"has maturity state\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000286\"\
      : {\"label\": \"has packaging\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000240\"\
      : {\"label\": \"has preservation method\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000440\"\
      : {\"label\": \"has physical state\", \"prefix\": \"obo\"},\n    # \"obo:FOODON_\"\
      : {\"label\": \"FoodON specific relation\", \"prefix\": \"obo\"}, # This generic\
      \ one might be too broad\n    \"obo:ERO_0000039\": {\"label\": \"has nutrient\"\
      , \"prefix\": \"obo\"},\n    \"obo:ERO_0000589\": {\"label\": \"dietary context\
      \ of\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C25277\": {\"label\": \"is allergen\
      \ of\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C48670\": {\"label\": \"has active\
      \ ingredient\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C64548\": {\"label\":\
      \ \"part of human diet\", \"prefix\": \"obo\"},\n    \"obo:PATO_0000001\": {\"\
      label\": \"has quality PATO\", \"prefix\": \"obo\"},\n}\n\nTARGET_RELATIONS_CURIES\
      \ = list(RELATION_CONFIG.keys())\n\n# CURIE Prefix Map for uri_to_curie and\
      \ curie_to_uri conversions\n# Ensure the keys are the *base URIs* that prefixes\
      \ are expected for.\nCURIE_PREFIX_MAP = {\n    \"http://purl.obolibrary.org/obo/FOODON_\"\
      : \"FOODON\",\n    \"http://purl.obolibrary.org/obo/BFO_\": \"BFO\",\n    \"\
      http://purl.obolibrary.org/obo/RO_\": \"RO\",\n    \"http://purl.obolibrary.org/obo/ERO_\"\
      : \"ERO\",\n    \"http://purl.obolibrary.org/obo/NCIT_\": \"NCIT\",\n    \"\
      http://purl.obolibrary.org/obo/PATO_\": \"PATO\",\n    \"http://purl.obolibrary.org/obo/IAO_\"\
      : \"IAO\",\n    \"http://www.w3.org/2000/01/rdf-schema#\": \"rdfs\",\n    \"\
      http://www.w3.org/1999/02/22-rdf-syntax-ns#\": \"rdf\",\n    \"http://www.w3.org/2002/07/owl#\"\
      : \"owl\",\n    \"http://www.w3.org/2004/02/skos/core#\": \"skos\",\n    \"\
      http://www.geneontology.org/formats/oboInOwl#\": \"oboInOwl\",\n    \"http://purl.obolibrary.org/obo/HANCESTRO_\"\
      : \"HANCESTRO\",  \n    \"http://purl.obolibrary.org/obo/GAZ_\": \"GAZ\",\n\
      \    \"http://purl.obolibrary.org/obo/CHEBI_\": \"CHEBI\",\n    \"http://purl.obolibrary.org/obo/NCBITaxon_\"\
      : \"NCBITaxon\",\n    \"http://purl.obolibrary.org/obo/UBERON_\": \"UBERON\"\
      ,\n    \"http://purl.obolibrary.org/obo/ENVO_\": \"ENVO\",\n    \"http://purl.obolibrary.org/obo/HP_\"\
      : \"HP\",\n    \"http://purl.obolibrary.org/obo/GO_\": \"GO\",\n    # General\
      \ OBO prefix - should be last or handled carefully to avoid overly broad matches\n\
      \    # if specific OBO sub-ontologies are listed above.\n    \"http://purl.obolibrary.org/obo/\"\
      : \"obo\",\n}\n\n# LLM API Key (placeholders)\nGEMINI_API_KEY = os.getenv(\"\
      GEMINI_API_KEY\")\n#LLM_SELECTOR_MODEL_NAME = \"gemini-1.5-flash-latest\"\n\
      LLM_SELECTOR_MODEL_NAME = \"gemini-2.5-flash-lite-preview-06-17\"\nOLLAMA_SELECTOR_MODEL_NAME\
      \ = 'llama3.1:8b'\n# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Logging\
      \ configuration\nLOG_LEVEL = \"INFO\"\nLOG_FILE = os.path.join(PROJECT_ROOT,\
      \ \"app.log\") # Example log file in project root"
    "src/main.py": "# src/main.py\nimport argparse\nimport json\nimport os\nimport\
      \ sys\n\n# --- Add project root to sys.path ---\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),\
      \ '..'))\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n\
      \n# Assuming you have an ollama_pipeline, but this works for gemini_pipeline\
      \ too\nfrom src.pipeline.ollama_pipeline import RAGPipeline \nfrom src.config\
      \ import DEFAULT_K_LEXICAL, DEFAULT_K_VECTOR\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"\
      Run the Onto-RAG pipeline with LLM selection.\")\n    parser.add_argument(\"\
      query\", type=str, help=\"The entity or text to search for (e.g., 'raw milk').\"\
      )\n    parser.add_argument(\"--lexical_k\", type=int, default=DEFAULT_K_LEXICAL,\
      \ help=f\"Initial candidates from lexical search (default: {DEFAULT_K_LEXICAL}).\"\
      )\n    parser.add_argument(\"--vector_k\", type=int, default=DEFAULT_K_VECTOR,\
      \ help=f\"Initial candidates from vector search (default: {DEFAULT_K_VECTOR}).\"\
      )\n    parser.add_argument(\"--top_n_rerank\", type=int, default=100, help=\"\
      Number of candidates to rerank and pass to the LLM (default: 10).\")\n    parser.add_argument(\"\
      --show_candidates\", action=\"store_true\", help=\"Show the list of candidates\
      \ provided to the LLM.\")\n    args = parser.parse_args()\n\n    pipeline =\
      \ None\n    try:\n        pipeline = RAGPipeline()\n        \n        # ---\
      \ MODIFIED: Unpack the tuple returned by pipeline.run ---\n        result_tuple\
      \ = pipeline.run(\n            query=args.query,\n            lexical_k=args.lexical_k,\n\
      \            vector_k=args.vector_k,\n            rerank_top_n=args.top_n_rerank\n\
      \        )\n        \n        # Handle case where pipeline returns None\n  \
      \      if not result_tuple:\n            final_result, candidates = None, []\n\
      \        else:\n            final_result, candidates = result_tuple\n\n    \
      \    # --- Print the final selection (no changes here) ---\n        print(\"\
      \\n--- Final LLM Selection ---\")\n        if not final_result:\n          \
      \  print(\"Could not determine a matching ontology term.\")\n        else:\n\
      \            print(f\"Query: '{args.query}'\")\n            print(\"---------------------------\"\
      )\n            print(f\"Chosen Term ID: {final_result.get('id', 'N/A')}\")\n\
      \            print(f\"Label:          {final_result.get('label', 'N/A')}\")\n\
      \            print(f\"Confidence:     {final_result.get('confidence_score',\
      \ 0.0):.1%}\")\n            print(f\"Definition:     {final_result.get('definition',\
      \ 'N/A')}\")\n            print(f\"Synonyms:       {'; '.join(final_result.get('synonyms',\
      \ [])) or 'None'}\")\n            print(\"\\nLLM Explanation:\")\n         \
      \   print(f\"  > {final_result.get('explanation', 'No explanation provided.')}\"\
      )\n        print(\"---------------------------\\n\")\n\n        # --- NEW: Print\
      \ the candidates if requested ---\n        if args.show_candidates and candidates:\n\
      \            print(f\"--- Top {len(candidates)} Candidates Provided to LLM ---\"\
      )\n            chosen_id = final_result.get('id') if final_result else None\n\
      \            \n            for i, candidate in enumerate(candidates):\n    \
      \            # Fetch full details for printing\n                details = pipeline.retriever.get_term_details(candidate.get('id'))\n\
      \                if not details: continue\n\n                marker = \"⭐️\"\
      \ if details.get('id') == chosen_id else \"  \"\n                rerank_score\
      \ = candidate.get('rerank_score')\n                score_str = f\"(Score: {rerank_score:.4f})\"\
      \ if rerank_score is not None else \"\"\n\n                print(f\"{i+1}. {marker}\
      \ {details.get('label', 'N/A')} `{details.get('id', 'N/A')}` {score_str}\")\n\
      \                definition = details.get('definition')\n                if\
      \ definition:\n                    print(f\"       Def: {definition[:150]}...\"\
      )  # Print first 150 chars of definition\n                else:\n          \
      \          print(f\"       Def: No definition available.\")\n\n            \
      \    if details.get('synonyms'):\n                    print(f\"       Syns:\
      \ {'; '.join(details.get('synonyms', []))}\")\n                print(\"-\" *\
      \ 20)\n            print(\"-------------------------------------------\\n\"\
      )\n        elif args.show_candidates:\n            print(\"--- No Candidates\
      \ to Display ---\")\n\n\n    except Exception as e:\n        print(f\"\\nAn\
      \ error occurred during the pipeline execution: {e}\", file=sys.stderr)\n  \
      \      import traceback\n        traceback.print_exc()\n    finally:\n     \
      \   if pipeline:\n            pipeline.close()\n\nif __name__ == \"__main__\"\
      :\n    main()"
  "directory_structure": |2-
        ├── .devcontainer/ (excluded)
        ├── .pytest_cache/ (excluded)
        ├── data/ (excluded)
        ├── docs/ (excluded)
        ├── models/ (excluded)
        ├── ontologies/ (excluded)
        ├── prompts/
        ├── scripts/
        ├── src/
        ├── .DS_Store (excluded)
        ├── .env (excluded)
        ├── .env.example
        ├── .gitignore
        ├── README.md (excluded)
        ├── evaluation_results_gemini_1.5-flash.json (excluded)
        ├── evaluation_results_llama.json (excluded)
        ├── export_files_contents.py (excluded)
        ├── project_contents_for_llm.txt (excluded)
        ├── requirements.txt
        ├── results.txt
        ├── review_app.py
        └── tempCodeRunnerFile.py (excluded)
        ├── .devcontainer/ (excluded)
            └── devcontainer.json (excluded)
        ├── .pytest_cache/ (excluded)
            ├── v/ (excluded)
            ├── .gitignore (excluded)
            ├── CACHEDIR.TAG (excluded)
            └── README.md (excluded)
            ├── v/ (excluded)
                └── cache/ (excluded)
                ├── cache/ (excluded)
                    ├── lastfailed (excluded)
                    ├── nodeids (excluded)
                    └── stepwise (excluded)
        ├── data/ (excluded)
            ├── outputs/ (excluded)
            ├── whoosh_index/ (excluded)
            ├── whoosh_index_chebi/ (excluded)
            ├── whoosh_index_foodon/ (excluded)
            ├── CafeteriaFCD_foodon.xml (excluded)
            ├── CafeteriaFCD_foodon_unique.xml (excluded)
            ├── CafeteriaFCD_snomedct.xml (excluded)
            ├── embeddings.json (excluded)
            ├── embeddings_chebi.json (excluded)
            ├── embeddings_foodon.json (excluded)
            ├── enriched_documents.json (excluded)
            ├── enriched_documents_chebi.json (excluded)
            ├── enriched_documents_foodon.json (excluded)
            ├── example_for_scoreing.txt (excluded)
            ├── faiss_index.bin (excluded)
            ├── faiss_index_chebi.bin (excluded)
            ├── faiss_index_foodon.bin (excluded)
            ├── faiss_metadata.json (excluded)
            ├── faiss_metadata_chebi.json (excluded)
            ├── faiss_metadata_foodon.json (excluded)
            ├── ontology_dump.json (excluded)
            ├── ontology_dump_chebi.json (excluded)
            ├── ontology_dump_foodon.json (excluded)
            ├── ontology_syns_dump.json (excluded)
            └── readable_evaluation_results.json (excluded)
            ├── outputs/ (excluded)
                ├── mapped_ingredients_output.json (excluded)
                └── parsed_ingredients_output.json (excluded)
            ├── whoosh_index/ (excluded)
                ├── MAIN_29x4gpdinh37pl3n.seg (excluded)
                ├── MAIN_WRITELOCK (excluded)
                └── _MAIN_1.toc (excluded)
            ├── whoosh_index_chebi/ (excluded)
                ├── MAIN_WRITELOCK (excluded)
                ├── MAIN_gc5ro7nrv510y84k.seg (excluded)
                └── _MAIN_1.toc (excluded)
            ├── whoosh_index_foodon/ (excluded)
                ├── MAIN_WRITELOCK (excluded)
                ├── MAIN_lyojtd7artupuv3p.seg (excluded)
                └── _MAIN_1.toc (excluded)
        ├── docs/ (excluded)
            ├── project_outline.txt (excluded)
            └── tasks.txt (excluded)
        ├── models/ (excluded)
            ├── food_bert_v3/ (excluded)
            └── .DS_Store (excluded)
            ├── food_bert_v3/ (excluded)
                ├── checkpoint-2000/ (excluded)
                ├── checkpoint-2280/ (excluded)
                ├── runs/ (excluded)
                ├── config.json (excluded)
                ├── model.safetensors (excluded)
                ├── special_tokens_map.json (excluded)
                ├── tokenizer_config.json (excluded)
                └── vocab.txt (excluded)
                ├── checkpoint-2000/ (excluded)
                    ├── config.json (excluded)
                    ├── model.safetensors (excluded)
                    ├── optimizer.pt (excluded)
                    ├── rng_state.pth (excluded)
                    ├── scheduler.pt (excluded)
                    ├── trainer_state.json (excluded)
                    └── training_args.bin (excluded)
                ├── checkpoint-2280/ (excluded)
                    ├── config.json (excluded)
                    ├── model.safetensors (excluded)
                    ├── optimizer.pt (excluded)
                    ├── rng_state.pth (excluded)
                    ├── scheduler.pt (excluded)
                    ├── trainer_state.json (excluded)
                    └── training_args.bin (excluded)
                ├── runs/ (excluded)
                    └── Mar18_19-24-31_Mac/ (excluded)
                    ├── Mar18_19-24-31_Mac/ (excluded)
                        └── events.out.tfevents.1742340271.Mac.12182.0 (excluded)
        ├── ontologies/ (excluded)
            ├── .DS_Store (excluded)
            ├── chebi.owl (excluded)
            ├── foodon.owl (excluded)
            ├── foodonmerged.owl (excluded)
            ├── test.sh (excluded)
            ├── test_foodon_snippet.owl (excluded)
            └── tmp.owl (excluded)
        ├── prompts/
            ├── final_selection.tpl
            ├── final_selection2.tpl
            ├── final_selection_ingredients.tpl
            ├── parse_ontology.tpl
            ├── reranker.tpl
            ├── selection_with_confidence.tpl
            └── strict_final_selection.tpl
        ├── scripts/
            ├── find_namespaces.py
            ├── format_evaluation_results.py
            ├── rebuild_base.bash
            └── reduce_evaluation_set.py
        ├── src/
            ├── __pycache__/
            ├── embeddings/
            ├── evaluation/
            ├── ingestion/
            ├── pipeline/
            ├── rag_selectors/
            ├── reranker/
            ├── retriever/
            ├── tests/
            ├── utils/
            ├── vector_store/
            ├── __init__.py
            ├── config.py
            ├── main.py
            ├── run_on_off.py
            └── tempCodeRunnerFile.py
            ├── __pycache__/
                ├── __init__.cpython-39.pyc (excluded)
                └── config.cpython-39.pyc (excluded)
            ├── embeddings/
                ├── __pycache__/
                └── embed_docs.py
                ├── __pycache__/
                    └── embed_docs.cpython-39.pyc (excluded)
            ├── evaluation/
                ├── __pycache__/
                ├── evaluate_pipeline.py
                └── evaluate_retriever_recall.py
                ├── __pycache__/
                    └── evaluate_retriever_recall.cpython-39.pyc (excluded)
            ├── ingestion/
                ├── __pycache__/
                ├── __init__.py
                ├── build_lexical_index.py
                ├── enrich_documents.py
                └── parse_ontology.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    ├── build_lexical_index.cpython-39.pyc (excluded)
                    ├── enrich_documents.cpython-39.pyc (excluded)
                    └── parse_ontology.cpython-39.pyc (excluded)
            ├── pipeline/
                ├── __pycache__/
                ├── gemini_pipeline.py
                └── ollama_pipeline.py
                ├── __pycache__/
                    ├── gemini_pipeline.cpython-39.pyc (excluded)
                    ├── ollama_pipeline.cpython-39.pyc (excluded)
                    └── pipeline.cpython-39.pyc (excluded)
            ├── rag_selectors/
                ├── __pycache__/
                ├── __init__.py
                ├── gemini_selector.py
                └── ollama_selector.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    ├── gemini_selector.cpython-39.pyc (excluded)
                    └── ollama_selector.cpython-39.pyc (excluded)
            ├── reranker/
                ├── __pycache__/
                ├── __init__.py
                └── llm_reranker.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    └── llm_reranker.cpython-39.pyc (excluded)
            ├── retriever/
                ├── __pycache__/
                ├── __init__.py
                └── hybrid_retriever.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    └── hybrid_retriever.cpython-39.pyc (excluded)
            ├── tests/
                ├── __pycache__/
                ├── __init__.py
                ├── test_lexical_index.py
                └── test_parse_ontology.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    ├── test_lexical_index.cpython-39-pytest-8.3.5.pyc (excluded)
                    └── test_parse_ontology.cpython-39-pytest-8.3.5.pyc (excluded)
            ├── utils/
                ├── __pycache__/
                ├── __init__.py
                └── ontology_utils.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    └── ontology_utils.cpython-39.pyc (excluded)
            ├── vector_store/
                ├── __pycache__/
                ├── __init__.py
                └── faiss_store.py
                ├── __pycache__/
                    ├── __init__.cpython-39.pyc (excluded)
                    └── faiss_store.cpython-39.pyc (excluded)
"file_contents":
  ".env.example": |
    # OpenAI & vector DB credentials
    OPENAI_API_KEY=your_openai_api_key_here
    PINECONE_API_KEY=your_pinecone_api_key_here
    PINECONE_ENV=your_pinecone_env
  ".gitignore": |-
    # Byte-compiled / cache
    __pycache__/
    *.py[cod]
    .pytest_cache/

    # Environment & secrets
    .env
    api_key.json

    # VSCode, macOS
    .vscode/
    .DS_Store

    # Large generated data and ontology files
    /data/
    /ontologies/
    /models/
  "requirements.txt": |-
    # onto_rag/requirements.txt

    # Core Ontology Processing
    rdflib>=6.0.0

    # Lexical Indexing
    whoosh>=2.7.0

    # Embeddings & Semantic Search
    sentence-transformers>=2.2.0
    faiss-cpu>=1.7.0   # Or faiss-gpu if you have a CUDA-enabled GPU and want to use it

    # Utilities
    numpy>=1.21.0     # Often a dependency, good to specify
    pandas>=1.3.0     # Dependency for libraries like 'datasets' used by sentence-transformers
    python-dotenv>=0.19.0 # For loading .env files
    tqdm>=4.60.0      # For progress bars in long operations

    # Testing (optional, but good for development)
    pytest>=6.2.0
  "results.txt": |-
    llama:
    2025-06-11 16:06:24,274 - INFO - --- Evaluation Complete ---
    2025-06-11 16:06:24,274 - INFO - Total entities evaluated: 948
    2025-06-11 16:06:24,274 - INFO - Retrieval Failures (no candidates): 0
    2025-06-11 16:06:24,274 - INFO - Selection Failures (LLM error): 0
    2025-06-11 16:06:24,274 - INFO - ---------------------------
    2025-06-11 16:06:24,274 - INFO - Valid attempts for selector: 948
    2025-06-11 16:06:24,274 - INFO - Correct selections (Hits): 386
    2025-06-11 16:06:24,275 - INFO - Accuracy: 0.4072 (386/948)
    2025-06-11 16:06:24,275 - INFO - Saving 562 incorrect selections to /Users/jandrole/projects/onto_rag/evaluation_results.json
    2025-06-11 16:06:24,301 - INFO - Evaluation finished.

    gemini:
    2025-06-16 10:43:04,962 - INFO - --- Evaluation Complete ---
    2025-06-16 10:43:04,962 - INFO - Total entities evaluated: 948
    2025-06-16 10:43:04,962 - INFO - Retrieval Failures (no candidates): 0
    2025-06-16 10:43:04,962 - INFO - Selection Failures (LLM error): 0
    2025-06-16 10:43:04,962 - INFO - ---------------------------
    2025-06-16 10:43:04,962 - INFO - Valid attempts for selector: 948
    2025-06-16 10:43:04,962 - INFO - Correct selections (Hits): 600
    2025-06-16 10:43:04,962 - INFO - Accuracy: 0.6329 (600/948)
    2025-06-16 10:43:04,962 - INFO - Saving 348 incorrect selections to /Users/jandrole/projects/onto_rag/evaluation_results.json
    2025-06-16 10:43:04,975 - INFO - Evaluation finished.
  "review_app.py": "import streamlit as st\nimport json\nfrom pathlib import Path\n\
    \n# --- Configuration ---\n# Set the path to the human-readable evaluation results\
    \ file.\n# This script assumes the file is in the 'data' subdirectory.\nEVALUATION_FILE_PATH\
    \ = Path(\"data\") / \"readable_evaluation_results.json\"\n\n# --- Helper Functions\
    \ ---\n\n@st.cache_data\ndef load_data(file_path: Path) -> list:\n    \"\"\"\n\
    \    Loads the evaluation data from the specified JSON file.\n    The @st.cache_data\
    \ decorator ensures the data is loaded only once.\n    \"\"\"\n    if not file_path.exists():\n\
    \        st.error(f\"Error: Evaluation file not found at '{file_path}'.\")\n \
    \       st.info(\"Please run the `scripts/format_evaluation_results.py` script\
    \ first to generate this file.\")\n        return None\n    with open(file_path,\
    \ 'r', encoding='utf-8') as f:\n        return json.load(f)\n\ndef display_term_details(term_data:\
    \ dict):\n    \"\"\"Renders the details of a single ontology term without a main\
    \ title.\"\"\"\n    if not term_data or not term_data.get(\"curie\"):\n      \
    \  st.warning(\"No data available for this term.\")\n        return\n\n    # Display\
    \ the label and CURIE\n    label = term_data.get('label', 'N/A')\n    curie =\
    \ term_data.get('curie', 'N/A')\n    st.markdown(f\"**{label}** (`{curie}`)\"\
    )\n\n    # Display the definition in an info box\n    definition = term_data.get('definition')\n\
    \    if definition:\n        st.info(f\"**Definition:** {definition}\")\n    else:\n\
    \        st.info(\"No definition provided.\")\n\n    # Display synonyms if they\
    \ exist\n    synonyms = term_data.get('synonyms', [])\n    if synonyms:\n    \
    \    st.markdown(f\"**Synonyms:** *{', '.join(synonyms)}*\")\n\ndef display_term(term_data:\
    \ dict, title: str):\n    \"\"\"Renders a single ontology term's details in a\
    \ structured format.\"\"\"\n    st.subheader(title)\n    display_term_details(term_data)\n\
    \n# --- Main Application Logic ---\n\n# Set the page configuration (title, icon,\
    \ layout)\nst.set_page_config(\n    page_title=\"Ontology Linking Review\",\n\
    \    page_icon=\"\U0001F9EA\",\n    layout=\"wide\"\n)\n\nst.title(\"\U0001F9EA\
    \ Ontology Linking Evaluation Review\")\nst.markdown(\"An interface for experts\
    \ to review the performance of the entity linking model.\")\n\n# Load the data\
    \ using the cached function\ndata = load_data(EVALUATION_FILE_PATH)\n\nif data:\n\
    \    # --- Session State for Navigation ---\n    # Initialize the session state\
    \ to keep track of the current item index\n    if 'current_index' not in st.session_state:\n\
    \        st.session_state.current_index = 0\n\n    # --- Navigation Controls ---\n\
    \    st.sidebar.header(\"Navigation\")\n    # Allow selection by query text for\
    \ easy lookup\n    query_list = [f\"{i+1}. {item['query']}\" for i, item in enumerate(data)]\n\
    \    selected_query = st.sidebar.selectbox(\"Select a Query to Review:\", query_list,\
    \ index=st.session_state.current_index)\n\n    # Update index based on selection\n\
    \    st.session_state.current_index = query_list.index(selected_query)\n\n   \
    \ col1, col2 = st.sidebar.columns(2)\n    # \"Previous\" button\n    if col1.button(\"\
    ⬅️ Previous\", use_container_width=True):\n        if st.session_state.current_index\
    \ > 0:\n            st.session_state.current_index -= 1\n            st.rerun()\n\
    \    # \"Next\" button\n    if col2.button(\"Next ➡️\", use_container_width=True):\n\
    \        if st.session_state.current_index < len(data) - 1:\n            st.session_state.current_index\
    \ += 1\n            st.rerun()\n\n    # --- Display the selected item ---\n  \
    \  item = data[st.session_state.current_index]\n    query = item.get(\"query\"\
    )\n\n    st.header(f\"Reviewing Query: \\\"{query}\\\"\", divider=\"rainbow\"\
    )\n\n    # --- CHANGE 1: Removed the Correct/Incorrect status message ---\n  \
    \  # The block checking item.get(\"is_correct\") was removed from here.\n\n  \
    \  # Use columns for a side-by-side comparison\n    left_col, right_col = st.columns(2)\n\
    \n    with left_col:\n        # Display the model's chosen term\n        display_term(item.get(\"\
    chosen_term\"), \"\U0001F916 Model's Choice\")\n        # Display the model's\
    \ reasoning in an expandable section\n        with st.expander(\"Show Model's\
    \ Explanation\"):\n            st.info(item.get(\"explanation\", \"No explanation\
    \ provided.\"))\n\n    with right_col:\n        # --- CHANGE 2: Display ALL ground\
    \ truth terms with full details ---\n        st.subheader(\"\U0001F3AF Ground\
    \ Truth(s)\")\n        ground_truth_terms = item.get(\"ground_truth_terms\", [])\n\
    \        \n        if not ground_truth_terms:\n             st.warning(\"No ground\
    \ truth terms provided for this query.\")\n        else:\n            for i, term\
    \ in enumerate(ground_truth_terms):\n                # Add a separator between\
    \ terms for clarity, but not before the first one\n                if i > 0:\n\
    \                    st.markdown(\"---\")\n                display_term_details(term)\n\
    \n\n    # --- Display the list of all candidates provided to the model ---\n \
    \   st.markdown(\"---\")\n    with st.expander(\"\U0001F575️‍♀️ View All Candidate\
    \ Terms Provided to Model\"):\n        st.markdown(\"This is the full list of\
    \ options the model had to choose from.\")\n\n        candidates = item.get(\"\
    candidate_terms_provided\", [])\n        chosen_curie = item.get(\"chosen_term\"\
    , {}).get(\"curie\")\n        ground_truth_curies = [gt.get(\"curie\") for gt\
    \ in item.get(\"ground_truth_terms\", [])]\n\n        if not candidates:\n   \
    \         st.info(\"No candidate terms were provided to the model for this query.\"\
    )\n        else:\n            for candidate in candidates:\n                label\
    \ = candidate.get('label', 'N/A')\n                curie = candidate.get('curie',\
    \ 'N/A')\n\n                # Highlight chosen and ground truth terms\n      \
    \          marker = \"\"\n                if curie == chosen_curie:\n        \
    \            marker += \"\U0001F916\"\n                if curie in ground_truth_curies:\n\
    \                    marker += \"\U0001F3AF\"\n\n                st.markdown(f\"\
    **{marker} {label}** (`{curie}`)\")\n                definition = candidate.get('definition')\n\
    \                if definition:\n                    st.text(f\"  - {definition[:200]}...\"\
    ) # Truncate long definitions\n                else:\n                    st.text(\"\
    \  - No definition.\")"
  "prompts":
    "final_selection.tpl": |-
      You are an expert ontologist specializing in food science. Your task is to analyze a list of candidate ontology terms and select the single most appropriate term that matches the user's provided entity.

      **User Entity:**
      [USER_ENTITY]

      **Candidate Ontology Terms:**
      [CANDIDATE_LIST]

      **Instructions:**
      1.  Carefully review the user's entity and each candidate's details (ID, Label, Definition, Synonyms).
      2.  Select the single best match. Consider exact matches of labels or synonyms as strong signals. If there are multiple good matches, prefer the more specific term over a general one.
      3.  Provide your response in a valid JSON format only. Do not add any text before or after the JSON block.
      4.  The JSON object must contain two keys:
          - "chosen_id": The CURIE (ID) of the single best matching term (e.g., "FOODON:00001290").
          - "explanation": A brief, clear explanation for your choice, justifying why it is the best fit compared to other options.

      **Example Response Format:**
      {
        "chosen_id": "FOODON:00001290",
        "explanation": "I chose 'garlic' because its label is an exact match for the user entity. Candidate 'allium sativum' is the scientific name but 'garlic' is the common term and therefore a better fit."
      }
    "final_selection2.tpl": "SYSTEM:\nYou are an expert ontologist specializing in\
      \ food science.  \nYour objective is to map a given *user_entity* to one—and\
      \ only one—candidate term drawn from FoodOn.  \nOperate deterministically: set\
      \ **temperature = 0** (greedy decoding).  \nReturn **only** a valid JSON object\
      \ as output—no prose before or after.\n\nINPUT (exact JSON structure):\n{\n\
      \  \"user_entity\": \"<string>\",\n  \"candidate_terms\": [\n    {\n      \"\
      id\": \"<CURIE>\",\n      \"label\": \"<string>\",\n      \"definition\": \"\
      <string>\",\n      \"synonyms\": [\"<string>\", ...]\n    }\n    // … additional\
      \ candidates in the same shape\n  ]\n}\n\nTASK INSTRUCTIONS\n1. Compare *user_entity*\
      \ against each candidate term’s label, definition, and synonyms.\n2. Identify\
      \ the single best match.  \n   • Exact lexical matches in *label* or *synonyms*\
      \ are strong signals.  \n   • If multiple candidates are plausible, **prefer\
      \ the most specific term** over broader ones.\n3. Produce a JSON object with\
      \ exactly two keys:\n   • **\"chosen_id\"** – the CURIE of the selected term.\
      \  \n   • **\"explanation\"** – a concise, step-by-step justification of why\
      \ this term outranks the others (implicit chain-of-thought).\n\nOUTPUT FORMAT\
      \ (nothing else):\n{\n  \"chosen_id\": \"<CURIE>\",\n  \"explanation\": \"<brief\
      \ but complete reasoning>\"\n}\n\nEDGE-CASE EXAMPLE (for the model’s patterning\
      \ only):\nInput:\n{\n  \"user_entity\": \"apple\",\n  \"candidate_terms\": [\n\
      \    {\"id\":\"FOODON:00002403\",\"label\":\"apple (fruit)\",\"definition\"\
      :\"The edible fruit of Malus domestica.\",\"synonyms\":[\"apple fruit\",\"fresh\
      \ apple\"]},\n    {\"id\":\"FOODON:03311015\",\"label\":\"fruit (plant product)\"\
      ,\"definition\":\"A botanical fruit.\",\"synonyms\":[\"plant fruit\"]},\n  \
      \  {\"id\":\"FOODON:00002405\",\"label\":\"apple pie\",\"definition\":\"A pie\
      \ made with apples.\",\"synonyms\":[\"apple tart\"]}\n  ]\n}\n\nExpected Output:\n\
      {\n  \"chosen_id\": \"FOODON:00002403\",\n  \"explanation\": \"Exact label match\
      \ with 'apple (fruit)'. While 'fruit (plant product)' matches only generically\
      \ and 'apple pie' is a derivative product, 'apple (fruit)' is the most specific,\
      \ conceptually precise fit.\"\n}\n\n\n\n\n\n\nYou are an expert ontologist specializing\
      \ in food science.\nRespond deterministically (the caller will invoke the model\
      \ with temperature 0).\n\n────────────────────────────────────────────────────────\n\
      USER ENTITY\n[USER_ENTITY]\n\nCANDIDATE ONTOLOGY TERMS\n[As numbered list —\
      \ each item has “ID”, “Label”, “Definition”, “Synonyms”.]\n[CANDIDATE_LIST]\n\
      ────────────────────────────────────────────────────────\n\nTASK\n1. Examine\
      \ the user entity against each candidate’s **Label, Definition, and Synonyms**.\n\
      2. Pick the **single** best-matching term.\n   • Exact lexical matches in Label\
      \ or Synonyms are strong signals.  \n   • If several terms could work, choose\
      \ the **most specific** one.\n3. Output **only** a valid JSON object – nothing\
      \ before or after it.\n\nOUTPUT SHAPE\n{\n  \"chosen_id\": \"<CURIE>\",\n  \"\
      explanation\": \"<concise step-by-step rationale comparing the chosen term to\
      \ close alternatives>\"\n}\n\n\U0001F4CC Do not wrap the JSON in markdown fences.\
      \ Do not emit any other text.\n\nEXAMPLE (shows specificity rule; do NOT repeat\
      \ in your answer)\nInput block (abbreviated):\n  USER_ENTITY: apple\n  CANDIDATE_LIST:\n\
      \    1. ID: FOODON:00002403 … Label: apple (fruit) …\n    2. ID: FOODON:03311015\
      \ … Label: fruit (plant product) …\n    3. ID: FOODON:00002405 … Label: apple\
      \ pie …\nExpected JSON:\n{\n  \"chosen_id\": \"FOODON:00002403\",\n  \"explanation\"\
      : \"Exact label match with 'apple (fruit)'. 'Fruit (plant product)' is generic\
      \ and 'apple pie' is a derivative food; therefore 'apple (fruit)' is the most\
      \ specific fit.\"\n}\n"
    "final_selection_ingredients.tpl": |-
      You are an expert ontologist specializing in food, ingredient, and chemical substance classification. Your task is to analyze a user-provided entity and select the single most appropriate term from a list of candidate ontology terms, based on a strict set of rules.

      You will be given a user entity and a numbered list of candidate terms. Each candidate in the list includes an ID, Label, Definition, and a list of Synonyms.

      **User Entity:**
      [USER_ENTITY]

      **Candidate List:**
      [CANDIDATE_LIST]

      **Instructions:**
      1.  Carefully evaluate the User Entity against the Label, Synonyms, and Definition of each candidate in the Candidate List.
      2.  Select the single best match. An exact match between the User Entity and a candidate's Label or one of its Synonyms is the strongest signal for selection.
      3.  **Specificity Rule:** If multiple candidates are good matches, you must choose the most specific term over the more general one.
      4.  Your response must be a single, valid JSON object only. Do not add any text, explanations, or comments before or after the JSON block.

      **Output Format:**
      The JSON object you return must contain two keys:
      * `"chosen_id"`: The ID of the single best matching term.
      * `"explanation"`: A brief justification for your choice. This explanation must clarify why the chosen term is the best fit and, if relevant, why it was chosen over other plausible candidates by applying the specificity rule.

      **Example:**
      ---
      **User Entity:**
      citric acid

      **Candidate List:**
      1. ID: FOODON:03301503
         Label: acidulant
         Definition: A food additive which increases the acidity or enhances the sour taste of a food.
         Synonyms: food acid

      2. ID: CHEBI:30769
         Label: citric acid
         Definition: A tricarboxylic acid that is propane-1,2,3-tricarboxylic acid bearing a hydroxy substituent at position 2.
         Synonyms: 2-hydroxypropane-1,2,3-tricarboxylic acid

      3. ID: FOODON:03301072
         Label: lemon juice
         Definition: The juice obtained from lemons, a common source of citric acid.
         Synonyms: None
      ---

      Your Response:
      ```json
      {
        "chosen_id": "CHEBI:30769",
        "explanation": "I chose 'citric acid' because its label is an exact match for the user entity. While 'acidulant' describes its function, 'citric acid' is the specific chemical entity and therefore the most precise match, adhering to the specificity rule. 'lemon juice' is a product that contains the entity, not the entity itself."
      }
    "parse_ontology.tpl": ""
    "reranker.tpl": ""
    "selection_with_confidence.tpl": |-
      You are an expert ontologist specializing in food, ingredient, and chemical substance classification. Your task is to analyze a user-provided entity and select the single most appropriate term from a list of candidate ontology terms, based on a strict set of rules.

      You will be given a user entity and a numbered list of candidate terms. Each candidate in the list includes an ID, Label, Definition, and a list of Synonyms.

      **User Entity**:
      [USER_ENTITY]

      **Candidate Ontology Terms**:
      [CANDIDATE_LIST]

      **INSTRUCTIONS**:

      Carefully review the user's entity and each candidate's details (ID, Label, Definition, Synonyms).
      Select the single best match. Consider exact matches of labels or synonyms as strong signals. If there are multiple good matches, prefer the more specific term over a general one.
      Provide your response in a valid JSON format only. Do not add any text, comments, or markdown fences before or after the JSON block.
      The JSON object must contain three keys:
      "chosen_id": The CURIE (ID) of the single best matching term (e.g., "FOODON:00001290").
      "confidence_score": A float between 0.0 and 1.0 indicating your confidence in the correct term being in the candidate list. If the candidate includes a similar and not exact chemical give low score.
      "explanation": A brief, clear explanation for your choice, justifying why it is the best fit. If confidence is low, explain the ambiguity.
      **EXAMPLE RESPONSE FORMAT**:

      Generated json
      {
        "chosen_id": "FOODON:00001290",
        "confidence_score": 1.0,
        "explanation": "The user entity 'garlic' is an exact match for the label of candidate FOODON:00001290. This is the most direct and specific match available."
      }
    "strict_final_selection.tpl": |-
      You are a rigorous and precise ontologist specializing in food, ingredient, and chemical substance classification. Your task is to analyze a user-provided entity and find the single most appropriate term from a list of candidate ontology terms. You must operate based on a strict, evidence-based rubric and avoid making speculative or purely semantic connections.

      You will be given a user entity and a numbered list of candidate terms. Each candidate in the list includes an ID, Label, Definition, and a list of Synonyms.

      **User Entity**:
      [USER_ENTITY]

      **Candidate Ontology Terms**:
      [CANDIDATE_LIST]

      **INSTRUCTIONS**:

      1.  **Analyze and Match:** Carefully compare the user entity against each candidate's Label and Synonyms list. Your goal is to find a match based on verifiable evidence.
      2.  **Apply the Rubric:** Use the following **Matching and Confidence Score Rubric** to determine the single best match and its corresponding confidence score. You must follow this hierarchy. Start at the top (1.0) and work your way down.
      3.  **Handle No Match:** If no candidate meets at least the criteria for a 0.4 confidence score, or if all potential matches have clear contradictory information, you must select "no match".
      4.  **Format Output:** Provide your response in a valid JSON format only. Do not add any text, comments, or markdown fences before or after the JSON block.

      ---
      ### **Matching and Confidence Score Rubric**

      *   **1.0 (Certain Match):**
          *   **Criteria:** The user entity is an exact, case-insensitive match for the candidate's `Label` or one of its `Synonyms`.
          *   **Example:** User entity `garlic` matches `Label: 'Garlic'`.

      *   **0.9 (High-Confidence Match):**
          *   **Criteria:** The user entity is a well-known alternative name, abbreviation, or a normalized form of the candidate's `Label` or `Synonyms` (e.g., handles plurals, spacing, or common initialisms like "MSG" for "monosodium glutamate").
          *   **Example:** User entity `powdered sugar` matches `Label: 'Icing Sugar'`.

      *   **0.6 (Plausible Match):**
          *   **Criteria:** The user entity is a substring of a label/synonym, or describes a very specific instance of the candidate, but is not a direct synonym. The connection is strong and highly likely, but not formally verified in the synonyms list.
          *   **Example:** User entity `organic apple` could plausibly match `Label: 'Apple'`. The explanation must note that the 'organic' quality is not captured in the ontology term.

      *   **0.4 (Speculative Match):**
          *   **Criteria:** The entities are only related by broad category or context, but there is no direct lexical overlap. **This should be used rarely.** The user entity and the candidate belong to the same specific class, but are clearly not the same thing.
          *   **Example:** User entity `lemonade` and a candidate `Label: 'Citrus Drink'`. The explanation must clearly state the speculative nature and the lack of a direct match.

      *   **NO MATCH (Confidence 0.0):**
          *   **Criteria:** No candidate meets the 0.4 criteria. Crucially, use this if the best potential match is a different, distinct entity within the same category.
          *   **Crucial Rule:** For chemicals, food colorings, or regulated substances, any difference in naming or numbering (e.g., "Blue 1" vs. "Blue 2") means they are **distinct entities and not a match**. Your example `blue 2 lake` vs. `copper(II) phthalocyanine` (Pigment Blue 15) is a classic case for **NO MATCH**.

      ---
      **JSON OUTPUT FORMAT**:

      The JSON object must contain three keys:
      *   `"chosen_id"`: The CURIE (ID) of the single best matching term. If no suitable match is found according to the rubric, this value **must be '-1'**.
      *   `"confidence_score"`: A float between 0.0 and 1.0, determined strictly by the rubric above. If `chosen_id` is `-1`, this **must be `0.0`**.
      *   `"explanation"`: A brief, clear explanation for the choice and score, referencing the rubric. If confidence is low or it's a "no match," explain the ambiguity or the reason for rejection.

      **EXAMPLE 1 (Perfect Match)**:
      *User Entity*: 'garlic'
      *Generated JSON*:

      {
        "chosen_id": "FOODON:00001290",
        "confidence_score": 1.0,
        "explanation": "The user entity 'garlic' is an exact match for the label of candidate FOODON:00001290, meeting the criteria for a 1.0 confidence score."
      }

      EXAMPLE 2 (No Match):
      User Entity: 'blue 2 lake'
      Candidate List containing CHEBI:155903 (copper(II) phthalocyanine)
      Generated JSON:
      {
        "chosen_id": "-1",
        "confidence_score": 0.0,
        "explanation": "No suitable match found. The user entity 'blue 2 lake' refers to FD&C Blue No. 2, a specific food dye. The closest candidate, CHEBI:155903 (copper(II) phthalocyanine), is a different chemical substance (Pigment Blue 15). Although both are blue pigments, they are distinct entities."
      }
  "scripts":
    "find_namespaces.py": |-
      # scripts/find_namespaces.py
      import rdflib
      from rdflib import URIRef
      from collections import Counter
      import re
      import os
      import sys

      # Add project root to allow importing src.config
      PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
      sys.path.append(PROJECT_ROOT)
      from src.config import FOODON_PATH # Or any other OWL file you want to inspect

      def get_base_uri(uri_str):
          """
          Tries to extract a base URI from a full URI.
          e.g., http://purl.obolibrary.org/obo/FOODON_0000123 -> http://purl.obolibrary.org/obo/FOODON_
          e.g., http://www.w3.org/2000/01/rdf-schema#label -> http://www.w3.org/2000/01/rdf-schema#
          """
          if '#' in uri_str:
              return uri_str.rsplit('#', 1)[0] + '#'
          # Common OBO pattern: ends with an underscore followed by digits
          match_obo = re.match(r'(.+[_A-Z]+_)(\d+)$', uri_str)
          if match_obo:
              return match_obo.group(1)
          # General pattern: up to the last slash
          if '/' in uri_str:
              return uri_str.rsplit('/', 1)[0] + '/'
          return None

      def main(ontology_path):
          print(f"Loading ontology from: {ontology_path}...")
          g = rdflib.Graph()
          try:
              g.parse(ontology_path) # format will be auto-detected
              print(f"Successfully parsed. Found {len(g)} triples.")
          except Exception as e:
              print(f"Error parsing ontology: {e}")
              return

          uris = set()
          for s, p, o in g:
              if isinstance(s, URIRef):
                  uris.add(str(s))
              if isinstance(p, URIRef):
                  uris.add(str(p))
              if isinstance(o, URIRef):
                  uris.add(str(o))

          print(f"\nFound {len(uris)} unique URIs.")

          base_uri_counts = Counter()
          for uri in uris:
              base = get_base_uri(uri)
              if base:
                  base_uri_counts[base] += 1

          print("\nPotential Base URIs (with counts of distinct full URIs using them):")
          # Sort by count descending
          for base, count in base_uri_counts.most_common():
              print(f"- \"{base}\": (used in {count} URIs)")

          print("\n--- rdflib's known namespaces (Prefix: Namespace) ---")
          for prefix, namespace in g.namespaces():
              print(f"  {prefix}: {namespace}")


      if __name__ == "__main__":
          # You can make the ontology path an argument if you like
          # For now, using FOODON_PATH from config
          if not os.path.exists(FOODON_PATH):
              print(f"ERROR: FoodON ontology file not found at {FOODON_PATH}")
          else:
              main(FOODON_PATH)
    "format_evaluation_results.py": "# scripts/format_evaluation_results.py\n\nimport\
      \ json\nimport sys\nimport argparse\nfrom pathlib import Path\nfrom typing import\
      \ Dict, Any, List, Optional\n\n# --- Add project root to sys.path to allow imports\
      \ from src ---\n# This assumes the script is in project_root/scripts/\nPROJECT_ROOT\
      \ = Path(__file__).resolve().parents[1]\nsys.path.append(str(PROJECT_ROOT))\n\
      # ---\n\ntry:\n    from src import config\nexcept ImportError:\n    print(\"\
      Error: Could not import from 'src'. Make sure the script is run from the project\
      \ root\"\n          \" or the project structure is correct.\")\n    # Provide\
      \ fallback paths if config fails to import\n    class config:\n        ONTOLOGY_DUMP_JSON\
      \ = PROJECT_ROOT / \"data\" / \"ontology_dump.json\"\n\ndef load_json_data(file_path:\
      \ Path) -> Any:\n    \"\"\"Loads data from a JSON file.\"\"\"\n    if not file_path.exists():\n\
      \        print(f\"Error: File not found at {file_path}\")\n        sys.exit(1)\n\
      \    print(f\"Loading data from {file_path}...\")\n    with open(file_path,\
      \ 'r', encoding='utf-8') as f:\n        return json.load(f)\n\ndef get_enriched_details(curie:\
      \ str, ontology_data: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Retrieves\
      \ detailed information for a given CURIE from the ontology dump.\n    \n   \
      \ Args:\n        curie: The CURIE to look up (e.g., \"FOODON:03316347\").\n\
      \        ontology_data: The loaded ontology_dump.json data.\n\n    Returns:\n\
      \        A dictionary with enriched details.\n    \"\"\"\n    term_data = ontology_data.get(curie)\n\
      \    if not term_data:\n        return {\n            \"curie\": curie,\n  \
      \          \"label\": \"--- CURIE NOT FOUND ---\",\n            \"definition\"\
      : \"N/A\",\n            \"synonyms\": []\n        }\n    \n    return {\n  \
      \      \"curie\": curie,\n        \"label\": term_data.get(\"label\"),\n   \
      \     \"definition\": term_data.get(\"definition\"),\n        \"synonyms\":\
      \ term_data.get(\"synonyms\", [])\n    }\n\ndef process_evaluation_file(\n \
      \   eval_results: List[Dict[str, Any]],\n    ontology_data: Dict[str, Any]\n\
      ) -> List[Dict[str, Any]]:\n    \"\"\"\n    Processes the evaluation results\
      \ to create a human-readable version.\n    \"\"\"\n    print(\"Enriching evaluation\
      \ results...\")\n    enriched_output = []\n\n    for item in eval_results:\n\
      \        query = item.get(\"query\")\n        chosen_curie = item.get(\"chosen_curie\"\
      )\n        true_curies = item.get(\"true_curies\", [])\n        candidates_provided\
      \ = item.get(\"candidates_provided\", [])\n        \n        # Check if the\
      \ chosen CURIE is in the list of true CURIEs\n        is_correct = chosen_curie\
      \ in true_curies\n\n        # Enrich all relevant CURIEs\n        chosen_details\
      \ = get_enriched_details(chosen_curie, ontology_data)\n        true_details\
      \ = [get_enriched_details(tc, ontology_data) for tc in true_curies]\n      \
      \  candidates_details = [get_enriched_details(cc, ontology_data) for cc in candidates_provided]\n\
      \n        enriched_item = {\n            \"query\": query,\n            \"is_correct\"\
      : is_correct,\n            \"explanation\": item.get(\"explanation\"),\n   \
      \         \"chosen_term\": chosen_details,\n            \"ground_truth_terms\"\
      : true_details,\n            \"candidate_terms_provided\": candidates_details\n\
      \        }\n        enriched_output.append(enriched_item)\n        \n    return\
      \ enriched_output\n\ndef main():\n    \"\"\"Main function to run the script.\"\
      \"\"\n    parser = argparse.ArgumentParser(\n        description=\"Enrich LLM\
      \ evaluation results with ontology details for human review.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n\
      \    )\n    parser.add_argument(\n        \"--input-file\",\n        type=Path,\n\
      \        default=PROJECT_ROOT / \"evaluation_results_gemini_1.5-flash.json\"\
      ,\n        help=\"Path to the raw evaluation results JSON file.\"\n    )\n \
      \   parser.add_argument(\n        \"--output-file\",\n        type=Path,\n \
      \       default=PROJECT_ROOT / \"data\" / \"readable_evaluation_results.json\"\
      ,\n        help=\"Path to save the enriched, human-readable JSON file.\"\n \
      \   )\n    parser.add_argument(\n        \"--ontology-dump\",\n        type=Path,\n\
      \        default=Path(config.ONTOLOGY_DUMP_JSON),\n        help=\"Path to the\
      \ ontology_dump.json file.\"\n    )\n    args = parser.parse_args()\n\n    #\
      \ Create output directory if it doesn't exist\n    args.output_file.parent.mkdir(parents=True,\
      \ exist_ok=True)\n\n    # Load data\n    ontology_data = load_json_data(args.ontology_dump)\n\
      \    eval_results = load_json_data(args.input_file)\n\n    # Process and enrich\
      \ the data\n    readable_results = process_evaluation_file(eval_results, ontology_data)\n\
      \n    # Save the new file\n    print(f\"Saving enriched results to {args.output_file}...\"\
      )\n    with open(args.output_file, 'w', encoding='utf-8') as f:\n        json.dump(readable_results,\
      \ f, indent=2, ensure_ascii=False)\n    \n    print(\"Done!\")\n\nif __name__\
      \ == \"__main__\":\n    main()"
    "rebuild_base.bash": |-
      #!/usr/bin/env bash
      #
      # Rebuilds all data and indexes for the Onto-RAG pipeline.
      # Loops through all ontologies defined in src/config.py.
      #
      # Usage: ./scripts/rebuild_base.bash            # runs with defaults
      #        ./scripts/rebuild_base.bash --skip-eval   # skip final recall test
      #
      set -euo pipefail

      ROOT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )/.." && pwd )"
      cd "${ROOT_DIR}"

      DATA_DIR="data"

      echo "🧹  Cleaning old data and index artifacts..."
      # Use wildcards to clean all ontology-specific files and directories
      rm -f "${DATA_DIR}"/ontology_dump_*.json
      rm -f "${DATA_DIR}"/enriched_documents_*.json
      rm -f "${DATA_DIR}"/embeddings_*.json
      rm -f "${DATA_DIR}"/faiss_index_*.bin
      rm -f "${DATA_DIR}"/faiss_metadata_*.json
      rm -rf "${DATA_DIR}"/whoosh_index_*
      echo "✅  Cleanup complete."
      echo

      # Each script now handles looping internally based on src/config.py
      echo "①  Parsing all configured ontologies..."
      python -m src.ingestion.parse_ontology
      echo

      echo "②  Creating all enriched document files..."
      python -m src.ingestion.enrich_documents
      echo

      echo "③  Embedding all enriched document sets..."
      python -m src.embeddings.embed_docs
      echo

      echo "④  Building all FAISS vector stores..."
      python -m src.vector_store.faiss_store
      echo

      echo "⑤  Building all Whoosh lexical indexes..."
      python -m src.ingestion.build_lexical_index
      echo

      # The evaluation script may need updates to work with the new retriever
      if [[ "${1-}" != "--skip-eval" ]]; then
        echo "⑥  Running evaluator (Note: may need updates for multi-ontology setup)..."
        python -m src.evaluation.evaluate_retriever_recall
      fi

      echo "✅  Pipeline rebuild finished successfully."
    "reduce_evaluation_set.py": "# scripts/reduce_evaluation_set.py\n\nimport xml.etree.ElementTree\
      \ as ET\nimport os\nimport sys\nfrom collections import defaultdict\n\n# Add\
      \ project root to Python path to allow direct imports if needed in the future\n\
      PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n\
      sys.path.append(PROJECT_ROOT)\n\n# --- Configuration ---\n# Assumes the data\
      \ folder is at the project root\nINPUT_XML_PATH = os.path.join(PROJECT_ROOT,\
      \ \"data\", \"CafeteriaFCD_foodon.xml\")\nOUTPUT_XML_PATH = os.path.join(PROJECT_ROOT,\
      \ \"data\", \"CafeteriaFCD_foodon_unique.xml\")\n\ndef create_unique_dataset(input_file:\
      \ str, output_file: str):\n    \"\"\"\n    Parses an XML annotation file and\
      \ creates a new, smaller XML file\n    containing only one instance of each\
      \ unique (text, semantic_tags) pair.\n    \"\"\"\n    if not os.path.exists(input_file):\n\
      \        print(f\"Error: Input file not found at {input_file}\")\n        return\n\
      \n    try:\n        tree = ET.parse(input_file)\n        root = tree.getroot()\n\
      \    except ET.ParseError as e:\n        print(f\"Error parsing XML file {input_file}:\
      \ {e}\")\n        return\n\n    # A set to store the unique identifiers of annotations\
      \ we've already added.\n    # The identifier will be a tuple: (UPPERCASE_TEXT,\
      \ sorted_tuple_of_tags)\n    seen_annotations = set()\n    \n    # These will\
      \ be the root and single document of our new XML file\n    new_root = ET.Element(\"\
      collection\")\n    new_doc = ET.SubElement(new_root, \"document\")\n    new_doc.set(\"\
      id\", \"unique_annotations_document\")\n\n    total_annotations_processed =\
      \ 0\n    unique_annotations_found = 0\n\n    # Find all annotation tags anywhere\
      \ in the document\n    for annotation_node in root.findall('.//annotation'):\n\
      \        total_annotations_processed += 1\n        \n        text_node = annotation_node.find('text')\n\
      \        tags_node = annotation_node.find('infon[@key=\"semantic_tags\"]')\n\
      \n        # Ensure both text and tags exist to form a valid entry\n        if\
      \ (text_node is not None and text_node.text and\n                tags_node is\
      \ not None and tags_node.text):\n            \n            # 1. Normalize the\
      \ text to be case-insensitive\n            text = text_node.text.strip().upper()\n\
      \n            # 2. Normalize the tags to be order-insensitive\n            raw_tags\
      \ = tags_node.text.strip()\n            # Split by ';', strip whitespace, filter\
      \ out any empty strings, and sort\n            sorted_tags = sorted([tag.strip()\
      \ for tag in raw_tags.split(';') if tag.strip()])\n\n            # Create a\
      \ unique, hashable key for this annotation\n            # We convert the list\
      \ of tags to a tuple to make it hashable for the set\n            annotation_key\
      \ = (text, tuple(sorted_tags))\n\n            # 3. If we haven't seen this unique\
      \ combination before, add it\n            if annotation_key not in seen_annotations:\n\
      \                seen_annotations.add(annotation_key)\n                # Append\
      \ the original annotation_node to our new document\n                new_doc.append(annotation_node)\n\
      \                unique_annotations_found += 1\n\n    print(f\"Processed {total_annotations_processed}\
      \ total annotations.\")\n    print(f\"Found {unique_annotations_found} unique\
      \ (text, tags) pairs.\")\n\n    # Write the new, smaller XML tree to the output\
      \ file\n    new_tree = ET.ElementTree(new_root)\n    # The indent function (Python\
      \ 3.9+) makes the XML output readable\n    if sys.version_info >= (3, 9):\n\
      \        ET.indent(new_tree)\n        \n    new_tree.write(output_file, encoding='utf-8',\
      \ xml_declaration=True)\n    print(f\"Successfully saved unique dataset to:\
      \ {output_file}\")\n\n\nif __name__ == \"__main__\":\n    create_unique_dataset(INPUT_XML_PATH,\
      \ OUTPUT_XML_PATH)"
  "src":
    "__init__.py": ""
    "config.py": "# src/config.py\nimport os\nfrom dotenv import load_dotenv\nfrom\
      \ rdflib import Namespace # This is fine, though rdflib.Namespace is not directly\
      \ used for string constants below.\n\n# Project Root Directory\nPROJECT_ROOT\
      \ = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \".\"))\n\
      # print(f\"Project root directory: {PROJECT_ROOT}\") # Keep for debugging if\
      \ you like\n\n# Load environment variables from .env file\nload_dotenv(os.path.join(PROJECT_ROOT,\
      \ \".env\"))\n\n# Data Directory (for ontology dump, indexes, etc.)\nDATA_DIR\
      \ = os.path.join(PROJECT_ROOT, \"data\")\nos.makedirs(DATA_DIR, exist_ok=True)\n\
      \n# Ontologies Directory\nONTOLOGIES_DIR = os.path.join(PROJECT_ROOT, \"ontologies\"\
      )\nFOODON_PATH = os.path.join(ONTOLOGIES_DIR, \"foodon-with-syns.owl\")\nCHEBI_PATH\
      \ = os.path.join(ONTOLOGIES_DIR, \"chebi.owl\")\nTEST_FOODON_SNIPPET_PATH =\
      \ os.path.join(ONTOLOGIES_DIR, \"test_foodon_snippet.owl\")\n\n# --- NEW: Central\
      \ Ontologies Configuration ---\n# The keys ('foodon', 'chebi') are used as identifiers\
      \ throughout the pipeline.\nONTOLOGIES_CONFIG = {\n    'foodon': {\n       \
      \ 'path': os.path.join(ONTOLOGIES_DIR, \"foodon.owl\"),\n        'prefix': 'FOODON:',\n\
      \        'dump_json_path': os.path.join(DATA_DIR, \"ontology_dump_foodon.json\"\
      ),\n        'enriched_docs_path': os.path.join(DATA_DIR, f\"enriched_documents_foodon.json\"\
      ),\n        'embeddings_path': os.path.join(DATA_DIR, f\"embeddings_foodon.json\"\
      ),\n        'whoosh_index_dir': os.path.join(DATA_DIR, f\"whoosh_index_foodon\"\
      ),\n        'faiss_index_path': os.path.join(DATA_DIR, f\"faiss_index_foodon.bin\"\
      ),\n        'faiss_metadata_path': os.path.join(DATA_DIR, f\"faiss_metadata_foodon.json\"\
      ),\n    },\n    'chebi': {\n        'path': os.path.join(ONTOLOGIES_DIR, \"\
      chebi.owl\"),\n        'prefix': 'CHEBI:',\n        'dump_json_path': os.path.join(DATA_DIR,\
      \ \"ontology_dump_chebi.json\"),\n        'enriched_docs_path': os.path.join(DATA_DIR,\
      \ f\"enriched_documents_chebi.json\"),\n        'embeddings_path': os.path.join(DATA_DIR,\
      \ f\"embeddings_chebi.json\"),\n        'whoosh_index_dir': os.path.join(DATA_DIR,\
      \ f\"whoosh_index_chebi\"),\n        'faiss_index_path': os.path.join(DATA_DIR,\
      \ f\"faiss_index_chebi.bin\"),\n        'faiss_metadata_path': os.path.join(DATA_DIR,\
      \ f\"faiss_metadata_chebi.json\"),\n    }\n}\n# Ensure Whoosh directories exist\n\
      for name, config_data in ONTOLOGIES_CONFIG.items():\n    os.makedirs(config_data['whoosh_index_dir'],\
      \ exist_ok=True)\n\n\n# Output file from parse_ontology.py\nONTOLOGY_DUMP_JSON\
      \ = os.path.join(DATA_DIR, \"ontology_syns_dump.json\")\n\n# Output file for\
      \ enriched documents\nENRICHED_DOCUMENTS_FILE = os.path.join(DATA_DIR, \"enriched_documents.json\"\
      )\n\n# Output file for embeddings (used for building FAISS index)\nEMBEDDINGS_FILE\
      \ = os.path.join(DATA_DIR, \"embeddings.json\")\n\n# Whoosh Index Directory\n\
      WHOOSH_INDEX_DIR = os.path.join(DATA_DIR, \"whoosh_index\")\nos.makedirs(WHOOSH_INDEX_DIR,\
      \ exist_ok=True)\n\n# FAISS Index Paths\nFAISS_INDEX_PATH = os.path.join(DATA_DIR,\
      \ \"faiss_index.bin\")\nFAISS_METADATA_PATH = os.path.join(DATA_DIR, \"faiss_metadata.json\"\
      )\n\n# Embedding Model Configuration\nEMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\
      \n#EMBEDDING_MODEL_NAME = \"Lajavaness/bilingual-embedding-large\" \n# RERANKER_MODEL_NAME\
      \ = \"intfloat/e5-mistral-7b-instruct\"\nRERANKER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\
      \nEMBEDDING_BATCH_SIZE = 32 # For batch embedding script\nEMBEDDING_DEVICE =\
      \ 'cpu'  # or 'cuda' if available, for embedding script\n\n# Default K values\
      \ for retrieval\nDEFAULT_K_LEXICAL = 20\nDEFAULT_K_VECTOR = 20\nDEFAULT_RERANK_K\
      \ = DEFAULT_K_LEXICAL + DEFAULT_K_VECTOR \n\n# Namespaces (using string constants\
      \ for broader compatibility if rdflib not always imported)\nRDFS_NS_STR = \"\
      http://www.w3.org/2000/01/rdf-schema#\"\nRDF_NS_STR = \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\
      \nOWL_NS_STR = \"http://www.w3.org/2002/07/owl#\"\nSKOS_NS_STR = \"http://www.w3.org/2004/02/skos/core#\"\
      \nOBO_NS_STR = \"http://purl.obolibrary.org/obo/\"\nOBOINOWL_NS_STR = \"http://www.geneontology.org/formats/oboInOwl#\"\
      \nIAO_NS_STR = \"http://purl.obolibrary.org/obo/IAO_\"\n\n# For rdflib usage\
      \ where Namespace objects are preferred:\nRDFS_NS = Namespace(RDFS_NS_STR)\n\
      RDF_NS = Namespace(RDF_NS_STR)\nOWL_NS = Namespace(OWL_NS_STR)\nSKOS_NS = Namespace(SKOS_NS_STR)\n\
      OBO_NS = Namespace(OBO_NS_STR)\nOBOINOWL_NS = Namespace(OBOINOWL_NS_STR)\nIAO_NS\
      \ = Namespace(IAO_NS_STR)\n\n\n# Mapping of common relation URIs/CURIEs to human-readable\
      \ names and default prefixes\n# Used by parse_ontology.py and potentially enrich_documents.py\n\
      RELATION_CONFIG = {\n    \"obo:BFO_0000050\": {\"label\": \"part of\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0001000\": {\"label\": \"derives from\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0002200\": {\"label\": \"has phenotype\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0002451\": {\"label\": \"has part\", \"prefix\": \"\
      obo\"},\n    \"obo:FOODON_0000246\": {\"label\": \"has ingredient\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000056\": {\"label\": \"participates in\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000057\": {\"label\": \"has participant\", \"prefix\"\
      : \"obo\"},\n    \"obo:RO_0000085\": {\"label\": \"functionally related to\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002090\": {\"label\": \"adjacent to\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002131\": {\"label\": \"overlaps\", \"\
      prefix\": \"obo\"},\n    \"obo:RO_0002150\": {\"label\": \"connected to\", \"\
      prefix\": \"obo\"},\n    \"obo:RO_0002202\": {\"label\": \"develops from\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002350\": {\"label\": \"member of\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002351\": {\"label\": \"has member\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002353\": {\"label\": \"output of\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002440\": {\"label\": \"causally related\
      \ to\", \"prefix\": \"obo\"},\n    \"obo:RO_0002450\": {\"label\": \"contains\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002500\": {\"label\": \"surrounds\",\
      \ \"prefix\": \"obo\"},\n    \"obo:RO_0002573\": {\"label\": \"has quality\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0002606\": {\"label\": \"is transformation\
      \ of\", \"prefix\": \"obo\"},\n    \"obo:RO_0002607\": {\"label\": \"has input\"\
      , \"prefix\": \"obo\"},\n    \"obo:RO_0003000\": {\"label\": \"used in\", \"\
      prefix\": \"obo\"},\n    \"obo:FOODON_0000380\": {\"label\": \"has preparation\
      \ method\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000226\": {\"label\": \"\
      has food source\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000388\": {\"label\"\
      : \"has maturity state\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000286\"\
      : {\"label\": \"has packaging\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000240\"\
      : {\"label\": \"has preservation method\", \"prefix\": \"obo\"},\n    \"obo:FOODON_0000440\"\
      : {\"label\": \"has physical state\", \"prefix\": \"obo\"},\n    # \"obo:FOODON_\"\
      : {\"label\": \"FoodON specific relation\", \"prefix\": \"obo\"}, # This generic\
      \ one might be too broad\n    \"obo:ERO_0000039\": {\"label\": \"has nutrient\"\
      , \"prefix\": \"obo\"},\n    \"obo:ERO_0000589\": {\"label\": \"dietary context\
      \ of\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C25277\": {\"label\": \"is allergen\
      \ of\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C48670\": {\"label\": \"has active\
      \ ingredient\", \"prefix\": \"obo\"},\n    \"obo:NCIT_C64548\": {\"label\":\
      \ \"part of human diet\", \"prefix\": \"obo\"},\n    \"obo:PATO_0000001\": {\"\
      label\": \"has quality PATO\", \"prefix\": \"obo\"},\n}\n\nTARGET_RELATIONS_CURIES\
      \ = list(RELATION_CONFIG.keys())\n\n# CURIE Prefix Map for uri_to_curie and\
      \ curie_to_uri conversions\n# Ensure the keys are the *base URIs* that prefixes\
      \ are expected for.\nCURIE_PREFIX_MAP = {\n    \"http://purl.obolibrary.org/obo/FOODON_\"\
      : \"FOODON\",\n    \"http://purl.obolibrary.org/obo/BFO_\": \"BFO\",\n    \"\
      http://purl.obolibrary.org/obo/RO_\": \"RO\",\n    \"http://purl.obolibrary.org/obo/ERO_\"\
      : \"ERO\",\n    \"http://purl.obolibrary.org/obo/NCIT_\": \"NCIT\",\n    \"\
      http://purl.obolibrary.org/obo/PATO_\": \"PATO\",\n    \"http://purl.obolibrary.org/obo/IAO_\"\
      : \"IAO\",\n    \"http://www.w3.org/2000/01/rdf-schema#\": \"rdfs\",\n    \"\
      http://www.w3.org/1999/02/22-rdf-syntax-ns#\": \"rdf\",\n    \"http://www.w3.org/2002/07/owl#\"\
      : \"owl\",\n    \"http://www.w3.org/2004/02/skos/core#\": \"skos\",\n    \"\
      http://www.geneontology.org/formats/oboInOwl#\": \"oboInOwl\",\n    \"http://purl.obolibrary.org/obo/HANCESTRO_\"\
      : \"HANCESTRO\",  \n    \"http://purl.obolibrary.org/obo/GAZ_\": \"GAZ\",\n\
      \    \"http://purl.obolibrary.org/obo/CHEBI_\": \"CHEBI\",\n    \"http://purl.obolibrary.org/obo/NCBITaxon_\"\
      : \"NCBITaxon\",\n    \"http://purl.obolibrary.org/obo/UBERON_\": \"UBERON\"\
      ,\n    \"http://purl.obolibrary.org/obo/ENVO_\": \"ENVO\",\n    \"http://purl.obolibrary.org/obo/HP_\"\
      : \"HP\",\n    \"http://purl.obolibrary.org/obo/GO_\": \"GO\",\n    # General\
      \ OBO prefix - should be last or handled carefully to avoid overly broad matches\n\
      \    # if specific OBO sub-ontologies are listed above.\n    \"http://purl.obolibrary.org/obo/\"\
      : \"obo\",\n}\n\n# LLM API Key (placeholders)\nGEMINI_API_KEY = os.getenv(\"\
      GEMINI_API_KEY\")\n#LLM_SELECTOR_MODEL_NAME = \"gemini-1.5-flash-latest\"\n\
      LLM_SELECTOR_MODEL_NAME = \"gemini-2.5-flash-lite-preview-06-17\"\nOLLAMA_SELECTOR_MODEL_NAME\
      \ = 'llama3.1:8b'\n# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Logging\
      \ configuration\nLOG_LEVEL = \"INFO\"\nLOG_FILE = os.path.join(PROJECT_ROOT,\
      \ \"app.log\") # Example log file in project root"
    "main.py": "# src/main.py\nimport argparse\nimport json\nimport os\nimport sys\n\
      \n# --- Add project root to sys.path ---\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),\
      \ '..'))\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n\
      \n# Assuming you have an ollama_pipeline, but this works for gemini_pipeline\
      \ too\nfrom src.pipeline.ollama_pipeline import RAGPipeline \nfrom src.config\
      \ import DEFAULT_K_LEXICAL, DEFAULT_K_VECTOR\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"\
      Run the Onto-RAG pipeline with LLM selection.\")\n    parser.add_argument(\"\
      query\", type=str, help=\"The entity or text to search for (e.g., 'raw milk').\"\
      )\n    parser.add_argument(\"--lexical_k\", type=int, default=DEFAULT_K_LEXICAL,\
      \ help=f\"Initial candidates from lexical search (default: {DEFAULT_K_LEXICAL}).\"\
      )\n    parser.add_argument(\"--vector_k\", type=int, default=DEFAULT_K_VECTOR,\
      \ help=f\"Initial candidates from vector search (default: {DEFAULT_K_VECTOR}).\"\
      )\n    parser.add_argument(\"--top_n_rerank\", type=int, default=100, help=\"\
      Number of candidates to rerank and pass to the LLM (default: 10).\")\n    parser.add_argument(\"\
      --show_candidates\", action=\"store_true\", help=\"Show the list of candidates\
      \ provided to the LLM.\")\n    args = parser.parse_args()\n\n    pipeline =\
      \ None\n    try:\n        pipeline = RAGPipeline()\n        \n        # ---\
      \ MODIFIED: Unpack the tuple returned by pipeline.run ---\n        result_tuple\
      \ = pipeline.run(\n            query=args.query,\n            lexical_k=args.lexical_k,\n\
      \            vector_k=args.vector_k,\n            rerank_top_n=args.top_n_rerank\n\
      \        )\n        \n        # Handle case where pipeline returns None\n  \
      \      if not result_tuple:\n            final_result, candidates = None, []\n\
      \        else:\n            final_result, candidates = result_tuple\n\n    \
      \    # --- Print the final selection (no changes here) ---\n        print(\"\
      \\n--- Final LLM Selection ---\")\n        if not final_result:\n          \
      \  print(\"Could not determine a matching ontology term.\")\n        else:\n\
      \            print(f\"Query: '{args.query}'\")\n            print(\"---------------------------\"\
      )\n            print(f\"Chosen Term ID: {final_result.get('id', 'N/A')}\")\n\
      \            print(f\"Label:          {final_result.get('label', 'N/A')}\")\n\
      \            print(f\"Confidence:     {final_result.get('confidence_score',\
      \ 0.0):.1%}\")\n            print(f\"Definition:     {final_result.get('definition',\
      \ 'N/A')}\")\n            print(f\"Synonyms:       {'; '.join(final_result.get('synonyms',\
      \ [])) or 'None'}\")\n            print(\"\\nLLM Explanation:\")\n         \
      \   print(f\"  > {final_result.get('explanation', 'No explanation provided.')}\"\
      )\n        print(\"---------------------------\\n\")\n\n        # --- NEW: Print\
      \ the candidates if requested ---\n        if args.show_candidates and candidates:\n\
      \            print(f\"--- Top {len(candidates)} Candidates Provided to LLM ---\"\
      )\n            chosen_id = final_result.get('id') if final_result else None\n\
      \            \n            for i, candidate in enumerate(candidates):\n    \
      \            # Fetch full details for printing\n                details = pipeline.retriever.get_term_details(candidate.get('id'))\n\
      \                if not details: continue\n\n                marker = \"⭐️\"\
      \ if details.get('id') == chosen_id else \"  \"\n                rerank_score\
      \ = candidate.get('rerank_score')\n                score_str = f\"(Score: {rerank_score:.4f})\"\
      \ if rerank_score is not None else \"\"\n\n                print(f\"{i+1}. {marker}\
      \ {details.get('label', 'N/A')} `{details.get('id', 'N/A')}` {score_str}\")\n\
      \                definition = details.get('definition')\n                if\
      \ definition:\n                    print(f\"       Def: {definition[:150]}...\"\
      )  # Print first 150 chars of definition\n                else:\n          \
      \          print(f\"       Def: No definition available.\")\n\n            \
      \    if details.get('synonyms'):\n                    print(f\"       Syns:\
      \ {'; '.join(details.get('synonyms', []))}\")\n                print(\"-\" *\
      \ 20)\n            print(\"-------------------------------------------\\n\"\
      )\n        elif args.show_candidates:\n            print(\"--- No Candidates\
      \ to Display ---\")\n\n\n    except Exception as e:\n        print(f\"\\nAn\
      \ error occurred during the pipeline execution: {e}\", file=sys.stderr)\n  \
      \      import traceback\n        traceback.print_exc()\n    finally:\n     \
      \   if pipeline:\n            pipeline.close()\n\nif __name__ == \"__main__\"\
      :\n    main()"
    "run_on_off.py": "# scripts/map_ingredients.py\n\"\"\"\nThis script loads extracted\
      \ ingredient entities from a JSON file,\nruns them through the RAG pipeline\
      \ to link them to ontology terms,\nand saves the structured results to a new\
      \ JSON file.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport logging\n\
      from tqdm import tqdm\n\n# --- Add project root to sys.path ---\n# This allows\
      \ the script to import modules from the 'src' directory\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),\
      \ '..'))\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n\
      \nfrom src.pipeline.gemini_pipeline import RAGPipeline\nfrom src import config\n\
      \n# --- Configuration ---\nLOGGING_LEVEL = logging.INFO\n# Limit the number\
      \ of products to process. Set to None to process all.\nPRODUCT_LIMIT = 5 \n\n\
      INPUT_FILE = os.path.join(PROJECT_ROOT, 'data', 'outputs', 'parsed_ingredients_output.json')\n\
      OUTPUT_FILE = os.path.join(PROJECT_ROOT, 'data', 'outputs', 'mapped_ingredients_output.json')\n\
      \n\n# --- Setup Logging ---\nlogging.basicConfig(level=LOGGING_LEVEL, format='%(asctime)s\
      \ - %(levelname)s - %(name)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\
      \n\ndef main():\n    \"\"\"\n    Main function to run the ingredient mapping\
      \ process.\n    \"\"\"\n    logger.info(\"Starting ingredient to ontology mapping\
      \ process...\")\n    \n    # Ensure the output directory exists\n    os.makedirs(os.path.dirname(OUTPUT_FILE),\
      \ exist_ok=True)\n\n    pipeline = None\n    try:\n        # --- 1. Load Input\
      \ Data ---\n        logger.info(f\"Loading ingredients from: {INPUT_FILE}\"\
      )\n        if not os.path.exists(INPUT_FILE):\n            logger.error(f\"\
      Input file not found: {INPUT_FILE}\")\n            logger.error(\"Please ensure\
      \ you have run the parsing script first.\")\n            return\n\n        with\
      \ open(INPUT_FILE, 'r', encoding='utf-8') as f:\n            all_ingredients_data\
      \ = json.load(f)\n\n        # --- 2. Initialize RAG Pipeline ---\n        logger.info(\"\
      Initializing RAG pipeline...\")\n        pipeline = RAGPipeline()\n        logger.info(\"\
      RAG pipeline initialized successfully.\")\n\n        # --- 3. Process Ingredients\
      \ ---\n        all_mappings = {}\n        \n        # Get items to process,\
      \ applying the limit if specified\n        items_to_process = list(all_ingredients_data.items())\n\
      \        if PRODUCT_LIMIT is not None:\n            items_to_process = items_to_process[:PRODUCT_LIMIT]\n\
      \            logger.warning(f\"Processing a limited set of {PRODUCT_LIMIT} products.\"\
      )\n\n        # Use tqdm for a progress bar\n        for product_id, ingredients\
      \ in tqdm(items_to_process, desc=\"Processing Products\"):\n            logger.info(f\"\
      --- Processing Product ID: {product_id} ---\")\n            \n            product_mappings\
      \ = []\n            unique_ingredients = sorted(list(set(ingredients))) # Process\
      \ unique ingredients to avoid duplicate work\n\n            for ingredient_query\
      \ in unique_ingredients:\n                logger.info(f\"Querying for: '{ingredient_query}'\"\
      )\n                \n                # Run the pipeline for the ingredient string\n\
      \                mapping_result = pipeline.run(query=ingredient_query)\n   \
      \             \n                if mapping_result:\n                    logger.info(f\"\
      \  -> Found mapping: '{mapping_result.get('label')}' (ID: {mapping_result.get('id')})\"\
      )\n                else:\n                    logger.warning(f\"  -> No mapping\
      \ found for '{ingredient_query}'\")\n                \n                # Store\
      \ the result in a structured format\n                product_mappings.append({\n\
      \                    \"original_ingredient\": ingredient_query,\n          \
      \          \"mapping_result\": mapping_result if mapping_result else \"No mapping\
      \ found\"\n                })\n            \n            all_mappings[product_id]\
      \ = product_mappings\n\n        # --- 4. Save Results ---\n        logger.info(f\"\
      Saving mapped results to: {OUTPUT_FILE}\")\n        with open(OUTPUT_FILE, 'w',\
      \ encoding='utf-8') as f:\n            json.dump(all_mappings, f, indent=4)\n\
      \            \n        logger.info(\"Mapping process completed successfully!\"\
      )\n\n    except Exception as e:\n        logger.error(f\"An unexpected error\
      \ occurred during the mapping process: {e}\", exc_info=True)\n    finally:\n\
      \        # --- 5. Clean up ---\n        if pipeline:\n            logger.info(\"\
      Closing pipeline resources.\")\n            pipeline.close()\n\nif __name__\
      \ == \"__main__\":\n    main()"
    "tempCodeRunnerFile.py": |-
      f
    "embeddings":
      "embed_docs.py": "# src/embeddings/embed_docs.py\nimport json\nimport os\nimport\
        \ logging\nimport time\nfrom typing import List, Dict, Any\n\n# Ensure src\
        \ is in path for imports if run directly\nimport sys\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..')))\n\ntry:\n    from sentence_transformers import SentenceTransformer\n\
        \    import torch \nexcept ModuleNotFoundError:\n    logging.error(\"Modules\
        \ sentence_transformers or torch not found. Please install them: pip install\
        \ sentence-transformers torch\")\n    sys.exit(1)\n\n# Changed: Use central\
        \ config and specific constants\nfrom src.config import ONTOLOGIES_CONFIG,\
        \ EMBEDDING_MODEL_NAME, EMBEDDING_BATCH_SIZE, EMBEDDING_DEVICE\n\nlogging.basicConfig(level=logging.INFO,\
        \ format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef load_enriched_documents(file_path:\
        \ str) -> List[Dict[str, Any]]:\n    \"\"\"Loads enriched documents from a\
        \ JSON file.\"\"\"\n    logging.info(f\"Loading enriched documents from {file_path}...\"\
        )\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n \
        \           documents = json.load(f)\n        logging.info(f\"Successfully\
        \ loaded {len(documents)} documents.\")\n        return documents\n    except\
        \ FileNotFoundError:\n        logging.error(f\"Error: Enriched documents file\
        \ not found at {file_path}\")\n        return []\n    except json.JSONDecodeError:\n\
        \        logging.error(f\"Error: Could not decode JSON from {file_path}\"\
        )\n        return []\n\ndef batch_embed_documents(\n    documents: List[Dict[str,\
        \ Any]], \n    model: SentenceTransformer,\n    batch_size: int = 32\n) ->\
        \ List[Dict[str, Any]]:\n    \"\"\"\n    Generates embeddings for a list of\
        \ documents using a pre-loaded SentenceTransformer model.\n\n    Args:\n \
        \       documents: A list of dictionaries, each with \"id\", \"label\", and\
        \ \"text\" keys.\n        model: The pre-loaded SentenceTransformer model\
        \ instance.\n        batch_size: The number of documents to process in each\
        \ batch.\n\n    Returns:\n        A list of dictionaries, each containing\
        \ \"id\", \"label\", and \"embedding\".\n    \"\"\"\n    if not documents:\n\
        \        logging.warning(\"No documents provided for embedding.\")\n     \
        \   return []\n\n    logging.info(\"Starting embedding process...\")\n   \
        \ texts_to_embed = [doc.get(\"text\", \"\") for doc in documents]\n    ids\
        \ = [doc.get(\"id\") for doc in documents]\n    labels = [doc.get(\"label\"\
        ) for doc in documents]\n\n    all_embeddings_data = []\n    start_time =\
        \ time.time()\n\n    for i in range(0, len(texts_to_embed), batch_size):\n\
        \        batch_texts = texts_to_embed[i:i+batch_size]\n        batch_ids =\
        \ ids[i:i+batch_size]\n        batch_labels = labels[i:i+batch_size]\n\n \
        \       logging.info(f\"  Processing batch {i//batch_size + 1}/{(len(texts_to_embed)\
        \ - 1)//batch_size + 1} (size: {len(batch_texts)})\")\n        \n        try:\n\
        \            embeddings = model.encode(batch_texts, convert_to_tensor=False,\
        \ show_progress_bar=False)\n            \n            for idx, embedding in\
        \ enumerate(embeddings):\n                all_embeddings_data.append({\n \
        \                   \"id\": batch_ids[idx],\n                    \"label\"\
        : batch_labels[idx],\n                    \"embedding\": embedding.tolist()\n\
        \                })\n        except Exception as e:\n            logging.error(f\"\
        Error embedding batch starting at index {i}: {e}\")\n\n    end_time = time.time()\n\
        \    logging.info(f\"Embedding process completed for {len(all_embeddings_data)}\
        \ documents in {end_time - start_time:.2f} seconds.\")\n    return all_embeddings_data\n\
        \ndef save_embeddings(embeddings_data: List[Dict[str, Any]], output_path:\
        \ str):\n    \"\"\"Saves the embeddings data to a JSON file.\"\"\"\n    if\
        \ not embeddings_data:\n        logging.warning(\"No embeddings data to save.\"\
        )\n        return\n\n    # Ensure output directory exists\n    os.makedirs(os.path.dirname(output_path),\
        \ exist_ok=True)\n    \n    logging.info(f\"Saving {len(embeddings_data)}\
        \ embeddings to {output_path}...\")\n    try:\n        with open(output_path,\
        \ 'w', encoding='utf-8') as f:\n            json.dump(embeddings_data, f)\
        \ # No indent for smaller file size\n        logging.info(\"Successfully saved\
        \ embeddings.\")\n    except IOError as e:\n        logging.error(f\"Error:\
        \ Could not write embeddings to {output_path}: {e}\")\n    except TypeError\
        \ as e:\n        logging.error(f\"TypeError during JSON serialization: {e}.\
        \ Check embedding data format.\")\n\ndef main():\n    \"\"\"Main function\
        \ to generate and save document embeddings for all configured ontologies.\"\
        \"\"\n    logging.info(\"--- Starting Embedding Generation for All Ontologies\
        \ ---\")\n\n    # 1. Load the model once to be reused for all ontologies\n\
        \    logging.info(f\"Loading SentenceTransformer model: {EMBEDDING_MODEL_NAME}\"\
        )\n    try:\n        model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=EMBEDDING_DEVICE,\
        \ trust_remote_code=True)\n        logging.info(f\"Model loaded successfully\
        \ on device: '{EMBEDDING_DEVICE}'\")\n        if hasattr(model, 'max_seq_length'):\n\
        \             logging.info(f\"Model max sequence length: {model.max_seq_length}\"\
        )\n    except Exception as e:\n        logging.error(f\"Fatal error loading\
        \ model: {e}\")\n        return\n\n    # 2. Loop over all configured ontologies\n\
        \    for name, config_data in ONTOLOGIES_CONFIG.items():\n        enriched_docs_path\
        \ = config_data.get('enriched_docs_path')\n        embeddings_output_path\
        \ = config_data.get('embeddings_path')\n        \n        logging.info(f\"\
        \\n--- Processing Ontology: {name} ---\")\n\n        if not enriched_docs_path\
        \ or not embeddings_output_path:\n            logging.warning(f\"Config for\
        \ '{name}' is missing 'enriched_docs_path' or 'embeddings_path'. Skipping.\"\
        )\n            continue\n        \n        logging.info(f\"Input: {enriched_docs_path}\"\
        )\n        logging.info(f\"Output: {embeddings_output_path}\")\n\n       \
        \ enriched_docs = load_enriched_documents(enriched_docs_path)\n        if\
        \ not enriched_docs:\n            logging.error(f\"Failed to load or empty\
        \ enriched documents file. Skipping '{name}'.\")\n            continue\n\n\
        \        embeddings_data = batch_embed_documents(\n            documents=enriched_docs,\n\
        \            model=model, # Pass the single, pre-loaded model\n          \
        \  batch_size=EMBEDDING_BATCH_SIZE\n        )\n\n        if embeddings_data:\n\
        \            save_embeddings(embeddings_data, embeddings_output_path)\n  \
        \          logging.info(f\"Generated and saved {len(embeddings_data)} embeddings\
        \ for '{name}'.\")\n        else:\n            logging.warning(f\"No embeddings\
        \ were generated for '{name}'.\")\n    \n    logging.info(\"\\n--- All Embedding\
        \ Generation Complete ---\")\n\nif __name__ == \"__main__\":\n    main()"
    "evaluation":
      "evaluate_pipeline.py": "# src/evaluation/evaluate_pipeline.py\n\nimport xml.etree.ElementTree\
        \ as ET\nimport os\nimport sys\nimport logging\nimport json\nfrom typing import\
        \ Tuple, List, Dict\n\n# Add project root to Python path to allow direct imports\
        \ from src\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..'))\nsys.path.append(PROJECT_ROOT)\n\nfrom src.retriever.hybrid_retriever\
        \ import HybridRetriever\nfrom src.rag_selectors.ollama_selector import OllamaSelector\
        \ # Import the new selector\nfrom src.rag_selectors.gemini_selector import\
        \ GeminiSelector # Import the old selector for reference\n\nfrom src.config\
        \ import (\n    ONTOLOGY_DUMP_JSON,\n    WHOOSH_INDEX_DIR,\n    FAISS_INDEX_PATH,\n\
        \    FAISS_METADATA_PATH,\n    EMBEDDING_MODEL_NAME,\n    CURIE_PREFIX_MAP,\n\
        \    DEFAULT_K_LEXICAL,\n    DEFAULT_K_VECTOR,\n    OLLAMA_SELECTOR_MODEL_NAME\
        \ # Import the model name\n)\nfrom src.utils.ontology_utils import uri_to_curie\n\
        \n# --- Configuration for this specific evaluation script ---\nEVALUATION_XML_FILE\
        \ = os.path.join(PROJECT_ROOT, \"data\", \"CafeteriaFCD_foodon_unique.xml\"\
        )\n# File to save detailed results of incorrect selections\nEVALUATION_OUTPUT_FILE\
        \ = os.path.join(PROJECT_ROOT, \"evaluation_results.json\")\n\n# --- Logging\
        \ Setup ---\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s -\
        \ %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n\
        # --- Re-used from evaluate_retriever_recall.py (no changes needed) ---\n\
        def parse_evaluation_xml(xml_file_path: str) -> list:\n    \"\"\"\n    Parses\
        \ the evaluation XML file to extract entities and their ground truth semantic\
        \ tags.\n    (This function is identical to the one in evaluate_retriever_recall.py)\n\
        \    \"\"\"\n    if not os.path.exists(xml_file_path):\n        logger.error(f\"\
        Evaluation XML file not found: {xml_file_path}\")\n        return []\n\n \
        \   gold_standard_data = []\n    try:\n        tree = ET.parse(xml_file_path)\n\
        \        root = tree.getroot()\n        for doc_idx, document_node in enumerate(root.findall('.//document')):\n\
        \            doc_id_node = document_node.find('id')\n            doc_id =\
        \ doc_id_node.text if doc_id_node is not None else f\"doc_{doc_idx}\"\n  \
        \          \n            for ann_idx, annotation_node in enumerate(document_node.findall('annotation')):\n\
        \                entity_text_node = annotation_node.find('text')\n       \
        \         semantic_tags_node = annotation_node.find('infon[@key=\"semantic_tags\"\
        ]')\n                ann_id_val = annotation_node.get('id', f\"ann_{doc_idx}_{ann_idx}\"\
        )\n\n                if entity_text_node is not None and semantic_tags_node\
        \ is not None and entity_text_node.text is not None and semantic_tags_node.text\
        \ is not None:\n                    entity_text = entity_text_node.text.strip()\n\
        \                    raw_tags = semantic_tags_node.text.strip()\n        \
        \            true_uris = {tag.strip() for tag in raw_tags.split(';') if tag.strip()}\n\
        \                    true_curies = {uri_to_curie(uri, CURIE_PREFIX_MAP) for\
        \ uri in true_uris} - {None}\n                    \n                    if\
        \ entity_text and true_curies:\n                        gold_standard_data.append({\n\
        \                            'text': entity_text,\n                      \
        \      'true_curies': true_curies,\n                            'doc_id':\
        \ doc_id,\n                            'ann_id': ann_id_val\n            \
        \            })\n    except ET.ParseError as e:\n        logger.error(f\"\
        Error parsing XML file {xml_file_path}: {e}\")\n        return []\n    \n\
        \    logger.info(f\"Successfully parsed {len(gold_standard_data)} entities\
        \ from {xml_file_path}\")\n    return gold_standard_data\n\n# --- New Evaluation\
        \ Logic for the Full Pipeline ---\ndef evaluate_full_pipeline(\n    retriever:\
        \ HybridRetriever, \n    selector: GeminiSelector,  #\n    gold_standard_data:\
        \ list, \n    lexical_k: int, \n    vector_k: int\n) -> Tuple[float, int,\
        \ int, int, int, List[Dict]]:\n    \"\"\"\n    Evaluates the full retrieval\
        \ and selection pipeline against the gold standard data.\n\n    Returns:\n\
        \        A tuple containing:\n        - accuracy_score (float)\n        -\
        \ total_entities_processed (int)\n        - correct_selections (int)\n   \
        \     - retrieval_failures (int): Count of queries that returned no candidates.\n\
        \        - selection_failures (int): Count of queries where the selector failed.\n\
        \        - incorrect_selections (list): A list of dictionaries detailing the\
        \ misses.\n    \"\"\"\n    total_entities_processed = 0\n    correct_selections\
        \ = 0\n    retrieval_failures = 0\n    selection_failures = 0\n    incorrect_selections\
        \ = []\n\n    if not gold_standard_data:\n        logger.warning(\"No gold\
        \ standard data provided for evaluation.\")\n        return 0.0, 0, 0, 0,\
        \ 0, []\n\n    for i, item in enumerate(gold_standard_data):\n        query_text\
        \ = item['text']\n        true_curies = item['true_curies']\n        \n  \
        \      total_entities_processed += 1\n        logger.info(f\"--- Processing\
        \ ({i+1}/{len(gold_standard_data)}): '{query_text}' (True: {true_curies})\
        \ ---\")\n\n        # 1. RETRIEVAL STEP: Get candidates\n        try:\n  \
        \          # Note: The HybridRetriever's search method should return a combined,\
        \ reranked list of candidates.\n            # We adapt to the provided retriever's\
        \ output format.\n            retriever_output_dict = retriever.search(\n\
        \                query_string=query_text, lexical_limit=lexical_k, vector_k=vector_k\n\
        \            )\n            lexical_results = retriever_output_dict.get(\"\
        lexical_results\", [])\n            vector_results = retriever_output_dict.get(\"\
        vector_results\", [])\n\n            # Combine and deduplicate\n         \
        \   candidates = []\n            seen_ids = set()\n            for doc in\
        \ lexical_results + vector_results:\n                doc_id = doc.get('id')\n\
        \                if doc_id and doc_id not in seen_ids:\n                 \
        \   candidates.append(doc)\n                    seen_ids.add(doc_id)\n   \
        \         \n            if not candidates:\n                logger.warning(f\"\
        Retrieval Failure: No candidates found for '{query_text}'.\")\n          \
        \      retrieval_failures += 1\n                continue\n\n        except\
        \ Exception as e:\n            logger.error(f\"Error during retrieval for\
        \ query '{query_text}': {e}\", exc_info=True)\n            retrieval_failures\
        \ += 1\n            continue\n\n        # 2. SELECTION STEP: Use OllamaSelector\n\
        \        try:\n            selection_result = selector.select_best_term(query=query_text,\
        \ candidates=candidates)\n\n            if not selection_result or \"chosen_id\"\
        \ not in selection_result:\n                logger.warning(f\"Selection Failure:\
        \ Selector did not return a valid choice for '{query_text}'.\")\n        \
        \        selection_failures += 1\n                continue\n            \n\
        \            chosen_curie = selection_result[\"chosen_id\"]\n\n        except\
        \ Exception as e:\n            logger.error(f\"Error during selection for\
        \ query '{query_text}': {e}\", exc_info=True)\n            selection_failures\
        \ += 1\n            continue\n        \n        # 3. COMPARISON STEP\n   \
        \     if chosen_curie in true_curies:\n            correct_selections += 1\n\
        \            logger.info(f\"✅ HIT! Query: '{query_text}'. Chosen: '{chosen_curie}'.\
        \ Correct.\")\n        else:\n            logger.info(f\"❌ MISS! Query: '{query_text}'.\
        \ Chosen: '{chosen_curie}', Expected: {true_curies}.\")\n            incorrect_selections.append({\n\
        \                \"query\": query_text,\n                \"chosen_curie\"\
        : chosen_curie,\n                \"true_curies\": list(true_curies),\n   \
        \             \"explanation\": selection_result.get(\"explanation\", \"N/A\"\
        ),\n                \"candidates_provided\": [cand['id'] for cand in candidates]\n\
        \            })\n\n    if (total_entities_processed - retrieval_failures -\
        \ selection_failures) == 0:\n        accuracy_score = 0.0\n    else:\n   \
        \     # Accuracy is based on the number of times the selector could make a\
        \ choice\n        accuracy_score = correct_selections / (total_entities_processed\
        \ - retrieval_failures - selection_failures)\n\n    return accuracy_score,\
        \ total_entities_processed, correct_selections, retrieval_failures, selection_failures,\
        \ incorrect_selections\n\ndef main():\n    logger.info(\"Starting Full Pipeline\
        \ Evaluation Script...\")\n    \n    # 1. Check for necessary model name in\
        \ config\n    if not OLLAMA_SELECTOR_MODEL_NAME:\n        logger.error(\"\
        OLLAMA_SELECTOR_MODEL_NAME is not set in config.py. Exiting.\")\n        return\n\
        \n    # 2. Parse Gold Standard XML\n    logger.info(f\"Loading gold standard\
        \ data from: {EVALUATION_XML_FILE}\")\n    gold_standard_data = parse_evaluation_xml(EVALUATION_XML_FILE)\n\
        \    if not gold_standard_data:\n        logger.error(\"Failed to load or\
        \ parse gold standard data. Exiting.\")\n        return\n\n    # 3. Initialize\
        \ Pipeline Components\n    try:\n        logger.info(\"Initializing HybridRetriever...\"\
        )\n        retriever = HybridRetriever(\n            ontology_data_path=ONTOLOGY_DUMP_JSON,\n\
        \            whoosh_index_dir=WHOOSH_INDEX_DIR,\n            faiss_index_path=FAISS_INDEX_PATH,\n\
        \            faiss_metadata_path=FAISS_METADATA_PATH,\n            embedding_model_name=EMBEDDING_MODEL_NAME\n\
        \        )\n        logger.info(\"HybridRetriever initialized successfully.\"\
        )\n\n        logger.info(f\"Initializing OllamaSelector with model '{OLLAMA_SELECTOR_MODEL_NAME}'...\"\
        )\n        selector = GeminiSelector(retriever=retriever)\n        logger.info(\"\
        OllamaSelector initialized successfully.\")\n\n    except Exception as e:\n\
        \        logger.error(f\"Failed to initialize pipeline components: {e}\",\
        \ exc_info=True)\n        return\n\n    # 4. Perform Evaluation\n    logger.info(\n\
        \        f\"Starting evaluation with Retriever(lexical_k={DEFAULT_K_LEXICAL},\
        \ vector_k={DEFAULT_K_VECTOR}) \"\n        f\"and Selector(model={OLLAMA_SELECTOR_MODEL_NAME})\"\
        \n    )\n    \n    accuracy, total, correct, ret_fails, sel_fails, misses\
        \ = evaluate_full_pipeline(\n        retriever, \n        selector, \n   \
        \     gold_standard_data, \n        lexical_k=DEFAULT_K_LEXICAL,\n       \
        \ vector_k=DEFAULT_K_VECTOR\n    )\n\n    # 5. Print and Save Results\n  \
        \  logger.info(\"--- Evaluation Complete ---\")\n    logger.info(f\"Total\
        \ entities evaluated: {total}\")\n    logger.info(f\"Retrieval Failures (no\
        \ candidates): {ret_fails}\")\n    logger.info(f\"Selection Failures (LLM\
        \ error): {sel_fails}\")\n    logger.info(\"-\" * 27)\n    valid_attempts\
        \ = total - ret_fails - sel_fails\n    logger.info(f\"Valid attempts for selector:\
        \ {valid_attempts}\")\n    logger.info(f\"Correct selections (Hits): {correct}\"\
        )\n    if valid_attempts > 0:\n        logger.info(f\"Accuracy: {accuracy:.4f}\
        \ ({correct}/{valid_attempts})\")\n    else:\n        logger.info(\"Accuracy:\
        \ N/A (no valid attempts were made)\")\n    \n    logger.info(f\"Saving {len(misses)}\
        \ incorrect selections to {EVALUATION_OUTPUT_FILE}\")\n    with open(EVALUATION_OUTPUT_FILE,\
        \ 'w', encoding='utf-8') as f:\n        json.dump(misses, f, indent=4)\n \
        \       \n    logger.info(\"Evaluation finished.\")\n\nif __name__ == \"__main__\"\
        :\n    if not os.path.exists(EVALUATION_XML_FILE):\n        logger.error(f\"\
        Evaluation XML file '{EVALUATION_XML_FILE}' not found.\")\n    else:\n   \
        \     main()"
      "evaluate_retriever_recall.py": "# src/evaluation/evaluate_retriever_recall.py\n\
        \nimport xml.etree.ElementTree as ET\nimport os\nimport sys\nimport logging\n\
        from collections import defaultdict\n\n# Add project root to Python path to\
        \ allow direct imports from src\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..'))\nsys.path.append(PROJECT_ROOT)\n\nfrom src.retriever.hybrid_retriever\
        \ import HybridRetriever\nfrom src.config import (\n    ONTOLOGY_DUMP_JSON,\n\
        \    WHOOSH_INDEX_DIR,\n    FAISS_INDEX_PATH,\n    FAISS_METADATA_PATH,\n\
        \    EMBEDDING_MODEL_NAME,\n    CURIE_PREFIX_MAP,\n    DEFAULT_K_LEXICAL,\n\
        \    DEFAULT_K_VECTOR,\n    DEFAULT_RERANK_K\n)\nfrom src.utils.ontology_utils\
        \ import uri_to_curie\n\n# --- Configuration for this specific evaluation\
        \ script ---\n# Path to your evaluation XML file\nEVALUATION_XML_FILE = os.path.join(PROJECT_ROOT,\
        \ \"data\", \"CafeteriaFCD_foodon_unique.xml\")\n# Top K results from the\
        \ combined retriever output to consider for a match\nRECALL_AT_K = DEFAULT_RERANK_K\
        \ # How many retrieved items to check for a match\n\n# --- Logging Setup ---\n\
        # Stays with basicConfig as src.utils.logging.get_logger is \"to be developed\"\
        \nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s\
        \ - %(message)s')\nlogger = logging.getLogger(__name__)\n\ndef parse_evaluation_xml(xml_file_path:\
        \ str) -> list:\n    \"\"\"\n    Parses the evaluation XML file to extract\
        \ entities and their ground truth semantic tags.\n\n    Args:\n        xml_file_path\
        \ (str): Path to the XML file.\n\n    Returns:\n        list: A list of dictionaries,\
        \ each containing:\n              {'text': str, 'true_curies': set_of_str,\
        \ 'doc_id': str, 'ann_id': str}\n              Returns an empty list if parsing\
        \ fails.\n    \"\"\"\n    if not os.path.exists(xml_file_path):\n        logger.error(f\"\
        Evaluation XML file not found: {xml_file_path}\")\n        return []\n\n \
        \   gold_standard_data = []\n    try:\n        tree = ET.parse(xml_file_path)\n\
        \        root = tree.getroot()\n        for doc_idx, document_node in enumerate(root.findall('.//document')):\n\
        \            doc_id_node = document_node.find('id')\n            doc_id =\
        \ doc_id_node.text if doc_id_node is not None else f\"doc_{doc_idx}\"\n  \
        \          \n            annotations = document_node.findall('annotation')\n\
        \            for ann_idx, annotation_node in enumerate(annotations):\n   \
        \             entity_text_node = annotation_node.find('text')\n          \
        \      semantic_tags_node = annotation_node.find('infon[@key=\"semantic_tags\"\
        ]')\n                ann_id_val = annotation_node.get('id', f\"ann_{doc_idx}_{ann_idx}\"\
        )\n\n\n                if entity_text_node is not None and semantic_tags_node\
        \ is not None and entity_text_node.text is not None and semantic_tags_node.text\
        \ is not None:\n                    entity_text = entity_text_node.text.strip()\n\
        \                    \n                    raw_tags = semantic_tags_node.text.strip()\n\
        \                    true_uris = {tag.strip() for tag in raw_tags.split(';')\
        \ if tag.strip()}\n                    \n                    true_curies =\
        \ set()\n                    for uri in true_uris:\n                     \
        \   try:\n                            # Adjusted: Use CURIE_PREFIX_MAP\n \
        \                           curie = uri_to_curie(uri, CURIE_PREFIX_MAP)\n\
        \                            if curie: \n                                true_curies.add(curie)\n\
        \                            else:\n                                logger.warning(f\"\
        Could not convert URI to CURIE: {uri} for entity '{entity_text}' in {doc_id}\
        \ (ann: {ann_id_val})\")\n                        except Exception as e:\n\
        \                            logger.error(f\"Error converting URI {uri} to\
        \ CURIE: {e}\")\n                    \n                    if entity_text\
        \ and true_curies:\n                        gold_standard_data.append({\n\
        \                            'text': entity_text,\n                      \
        \      'true_curies': true_curies,\n                            'doc_id':\
        \ doc_id,\n                            'ann_id': ann_id_val\n            \
        \            })\n                    elif entity_text: # Has text but no convertible\
        \ true CURIEs\n                        logger.warning(f\"Entity '{entity_text}'\
        \ in {doc_id} (ann: {ann_id_val}) had no convertible true CURIEs from URIs:\
        \ {true_uris}\")\n\n                # else: # This can be too verbose if many\
        \ annotations don't have these specific fields\n                #     logger.debug(f\"\
        Annotation missing text or semantic_tags in {doc_id}, ann_id={ann_id_val}\"\
        )\n\n\n    except ET.ParseError as e:\n        logger.error(f\"Error parsing\
        \ XML file {xml_file_path}: {e}\")\n        return []\n    except Exception\
        \ as e:\n        logger.error(f\"An unexpected error occurred during XML parsing:\
        \ {e}\", exc_info=True)\n        return []\n        \n    logger.info(f\"\
        Successfully parsed {len(gold_standard_data)} entities with text and true\
        \ CURIEs from {xml_file_path}\")\n    return gold_standard_data\n\ndef evaluate_retriever(retriever:\
        \ HybridRetriever, gold_standard_data: list, recall_at_k: int, lexical_k:\
        \ int, vector_k: int) -> tuple[float, int, int]:\n    \"\"\"\n    Evaluates\
        \ the retriever against the gold standard data.\n\n    Args:\n        retriever\
        \ (HybridRetriever): The initialized hybrid retriever.\n        gold_standard_data\
        \ (list): List of gold standard entities and their CURIEs.\n        recall_at_k\
        \ (int): The K value for Recall@K (slice of combined results).\n        lexical_k\
        \ (int): Number of results to fetch from lexical search.\n        vector_k\
        \ (int): Number of results to fetch from vector search.\n\n    Returns:\n\
        \        tuple: (recall_score, total_entities_processed, hits)\n    \"\"\"\
        \n    total_entities_processed = 0\n    hits = 0\n    \n    if not gold_standard_data:\n\
        \        logger.warning(\"No gold standard data provided for evaluation.\"\
        )\n        return 0.0, 0, 0\n\n    for i, item in enumerate(gold_standard_data):\n\
        \        query_text = item['text']\n        true_curies = item['true_curies']\n\
        \        \n        if not query_text or not true_curies:\n            # This\
        \ should ideally be filtered by parse_evaluation_xml already\n           \
        \ logger.warning(f\"Skipping item with empty query text or true_curies: {item}\"\
        )\n            continue\n            \n        total_entities_processed +=\
        \ 1\n        \n        logger.debug(f\"({i+1}/{len(gold_standard_data)}) Querying\
        \ for: '{query_text}', True CURIEs: {true_curies}\")\n\n        try:\n   \
        \         # Adjusted: HybridRetriever.search returns a dict: {\"lexical_results\"\
        : [], \"vector_results\": []}\n            retriever_output_dict = retriever.search(\n\
        \                query_string=query_text,\n                lexical_limit=lexical_k,\n\
        \                vector_k=vector_k,\n                target_ontologies=[\"\
        foodon\", \"chebi\"], # Adjusted: Use target_ontologies to limit search\n\
        \            )\n            \n            lexical_results = retriever_output_dict.get(\"\
        lexical_results\", [])\n            vector_results = retriever_output_dict.get(\"\
        vector_results\", [])\n\n            # Combine and deduplicate results, lexical\
        \ first then vector.\n            # Scores are not comparable, so this is\
        \ a simple merge strategy.\n            # The 'id' field from result dicts\
        \ contains the CURIE.\n            combined_ordered_results = []\n       \
        \     seen_ids = set()\n\n            for doc in lexical_results:\n      \
        \          doc_id = doc.get('id') # 'id' is the CURIE\n                if\
        \ doc_id and doc_id not in seen_ids:\n                    combined_ordered_results.append(doc)\
        \ # doc contains 'id', 'label', 'score', etc.\n                    seen_ids.add(doc_id)\n\
        \            \n            for doc in vector_results:\n                doc_id\
        \ = doc.get('id') # 'id' is the CURIE\n                if doc_id and doc_id\
        \ not in seen_ids:\n                    combined_ordered_results.append(doc)\n\
        \                    seen_ids.add(doc_id)\n            \n            # Extract\
        \ the CURIEs from the top `recall_at_k` combined documents\n            #\
        \ Adjusted: use doc['id'] as it stores the CURIE\n            retrieved_curies_set\
        \ = {doc['id'] for doc in combined_ordered_results[:recall_at_k]}\n      \
        \      logger.debug(f\"Retrieved CURIEs (top {recall_at_k} from combined):\
        \ {retrieved_curies_set}\")\n\n            if not true_curies.isdisjoint(retrieved_curies_set):\n\
        \                hits += 1\n                logger.info(f\"HIT! Query: '{query_text}'.\
        \ True: {true_curies}. Found in top {recall_at_k}: {true_curies.intersection(retrieved_curies_set)}\"\
        )\n            else:\n                logger.info(f\"MISS. Query: '{query_text}'.\
        \ True: {true_curies}. Top {recall_at_k} (combined) CURIEs: {retrieved_curies_set}\"\
        )\n                # For misses, log more details if needed, e.g. full combined_ordered_results\n\
        \                # logger.debug(f\"Full combined/ordered results for miss:\
        \ {combined_ordered_results}\")\n\n\n        except Exception as e:\n    \
        \        logger.error(f\"Error during retrieval or processing for query '{query_text}':\
        \ {e}\", exc_info=True)\n            \n    if total_entities_processed ==\
        \ 0:\n        logger.warning(\"No valid entities were processed for evaluation.\"\
        )\n        return 0.0, 0, 0\n        \n    recall_score = hits / total_entities_processed\n\
        \    return recall_score, total_entities_processed, hits\n\ndef main():\n\
        \    logger.info(\"Starting Retriever Evaluation Script...\")\n\n    # 1.\
        \ Parse the Gold Standard XML\n    logger.info(f\"Loading gold standard data\
        \ from: {EVALUATION_XML_FILE}\")\n    gold_standard_data = parse_evaluation_xml(EVALUATION_XML_FILE)\n\
        \    if not gold_standard_data:\n        logger.error(\"Failed to load or\
        \ parse gold standard data. Exiting.\")\n        return\n\n    # 2. Initialize\
        \ the HybridRetriever\n    logger.info(\"Initializing HybridRetriever...\"\
        )\n    try:\n        # Ensure all paths are valid and files exist where expected\
        \ by HybridRetriever\n        # Adjusted: Use ONTOLOGY_DUMP_JSON for ontology_data_path\n\
        \        if not os.path.exists(ONTOLOGY_DUMP_JSON):\n            logger.error(f\"\
        Ontology dump not found: {ONTOLOGY_DUMP_JSON}. Run ingestion pipeline.\")\n\
        \            return\n        if not os.path.exists(WHOOSH_INDEX_DIR) or not\
        \ os.listdir(WHOOSH_INDEX_DIR): # Check if dir exists and is not empty\n \
        \            logger.error(f\"Whoosh index directory is empty or not found:\
        \ {WHOOSH_INDEX_DIR}. Run ingestion pipeline.\")\n             return\n  \
        \      if not os.path.exists(FAISS_INDEX_PATH):\n            logger.error(f\"\
        FAISS index not found: {FAISS_INDEX_PATH}. Run ingestion pipeline.\")\n  \
        \          return\n        if not os.path.exists(FAISS_METADATA_PATH):\n \
        \           logger.error(f\"FAISS metadata not found: {FAISS_METADATA_PATH}.\
        \ Run ingestion pipeline.\")\n            return\n\n        retriever = HybridRetriever()\n\
        \        logger.info(\"HybridRetriever initialized successfully.\")\n    except\
        \ Exception as e:\n        logger.error(f\"Failed to initialize HybridRetriever:\
        \ {e}\", exc_info=True)\n        return\n\n    # 3. Perform Evaluation\n \
        \   logger.info(f\"Starting evaluation with Recall@{RECALL_AT_K}...\")\n \
        \   # Adjusted: Use DEFAULT_K_LEXICAL and DEFAULT_K_VECTOR\n    logger.info(f\"\
        HybridRetriever search params: Lexical K={DEFAULT_K_LEXICAL}, Vector K={DEFAULT_K_VECTOR}\"\
        )\n    \n    recall_score, total_entities, hits = evaluate_retriever(\n  \
        \      retriever, \n        gold_standard_data, \n        recall_at_k=RECALL_AT_K,\n\
        \        lexical_k=DEFAULT_K_LEXICAL, # Adjusted\n        vector_k=DEFAULT_K_VECTOR\
        \    # Adjusted\n    )\n\n    # 4. Print Results\n    logger.info(\"--- Evaluation\
        \ Results ---\")\n    logger.info(f\"Total entities processed: {total_entities}\"\
        )\n    logger.info(f\"Number of hits (at least one true CURIE found in top\
        \ {RECALL_AT_K} combined results): {hits}\")\n    if total_entities > 0:\n\
        \        logger.info(f\"Recall@{RECALL_AT_K}: {recall_score:.4f}\")\n    else:\n\
        \        logger.info(\"Recall not calculated as no entities were processed.\"\
        )\n\n    # 5. Clean up\n    try:\n        if hasattr(retriever, 'close') and\
        \ callable(retriever.close):\n            retriever.close()\n            logger.info(\"\
        Retriever resources closed.\")\n    except Exception as e:\n        logger.error(f\"\
        Error closing retriever resources: {e}\")\n\nif __name__ == \"__main__\":\n\
        \    eval_dir = os.path.dirname(EVALUATION_XML_FILE)\n    if not os.path.exists(eval_dir):\n\
        \        try:\n            os.makedirs(eval_dir)\n            logger.info(f\"\
        Created directory: {eval_dir}\")\n            logger.info(f\"Please place\
        \ '{os.path.basename(EVALUATION_XML_FILE)}' in {eval_dir} to run the evaluation.\"\
        )\n        except OSError as e:\n            logger.error(f\"Failed to create\
        \ directory {eval_dir}: {e}\")\n            sys.exit(1) # Exit if cannot create\
        \ data directory for eval file\n    \n    if not os.path.exists(EVALUATION_XML_FILE):\n\
        \        logger.error(f\"Evaluation XML file '{EVALUATION_XML_FILE}' not found.\
        \ Please place it in the correct directory.\")\n    else:\n        main()"
    "ingestion":
      "__init__.py": ""
      "build_lexical_index.py": "# src/ingestion/build_lexical_index.py\nimport sys\n\
        import os\nimport json\nfrom whoosh.index import create_in\nfrom whoosh.fields\
        \ import Schema, ID, TEXT\nimport traceback\n\n# --- Add project root to sys.path\
        \ ---\nproject_root = os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..'))\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n\
        # --- End sys.path modification ---\n\n# Now import using the 'src' package\
        \ prefix\nfrom src.config import ONTOLOGIES_CONFIG # Changed: Use the main\
        \ config dict\n\ndef build_single_index(json_path: str, index_dir: str):\n\
        \    \"\"\"Builds a single Whoosh index from a given JSON dump file.\"\"\"\
        \n    print(f\"Building lexical index from {json_path} into {index_dir}\"\
        )\n\n    schema = Schema(\n        curie=ID(stored=True, unique=True),\n \
        \       label=TEXT(stored=True),\n        synonyms=TEXT(stored=True),\n  \
        \      definition=TEXT(stored=True),\n        relations_text=TEXT(stored=False)\
        \ # Indexed but not stored to save space\n    )\n\n    # Ensure the target\
        \ directory exists\n    os.makedirs(index_dir, exist_ok=True)\n    \n    try:\n\
        \        # create_in will overwrite if the index already exists.\n       \
        \ ix = create_in(index_dir, schema)\n        print(f\"Whoosh index schema\
        \ created in {index_dir}\")\n    except Exception as e:\n         print(f\"\
        Error creating Whoosh index directory or schema: {e}\")\n         traceback.print_exc()\n\
        \         return\n\n    writer = ix.writer()\n    try:\n        with open(json_path,\
        \ 'r', encoding='utf-8') as f:\n            ontology_data = json.load(f)\n\
        \n        print(f\"Indexing {len(ontology_data)} entities...\")\n        indexed_count\
        \ = 0\n        for curie, data in ontology_data.items():\n            label\
        \ = data.get('label', '') or ''\n            synonyms_list = data.get('synonyms',\
        \ [])\n            synonyms = \" \".join(s for s in synonyms_list if s)\n\
        \            definition = data.get('definition', '') or ''\n            \n\
        \            # Create a searchable string from relation data\n           \
        \ relations_text = \"\"\n            relations_dict = data.get('relations',\
        \ {})\n            if relations_dict:\n                 # The 'key' is the\
        \ human-readable relation name (e.g., \"has ingredient\")\n              \
        \   # The 'value' is a list of target CURIEs (which we don't need for this\
        \ text field)\n                 relations_text = \" \".join(relations_dict.keys())\n\
        \            \n            writer.add_document(\n                curie=curie,\n\
        \                label=label,\n                synonyms=synonyms,\n      \
        \          definition=definition,\n                relations_text=relations_text\n\
        \            )\n            indexed_count += 1\n            if indexed_count\
        \ % 1000 == 0:\n                 print(f\"  ...indexed {indexed_count} entities...\"\
        )\n        \n        print(f\"Committing index with {indexed_count} documents.\"\
        )\n        writer.commit()\n        print(\"Lexical index built successfully.\"\
        )\n\n    except FileNotFoundError:\n        print(f\"Error: Ontology dump\
        \ file not found at {json_path}\")\n        traceback.print_exc()\n      \
        \  writer.cancel() \n    except Exception as e:\n        print(f\"An error\
        \ occurred during index building: {e}\")\n        traceback.print_exc()\n\
        \        writer.cancel()\n\ndef main():\n    \"\"\"Loops over all configured\
        \ ontologies and builds a lexical index for each.\"\"\"\n    for name, config_data\
        \ in ONTOLOGIES_CONFIG.items():\n        dump_path = config_data.get('dump_json_path')\n\
        \        index_dir = config_data.get('whoosh_index_dir')\n        \n     \
        \   print(f\"\\n--- Building Lexical Index for: '{name}' ---\")\n\n      \
        \  if not dump_path or not index_dir:\n            print(f\"Warning: Configuration\
        \ for '{name}' is missing 'dump_json_path' or 'whoosh_index_dir'. Skipping.\"\
        )\n            continue\n            \n        if not os.path.exists(dump_path):\n\
        \            print(f\"Error: Ontology dump file not found at {dump_path}.\
        \ Skipping '{name}'.\")\n            print(\"Please run 'src/ingestion/parse_ontology.py'\
        \ and 'src/ingestion/enrich_documents.py' first.\")\n            continue\n\
        \            \n        build_single_index(dump_path, index_dir)\n        \n\
        \    print(\"\\n--- All lexical indexes have been built. ---\")\n\nif __name__\
        \ == \"__main__\":\n    main()"
      "enrich_documents.py": "# src/ingestion/enrich_documents.py\nimport json\nimport\
        \ os\nimport logging\nfrom typing import Dict, Any, List, Optional\n\n# ---\
        \ Start of corrected import block ---\nimport sys\n# Calculate the project\
        \ root directory based on the script's location\n# For .../onto_rag/src/ingestion/enrich_documents.py,\
        \ _PROJECT_ROOT becomes .../onto_rag\n_PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..'))\n\n# Add the project root to sys.path if it's not already there\n\
        if _PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, _PROJECT_ROOT) #\
        \ Insert at the beginning\n\ntry:\n    # Changed: Import specific configs\
        \ needed\n    from src.config import ONTOLOGIES_CONFIG, RELATION_CONFIG\n\
        except ModuleNotFoundError as e:\n    print(f\"CRITICAL ERROR: Could not import\
        \ project modules. Exception: {e}\")\n    print(f\"This script expects to\
        \ be run in a way that the 'src' package is discoverable.\")\n    print(f\"\
        Attempted to add project root '{_PROJECT_ROOT}' to sys.path.\")\n    print(f\"\
        Current sys.path: {sys.path}\")\n    print(\"Please ensure you are running\
        \ this script from the project's root directory ('onto_rag/'), for example:\"\
        )\n    print(\"  python src/ingestion/enrich_documents.py\")\n    sys.exit(1)\n\
        \nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s\
        \ - %(message)s')\n\ndef get_label_for_curie(curie: str, ontology_data: Dict[str,\
        \ Dict[str, Any]], default_value: Optional[str] = None) -> Optional[str]:\n\
        \    \"\"\"\n    Retrieves the label for a given CURIE from the ontology data.\n\
        \    Args:\n        curie: The CURIE string (e.g., \"FOODON:00001234\").\n\
        \        ontology_data: The loaded ontology data dictionary.\n        default_value:\
        \ Value to return if CURIE not found or has no label. Defaults to the CURIE\
        \ itself.\n    Returns:\n        The label string, or the default_value.\n\
        \    \"\"\"\n    if default_value is None:\n        default_value = curie\
        \ # Fallback to CURIE if no specific default\n\n    term_info = ontology_data.get(curie)\n\
        \    if term_info and term_info.get(\"label\"):\n        return term_info[\"\
        label\"]\n    return default_value\n\ndef get_relation_name(relation_curie:\
        \ str) -> str:\n    \"\"\"\n    Gets a human-readable name for a relation\
        \ CURIE using RELATION_CONFIG.\n    \"\"\"\n    # Exact match\n    if relation_curie\
        \ in RELATION_CONFIG:\n        return RELATION_CONFIG[relation_curie][\"label\"\
        ]\n    \n    # Check for generic FoodON prefix if specific one not found\n\
        \    generic_foodon_prefix = \"obo:FOODON_\"\n    if relation_curie.startswith(generic_foodon_prefix)\
        \ and generic_foodon_prefix in RELATION_CONFIG:\n         # Attempt to make\
        \ it slightly more readable if it's like \"obo:FOODON_0000XXXX\"\n       \
        \ relation_suffix = relation_curie.split('_')[-1]\n        return f\"FoodON\
        \ relation {relation_suffix}\"\n\n    # Fallback for other OBO relations\n\
        \    if relation_curie.startswith(\"obo:\"):\n        name_part = relation_curie.split(':')[-1].replace(\"\
        _\", \" \")\n        return name_part\n\n    return relation_curie # Fallback\
        \ to the CURIE itself\n\ndef create_enriched_documents(ontology_data_path:\
        \ str, output_path: str) -> List[Dict[str, Any]]:\n    \"\"\"\n    Creates\
        \ enriched text documents for each ontology entry.\n    Args:\n        ontology_data_path:\
        \ Path to the ontology_dump.json file.\n        output_path: Path to save\
        \ the enriched_documents.json file.\n    Returns:\n        A list of enriched\
        \ document dictionaries.\n    \"\"\"\n    logging.info(f\"Loading ontology\
        \ data from {ontology_data_path}...\")\n    try:\n        with open(ontology_data_path,\
        \ 'r', encoding='utf-8') as f:\n            ontology_data = json.load(f)\n\
        \    except FileNotFoundError:\n        logging.error(f\"Error: Ontology data\
        \ file not found at {ontology_data_path}\")\n        return []\n    except\
        \ json.JSONDecodeError:\n        logging.error(f\"Error: Could not decode\
        \ JSON from {ontology_data_path}\")\n        return []\n\n    logging.info(f\"\
        Successfully loaded {len(ontology_data)} terms.\")\n\n    enriched_docs =\
        \ []\n\n    for term_curie, term_data in ontology_data.items():\n        doc_parts\
        \ = []\n\n        # 1. Label\n        label = term_data.get(\"label\")\n \
        \       if not label:\n            logging.warning(f\"Term {term_curie} has\
        \ no label. Using CURIE as label for document generation.\")\n           \
        \ label = term_curie \n        \n        doc_parts.append(f\"{label}.\")\n\
        \n        # 2. Definition\n        definition = term_data.get(\"definition\"\
        )\n        if definition:\n            doc_parts.append(f\"{definition}.\"\
        )\n\n        # 3. Synonyms\n        synonyms = term_data.get(\"synonyms\"\
        )\n        if synonyms:\n            synonyms_text = \"; \".join(synonyms)\n\
        \            doc_parts.append(f\"Also known as: {synonyms_text}.\")\n\n  \
        \      # 4. Parents (direct subclasses)\n        parent_curies = term_data.get(\"\
        parents\", [])\n        if parent_curies:\n            # Since this function\
        \ processes one self-contained ontology dump,\n            # all parent lookups\
        \ are valid within the same ontology_data.\n            parent_labels = [get_label_for_curie(p_curie,\
        \ ontology_data) for p_curie in parent_curies]\n            parent_labels_filtered\
        \ = [l for l in parent_labels if l]\n            if parent_labels_filtered:\n\
        \                if len(parent_labels_filtered) == 1:\n                  \
        \  doc_parts.append(f\"Is a type of: {parent_labels_filtered[0]}.\")\n   \
        \             else:\n                    doc_parts.append(f\"Is a type of:\
        \ {'; '.join(parent_labels_filtered)}.\")\n        \n        # 5. Relations\n\
        \        relations = term_data.get(\"relations\", {})\n        relation_texts\
        \ = []\n        for rel_name, target_curies_list in relations.items():\n \
        \           # In the new structure, rel_name is the human-readable name from\
        \ parse_ontology\n            target_labels = [get_label_for_curie(t_curie,\
        \ ontology_data) for t_curie in target_curies_list]\n            target_labels_filtered\
        \ = [l for l in target_labels if l]\n            if target_labels_filtered:\n\
        \                relation_texts.append(f\"{rel_name}: {', '.join(target_labels_filtered)}\"\
        )\n        \n        if relation_texts:\n            doc_parts.append(\"Key\
        \ characteristics include: \" + \"; \".join(relation_texts) + \".\")\n\n \
        \       # Combine all parts into a single text\n        enriched_text = \"\
        \ \".join(doc_parts).replace(\"..\", \".\").strip()\n\n        enriched_docs.append({\n\
        \            \"id\": term_curie,\n            \"label\": term_data.get(\"\
        label\", term_curie), # Store original label if available, else CURIE\n  \
        \          \"text\": enriched_text\n        })\n\n    logging.info(f\"Created\
        \ {len(enriched_docs)} enriched documents.\")\n\n    logging.info(f\"Saving\
        \ enriched documents to {output_path}...\")\n    try:\n        with open(output_path,\
        \ 'w', encoding='utf-8') as f:\n            json.dump(enriched_docs, f, indent=2)\n\
        \        logging.info(\"Successfully saved enriched documents.\")\n    except\
        \ IOError:\n        logging.error(f\"Error: Could not write enriched documents\
        \ to {output_path}\")\n\n    return enriched_docs\n\ndef main():\n    \"\"\
        \"Main function to create enriched documents for each configured ontology.\"\
        \"\"\n    for name, config_data in ONTOLOGIES_CONFIG.items():\n        logging.info(f\"\
        \\n--- Enriching documents for '{name}' ---\")\n        \n        ontology_dump_path\
        \ = config_data.get('dump_json_path')\n        enriched_docs_output_path =\
        \ config_data.get('enriched_docs_path')\n\n        if not ontology_dump_path\
        \ or not enriched_docs_output_path:\n            logging.warning(f\"Configuration\
        \ for '{name}' is missing 'dump_json_path' or 'enriched_docs_path'. Skipping.\"\
        )\n            continue\n            \n        if not os.path.exists(ontology_dump_path):\n\
        \            logging.error(f\"Dump file not found: {ontology_dump_path}. Skipping\
        \ '{name}'.\")\n            continue\n\n        # Ensure output directory\
        \ exists\n        os.makedirs(os.path.dirname(enriched_docs_output_path),\
        \ exist_ok=True)\n            \n        create_enriched_documents(\n     \
        \       ontology_data_path=ontology_dump_path,\n            output_path=enriched_docs_output_path\n\
        \        )\n        \n    logging.info(\"\\n--- All enriched document file\
        \ creation complete. ---\")\n\n\nif __name__ == \"__main__\":\n    main()"
      "parse_ontology.py": "# src/ingestion/parse_ontology.py\nimport sys\nimport\
        \ os\nimport logging # Import logging\nimport rdflib\nfrom rdflib import Graph,\
        \ Namespace, URIRef, RDFS, OWL, RDF\nfrom typing import Dict, List, Any\n\
        import json\nimport traceback\n\n# --- Add project root to sys.path ---\n\
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..',\
        \ '..'))\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n\
        # --- End sys.path modification ---\n\n# Now import using the 'src' package\
        \ prefix\nfrom src.config import (\n    ONTOLOGIES_CONFIG,          # Changed:\
        \ Using a central config dict\n    CURIE_PREFIX_MAP,\n    RELATION_CONFIG,\n\
        \    TARGET_RELATIONS_CURIES,\n    IAO_NS_STR,\n    OBOINOWL_NS_STR,\n)\n\
        from src.utils.ontology_utils import uri_to_curie, curie_to_uri\n\n# --- Logging\
        \ Setup ---\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s -\
        \ %(levelname)s - %(module)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\
        \n\n# Define commonly used namespaces (can still use these locally for convenience)\n\
        IAO = Namespace(IAO_NS_STR)\nOBOINOWL = Namespace(OBOINOWL_NS_STR)\n\n\ndef\
        \ load_ontology(path: str) -> rdflib.Graph:\n    g = Graph()\n    try:\n \
        \       logger.info(f\"Loading ontology from: {path}\")\n        try:\n  \
        \          g.parse(path, format=\"application/rdf+xml\")\n        except Exception\
        \ as e_xml:\n            logger.warning(f\"Failed to parse as RDF/XML: {e_xml}.\
        \ Trying Turtle...\")\n            try:\n                g.parse(path, format=\"\
        turtle\")\n            except Exception as e_ttl:\n                logger.warning(f\"\
        Failed to parse as Turtle: {e_ttl}. Trying auto-detection...\")\n        \
        \        g.parse(path)\n\n        logger.info(f\"Ontology loaded successfully.\
        \ Contains {len(g)} triples.\")\n        return g\n    except FileNotFoundError:\n\
        \        logger.error(f\"Error: Ontology file not found at {path}\")\n   \
        \     raise\n    except Exception as e:\n        logger.error(f\"Error parsing\
        \ ontology file {path}: {e}\")\n        traceback.print_exc()\n        raise\n\
        \ndef get_ancestors(g: Graph, term_uri: URIRef, prefix_map: Dict[str, str],\
        \ visited_uris: set = None) -> List[str]:\n    if visited_uris is None:\n\
        \        visited_uris = set()\n\n    ancestor_curies = set()\n    for parent_uri\
        \ in g.objects(subject=term_uri, predicate=RDFS.subClassOf):\n        if isinstance(parent_uri,\
        \ URIRef) and parent_uri != OWL.Thing:\n            if parent_uri not in visited_uris:\n\
        \                visited_uris.add(parent_uri)\n                # Pass the\
        \ prefix_map explicitly\n                parent_curie = uri_to_curie(parent_uri,\
        \ prefix_map)\n                if parent_curie and parent_curie != str(parent_uri):\n\
        \                    ancestor_curies.add(parent_curie)\n                 \
        \   ancestor_curies.update(get_ancestors(g, parent_uri, prefix_map, visited_uris))\n\
        \    return list(ancestor_curies)\n\n\ndef extract_labels_and_synonyms(g:\
        \ Graph, prefix_map: Dict[str, str]) -> Dict[str, Dict[str, Any]]:\n    data\
        \ = {}\n    relevant_predicates = [\n        RDFS.label,\n        OBOINOWL.hasExactSynonym,\
        \ OBOINOWL.hasRelatedSynonym,\n        OBOINOWL.hasNarrowSynonym, OBOINOWL.hasBroadSynonym\n\
        \    ]\n    \n    processed_subjects = set()\n\n    for pred in relevant_predicates:\n\
        \        for s_uri in g.subjects(predicate=pred):\n            if not isinstance(s_uri,\
        \ URIRef) or s_uri in processed_subjects:\n                continue\n    \
        \        \n            # Pass the prefix_map explicitly\n            curie\
        \ = uri_to_curie(s_uri, prefix_map)\n            if not curie or curie ==\
        \ str(s_uri): # Skip if not converted to a CURIE effectively\n           \
        \     continue\n\n            if curie not in data:\n                data[curie]\
        \ = {\"label\": None, \"synonyms\": []}\n\n            # Label\n         \
        \   label_val = g.value(subject=s_uri, predicate=RDFS.label)\n           \
        \ if label_val and isinstance(label_val, rdflib.Literal):\n              \
        \  data[curie][\"label\"] = str(label_val)\n\n            # Synonyms\n   \
        \         current_synonyms = []\n            for syn_prop in [OBOINOWL.hasExactSynonym,\
        \ OBOINOWL.hasRelatedSynonym,\n                             OBOINOWL.hasNarrowSynonym,\
        \ OBOINOWL.hasBroadSynonym]:\n                for syn_obj in g.objects(subject=s_uri,\
        \ predicate=syn_prop):\n                    if isinstance(syn_obj, rdflib.Literal):\n\
        \                        current_synonyms.append(str(syn_obj))\n         \
        \   \n            if \"synonyms\" not in data[curie] or data[curie][\"synonyms\"\
        ] is None:\n                data[curie][\"synonyms\"] = []\n            for\
        \ s in current_synonyms:\n                if s not in data[curie][\"synonyms\"\
        ]:\n                    data[curie][\"synonyms\"].append(s)\n\n          \
        \  processed_subjects.add(s_uri)\n            \n    final_data = {k: v for\
        \ k, v in data.items() if v.get(\"label\") or v.get(\"synonyms\")}\n    logger.info(f\"\
        Extracted labels and synonyms for {len(final_data)} terms.\")\n    return\
        \ final_data\n\n\ndef extract_definitions(g: Graph, prefix_map: Dict[str,\
        \ str]) -> Dict[str, str]:\n    definitions = {}\n    definition_prop_uri\
        \ = IAO['0000115'] # IAO:0000115 is 'definition'\n    for s_uri in g.subjects(predicate=definition_prop_uri):\n\
        \        if not isinstance(s_uri, URIRef):\n            continue\n       \
        \ \n        curie = uri_to_curie(s_uri, prefix_map)\n        if not curie\
        \ or curie == str(s_uri):\n            continue\n        \n        def_obj\
        \ = g.value(subject=s_uri, predicate=definition_prop_uri)\n        if def_obj\
        \ and isinstance(def_obj, rdflib.Literal):\n            definitions[curie]\
        \ = str(def_obj)\n            \n    logger.info(f\"Extracted definitions for\
        \ {len(definitions)} terms.\")\n    return definitions\n\ndef extract_hierarchy(g:\
        \ Graph, prefix_map: Dict[str, str]) -> Dict[str, Dict[str, List[str]]]:\n\
        \    hierarchy_data = {}\n    all_terms_in_hierarchy = set()\n    for s, p,\
        \ o in g.triples((None, RDFS.subClassOf, None)):\n        if isinstance(s,\
        \ URIRef): all_terms_in_hierarchy.add(s)\n        if isinstance(o, URIRef):\
        \ all_terms_in_hierarchy.add(o)\n    \n    for term_uri in all_terms_in_hierarchy:\n\
        \        if term_uri == OWL.Thing:\n            continue\n\n        curie\
        \ = uri_to_curie(term_uri, prefix_map)\n        if not curie or curie == str(term_uri):\n\
        \            continue\n\n        direct_parent_curies = []\n        for parent_uri\
        \ in g.objects(subject=term_uri, predicate=RDFS.subClassOf):\n           \
        \ if isinstance(parent_uri, URIRef) and parent_uri != OWL.Thing:\n       \
        \         parent_curie = uri_to_curie(parent_uri, prefix_map)\n          \
        \      if parent_curie and parent_curie != str(parent_uri):\n            \
        \        direct_parent_curies.append(parent_curie)\n        \n        ancestor_curies\
        \ = get_ancestors(g, term_uri, prefix_map, visited_uris=set())\n        \n\
        \        if direct_parent_curies or ancestor_curies:\n            hierarchy_data[curie]\
        \ = {\n                \"parents\": list(set(direct_parent_curies)),\n   \
        \             \"ancestors\": list(set(ancestor_curies))\n            }\n \
        \           \n    logger.info(f\"Extracted hierarchy data for {len(hierarchy_data)}\
        \ terms.\")\n    return hierarchy_data\n\n\ndef extract_relations(g: Graph,\
        \ props_to_extract: Dict[str, str], prefix_map: Dict[str, str]) -> Dict[str,\
        \ Dict[str, List[str]]]:\n    relations_data = {}\n    \n    for term_uri\
        \ in g.subjects(unique=True):\n        if not isinstance(term_uri, URIRef):\n\
        \            continue\n\n        curie = uri_to_curie(term_uri, prefix_map)\n\
        \        if not curie or curie == str(term_uri):\n            continue\n\n\
        \        term_specific_relations = {}\n        for rel_readable_name, rel_uri_str\
        \ in props_to_extract.items():\n            rel_uri = URIRef(rel_uri_str)\n\
        \            target_curies = []\n            for target_obj in g.objects(subject=term_uri,\
        \ predicate=rel_uri):\n                if isinstance(target_obj, URIRef):\n\
        \                    target_curie = uri_to_curie(target_obj, prefix_map)\n\
        \                    if target_curie and target_curie != str(target_obj):\n\
        \                        target_curies.append(target_curie)\n            \n\
        \            if target_curies:\n                term_specific_relations[rel_readable_name]\
        \ = list(set(target_curies))\n\n        if term_specific_relations:\n    \
        \        relations_data[curie] = term_specific_relations\n            \n \
        \   logger.info(f\"Extracted relations for {len(relations_data)} terms based\
        \ on {len(props_to_extract)} specified properties.\")\n    return relations_data\n\
        \n\ndef main():\n    logger.info(\"--- Starting Ontology Parsing for Each\
        \ Configured Ontology ---\")\n\n    # Prepare relation properties once\n \
        \   relation_properties_for_extraction = {}\n    for rel_curie_str in TARGET_RELATIONS_CURIES:\n\
        \        if rel_curie_str in RELATION_CONFIG:\n            readable_name =\
        \ RELATION_CONFIG[rel_curie_str].get(\"label\", rel_curie_str)\n         \
        \   full_rel_uri = curie_to_uri(rel_curie_str, CURIE_PREFIX_MAP)\n       \
        \     if full_rel_uri:\n                relation_properties_for_extraction[readable_name]\
        \ = str(full_rel_uri)\n            else:\n                logger.warning(f\"\
        Could not convert relation CURIE {rel_curie_str} to URI. Skipping this relation.\"\
        )\n        else:\n            logger.warning(f\"Relation CURIE {rel_curie_str}\
        \ from TARGET_RELATIONS_CURIES not found in RELATION_CONFIG.\")\n\n    # Loop\
        \ through each ontology configured in config.py\n    for name, config_data\
        \ in ONTOLOGIES_CONFIG.items():\n        ontology_path = config_data['path']\n\
        \        dump_path = config_data['dump_json_path']\n        \n        # Ensure\
        \ the output directory exists\n        os.makedirs(os.path.dirname(dump_path),\
        \ exist_ok=True)\n        \n        logger.info(f\"\\n--- Processing Ontology:\
        \ '{name}' ---\")\n        logger.info(f\"Source: {ontology_path}\")\n   \
        \     logger.info(f\"Destination: {dump_path}\")\n\n        if not os.path.exists(ontology_path):\n\
        \            logger.error(f\"Ontology file not found. Skipping '{name}'.\"\
        )\n            continue\n\n        try:\n            # 1. Load the single\
        \ ontology graph\n            g = load_ontology(ontology_path)\n\n       \
        \     # 2. Extract data FROM THIS GRAPH ONLY\n            logger.info(f\"\
        Extracting data for '{name}'...\")\n            labels_synonyms = extract_labels_and_synonyms(g,\
        \ CURIE_PREFIX_MAP)\n            definitions = extract_definitions(g, CURIE_PREFIX_MAP)\n\
        \            hierarchy = extract_hierarchy(g, CURIE_PREFIX_MAP)\n        \
        \    relations = extract_relations(g, relation_properties_for_extraction,\
        \ CURIE_PREFIX_MAP)\n\n            # 3. Merge extracted data for this ontology\n\
        \            logger.info(\"Merging extracted data...\")\n            ontology_specific_data\
        \ = {}\n            all_curies = set(labels_synonyms.keys()) | set(definitions.keys())\
        \ | set(hierarchy.keys()) | set(relations.keys())\n\n            for curie_key\
        \ in all_curies:\n                ontology_specific_data[curie_key] = {\n\
        \                    \"label\": labels_synonyms.get(curie_key, {}).get(\"\
        label\"),\n                    \"synonyms\": labels_synonyms.get(curie_key,\
        \ {}).get(\"synonyms\", []),\n                    \"definition\": definitions.get(curie_key),\n\
        \                    \"parents\": hierarchy.get(curie_key, {}).get(\"parents\"\
        , []),\n                    \"ancestors\": hierarchy.get(curie_key, {}).get(\"\
        ancestors\", []),\n                    \"relations\": relations.get(curie_key,\
        \ {})\n                }\n            \n            final_data = {k: v for\
        \ k, v in ontology_specific_data.items() if any(v.values())}\n           \
        \ \n            # 4. Save the dedicated dump file\n            logger.info(f\"\
        Found {len(final_data)} entities with data in '{name}'.\")\n            logger.info(f\"\
        Writing data to {dump_path}\")\n            with open(dump_path, 'w', encoding='utf-8')\
        \ as f:\n                json.dump(final_data, f, indent=4, ensure_ascii=False)\n\
        \n            logger.info(f\"Successfully processed '{name}'.\")\n\n     \
        \   except Exception as e:\n            logger.error(f\"An error occurred\
        \ while processing '{name}': {e}\")\n            traceback.print_exc()\n\n\
        \    logger.info(\"\\n--- All Ontology Parsing Complete ---\")\n\n\nif __name__\
        \ == \"__main__\":\n    main()"
    "pipeline":
      "gemini_pipeline.py": "# src/pipeline/gemini_pipeline.py\nimport os\nimport\
        \ sys\nimport logging\nfrom typing import List, Dict, Any, Optional\n\n# ---\
        \ Add project root to sys.path ---\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..'))\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n\
        \nfrom src.retriever.hybrid_retriever import HybridRetriever\nfrom src.reranker.llm_reranker\
        \ import LLMReranker\nfrom src.rag_selectors.gemini_selector import GeminiSelector\n\
        from src import config\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s\
        \ - %(levelname)s - %(name)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\
        \nclass RAGPipeline:\n    def __init__(self):\n        logger.info(\"Initializing\
        \ RAG Pipeline...\")\n        try:\n            self.retriever = HybridRetriever()\n\
        \n            # --- CORRECTED PART ---\n            # Gather all enriched\
        \ document paths from the config\n            all_enriched_docs_paths = [\n\
        \                data['enriched_docs_path'] \n                for data in\
        \ config.ONTOLOGIES_CONFIG.values()\n                if os.path.exists(data['enriched_docs_path'])\
        \ # Safety check\n            ]\n\n            if not all_enriched_docs_paths:\n\
        \                raise FileNotFoundError(\"No enriched document files found\
        \ for any configured ontology. Please run the ingestion pipeline.\")\n\n \
        \           logger.info(f\"Initializing LLMReranker with {len(all_enriched_docs_paths)}\
        \ enriched document file(s).\")\n            \n            self.reranker =\
        \ LLMReranker(\n                model_name=config.RERANKER_MODEL_NAME,\n \
        \               enriched_docs_paths=all_enriched_docs_paths,  # Pass the list\
        \ of paths\n                device=config.EMBEDDING_DEVICE\n            )\n\
        \            # --- END OF CORRECTION ---\n\n            self.selector = GeminiSelector(retriever=self.retriever)\n\
        \            logger.info(\"RAG Pipeline initialized successfully.\")\n   \
        \         \n        except (FileNotFoundError, ValueError) as e:\n       \
        \     logger.error(f\"Failed to initialize pipeline: {e}\")\n            logger.error(\"\
        Please run 'scripts/rebuild_base.bash' and ensure necessary API keys are set\
        \ in your .env file.\")\n            raise\n        except Exception as e:\n\
        \            logger.error(f\"An unexpected error occurred during pipeline\
        \ initialization: {e}\", exc_info=True)\n            raise\n\n    # --- MODIFIED\
        \ 'run' METHOD ---\n    def run(self, \n            query: str, \n       \
        \     lexical_k: int = config.DEFAULT_K_LEXICAL, \n            vector_k: int\
        \ = config.DEFAULT_K_VECTOR, \n            rerank_top_n: int = 10\n      \
        \      ) -> Optional[tuple[Dict[str, Any], List[Dict[str, Any]]]]:\n     \
        \   \"\"\"\n        Executes the full pipeline for a given query.\n      \
        \  Returns:\n            A tuple containing (final_result_dict, candidates_list),\
        \ or None.\n            The final_result_dict includes the confidence score\
        \ and reasoning.\n        \"\"\"\n        logger.info(\"Running pipeline for\
        \ query: '%s'\", query)\n\n        # 1. Retrieve\n        retriever_output\
        \ = self.retriever.search(query, lexical_limit=lexical_k, vector_k=vector_k)\n\
        \        lexical_results = retriever_output.get(\"lexical_results\", [])\n\
        \        vector_results = retriever_output.get(\"vector_results\", [])\n \
        \       \n        # 2. Merge\n        combined_candidates = []\n        seen_ids\
        \ = set()\n        for doc in lexical_results + vector_results:\n        \
        \    doc_id = doc.get('id')\n            if doc_id and doc_id not in seen_ids:\n\
        \                combined_candidates.append(doc)\n                seen_ids.add(doc_id)\n\
        \        \n        if not combined_candidates:\n            logger.warning(\"\
        No candidates found.\")\n            return None\n\n        # 3. Rerank\n\
        \        reranked_candidates = self.reranker.rerank(query, combined_candidates,\
        \ top_n=rerank_top_n)\n\n        if not reranked_candidates:\n           \
        \ logger.warning(\"No candidates left after reranking.\")\n            return\
        \ None\n        \n        logger.info(f\"Top {len(reranked_candidates)} candidates\
        \ after reranking passed to LLM selector.\")\n\n        # 4. Select with LLM\n\
        \        selection = self.selector.select_best_term(query, reranked_candidates)\n\
        \n        if not selection:\n            logger.error(\"LLM selection failed.\
        \ Returning the top reranked result as a fallback.\")\n            top_fallback\
        \ = reranked_candidates[0]\n            chosen_term_details = self.retriever.get_term_details(top_fallback['id'])\n\
        \            # Ensure chosen_term_details is not None before modifying\n \
        \           if chosen_term_details:\n                chosen_term_details['confidence_score']\
        \ = 0.0 # Default confidence for fallback\n                chosen_term_details['explanation']\
        \ = \"FALLBACK: LLM selection failed. This is the top result from the reranker.\"\
        \n            return chosen_term_details, reranked_candidates\n\n        #\
        \ 5. Get final details and return\n        chosen_id = selection['chosen_id']\n\
        \        if chosen_id == '0' or chosen_id == '-1':\n            logger.info(\"\
        LLM selected no suitable match. Returning No match.\")\n            no_match_result\
        \ = {\n                'id': chosen_id,  # Preserve the -1 or 0 ID as the\
        \ signal\n                'label': 'No Match Found',\n                'definition':\
        \ 'The Language Model determined that no candidate was a suitable match for\
        \ the query.',\n                'synonyms': [],\n                'parents':\
        \ [],\n                'ancestors': [],\n                'relations': {},\n\
        \                'confidence_score': selection.get('confidence_score', 0.0),\n\
        \                'explanation': selection.get('explanation', 'No explanation\
        \ provided.')\n            }\n            return no_match_result, reranked_candidates\n\
        \        \n        chosen_term_details = self.retriever.get_term_details(chosen_id)\n\
        \        if not chosen_term_details:\n            logger.error(\"LLM chose\
        \ ID '%s', but its details could not be retrieved.\", chosen_id)\n       \
        \     return None\n\n        # Add the confidence and explanation from the\
        \ selector to the final result\n        chosen_term_details['confidence_score']\
        \ = selection.get('confidence_score', 0.0) # Use 'confidence' as per selector's\
        \ output\n        chosen_term_details['explanation'] = selection.get('explanation',\
        \ 'No explanation provided.')\n        return chosen_term_details, reranked_candidates\n\
        \    # --- END OF MODIFIED 'run' METHOD ---\n\n    def close(self):\n    \
        \    if hasattr(self.retriever, 'close'):\n            self.retriever.close()\n\
        \        logger.info(\"Pipeline resources closed.\")"
      "ollama_pipeline.py": "# src/pipeline/pipeline.py\nimport os\nimport sys\nimport\
        \ logging\nfrom typing import List, Dict, Any, Optional\n\n# --- Add project\
        \ root to sys.path ---\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..'))\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n\
        \nfrom src.retriever.hybrid_retriever import HybridRetriever\nfrom src.reranker.llm_reranker\
        \ import LLMReranker\nfrom src.rag_selectors.ollama_selector import OllamaSelector\n\
        from src import config\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s\
        \ - %(levelname)s - %(name)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\
        \nclass RAGPipeline:\n    def __init__(self):\n        logger.info(\"Initializing\
        \ RAG Pipeline...\")\n        try:\n            self.retriever = HybridRetriever()\n\
        \n            all_enriched_docs_paths = [\n                data['enriched_docs_path']\
        \ \n                for data in config.ONTOLOGIES_CONFIG.values()\n      \
        \          if os.path.exists(data['enriched_docs_path']) \n            ]\n\
        \n            if not all_enriched_docs_paths:\n                raise FileNotFoundError(\"\
        No enriched document files found for any configured ontology. Please run the\
        \ ingestion pipeline.\")\n\n            logger.info(f\"Initializing LLMReranker\
        \ with {len(all_enriched_docs_paths)} enriched document file(s).\")\n    \
        \        \n            self.reranker = LLMReranker(\n                model_name=config.RERANKER_MODEL_NAME,\n\
        \                enriched_docs_paths=all_enriched_docs_paths,  # Pass the\
        \ list of paths\n                device=config.EMBEDDING_DEVICE\n        \
        \    )\n            self.selector = OllamaSelector(retriever=self.retriever)\
        \ # <--- UPDATED INSTANTIATION\n            logger.info(\"RAG Pipeline initialized\
        \ successfully.\")\n        except (FileNotFoundError, ValueError) as e:\n\
        \            logger.error(f\"Failed to initialize pipeline: {e}\")\n     \
        \       logger.error(\"Please run 'scripts/rebuild_base.bash' and ensure GEMINI_API_KEY\
        \ is set in your .env file.\")\n            raise\n        except Exception\
        \ as e:\n            logger.error(f\"An unexpected error occurred during pipeline\
        \ initialization: {e}\", exc_info=True)\n            raise\n\n    def run(self,\
        \ \n            query: str, \n            lexical_k: int = config.DEFAULT_K_LEXICAL,\
        \ \n            vector_k: int = config.DEFAULT_K_VECTOR, \n            rerank_top_n:\
        \ int = 10\n            ) -> Optional[tuple[Dict[str, Any], List[Dict[str,\
        \ Any]]]]:\n        \"\"\"\n        Executes the full pipeline for a given\
        \ query.\n        Returns:\n            A tuple containing (final_result_dict,\
        \ candidates_list), or None.\n            The final_result_dict includes the\
        \ confidence score and reasoning.\n        \"\"\"\n        logger.info(\"\
        Running pipeline for query: '%s'\", query)\n\n        # 1. Retrieve\n    \
        \    retriever_output = self.retriever.search(query, lexical_limit=lexical_k,\
        \ vector_k=vector_k)\n        lexical_results = retriever_output.get(\"lexical_results\"\
        , [])\n        vector_results = retriever_output.get(\"vector_results\", [])\n\
        \        \n        # 2. Merge\n        combined_candidates = []\n        seen_ids\
        \ = set()\n        for doc in lexical_results + vector_results:\n        \
        \    doc_id = doc.get('id')\n            if doc_id and doc_id not in seen_ids:\n\
        \                combined_candidates.append(doc)\n                seen_ids.add(doc_id)\n\
        \        \n        if not combined_candidates:\n            logger.warning(\"\
        No candidates found.\")\n            return None\n\n        # 3. Rerank\n\
        \        reranked_candidates = self.reranker.rerank(query, combined_candidates,\
        \ top_n=rerank_top_n)\n\n        if not reranked_candidates:\n           \
        \ logger.warning(\"No candidates left after reranking.\")\n            return\
        \ None\n        \n        logger.info(f\"Top {len(reranked_candidates)} candidates\
        \ after reranking passed to LLM selector.\")\n\n        # 4. Select with LLM\n\
        \        selection = self.selector.select_best_term(query, reranked_candidates)\n\
        \n        if not selection:\n            logger.error(\"LLM selection failed.\
        \ Returning the top reranked result as a fallback.\")\n            top_fallback\
        \ = reranked_candidates[0]\n            chosen_term_details = self.retriever.get_term_details(top_fallback['id'])\n\
        \            chosen_term_details['explanation'] = \"FALLBACK: LLM selection\
        \ failed. This is the top result from the reranker.\"\n            return\
        \ chosen_term_details, reranked_candidates\n\n        # 5. Get final details\
        \ and return\n        chosen_id = selection['chosen_id']\n        if chosen_id\
        \ == '0' or chosen_id == '-1':\n            logger.info(\"LLM selected no\
        \ suitable match. Returning No match.\")\n            no_match_result = {\n\
        \                'id': chosen_id,  # Preserve the -1 or 0 ID as the signal\n\
        \                'label': 'No Match Found',\n                'definition':\
        \ 'The Language Model determined that no candidate was a suitable match for\
        \ the query.',\n                'synonyms': [],\n                'parents':\
        \ [],\n                'ancestors': [],\n                'relations': {},\n\
        \                'confidence_score': selection.get('confidence_score', 0.0),\n\
        \                'explanation': selection.get('explanation', 'No explanation\
        \ provided.')\n            }\n            return no_match_result, reranked_candidates\n\
        \        chosen_term_details = self.retriever.get_term_details(chosen_id)\n\
        \        \n        if not chosen_term_details:\n            logger.error(\"\
        LLM chose ID '%s', but its details could not be retrieved.\", chosen_id)\n\
        \            return None\n\n        chosen_term_details['confidence_score']\
        \ = selection.get('confidence_score')\n        chosen_term_details['explanation']\
        \ = selection['explanation']\n        return chosen_term_details, reranked_candidates\n\
        \n    def close(self):\n        if hasattr(self.retriever, 'close'):\n   \
        \         self.retriever.close()\n        logger.info(\"Pipeline resources\
        \ closed.\")"
    "rag_selectors":
      "__init__.py": ""
      "gemini_selector.py": "# src/selectors/gemini_selector.py\nimport os\nimport\
        \ logging\nimport json\nfrom typing import List, Dict, Any, Optional\n\nfrom\
        \ google import genai\nfrom google.api_core import exceptions  # <-- Import\
        \ for better error handling\n\nfrom src.retriever.hybrid_retriever import\
        \ HybridRetriever\nfrom src import config\n\nlogger = logging.getLogger(__name__)\n\
        \nclass GeminiSelector:\n    \"\"\"\n    Uses the Google Gemini model to select\
        \ the best ontology term\n    from a list of candidates.\n    \"\"\"\n   \
        \ def __init__(self, retriever: HybridRetriever):\n        \"\"\"\n      \
        \  Initializes the GeminiSelector.\n\n        Args:\n            retriever\
        \ (HybridRetriever): An initialized retriever instance,\n                \
        \                         used to fetch full details of candidate terms.\n\
        \        \"\"\"\n        self.retriever = retriever\n        self.prompt_template\
        \ = self._load_prompt_template()\n\n        if not config.GEMINI_API_KEY:\n\
        \            raise ValueError(\"GEMINI_API_KEY not found in environment variables.\"\
        )\n        \n        # --- CORRECTED PART 1: Client Instantiation ---\n  \
        \      # Instead of genai.configure(), we instantiate a client.\n        self.client\
        \ = genai.Client(api_key=config.GEMINI_API_KEY)\n        self.model_name =\
        \ config.LLM_SELECTOR_MODEL_NAME\n        \n        logger.info(f\"GeminiSelector\
        \ initialized for model: {self.model_name}\")\n\n    def _load_prompt_template(self)\
        \ -> str:\n        \"\"\"Loads the prompt template from the file.\"\"\"\n\
        \        template_path = os.path.join(config.PROJECT_ROOT, \"prompts\", \"\
        strict_final_selection.tpl\")\n        try:\n            with open(template_path,\
        \ 'r', encoding='utf-8') as f:\n                return f.read()\n        except\
        \ FileNotFoundError:\n            logger.error(f\"Prompt template not found\
        \ at {template_path}\")\n            raise\n\n    def _format_candidates_for_prompt(self,\
        \ candidates: List[Dict[str, Any]]) -> str:\n        \"\"\"Formats the list\
        \ of candidate documents into a string for the prompt.\"\"\"\n        formatted_list\
        \ = []\n        for i, candidate in enumerate(candidates):\n            term_id\
        \ = candidate.get(\"id\")\n            if not term_id:\n                continue\n\
        \            \n            # Fetch full details using the retriever\n    \
        \        details = self.retriever.get_term_details(term_id)\n            if\
        \ not details:\n                continue\n\n            # Format the details\
        \ for display\n            label = details.get(\"label\", \"N/A\")\n     \
        \       definition = details.get(\"definition\", \"No definition available.\"\
        )\n            synonyms = \"; \".join(details.get(\"synonyms\", [])) or \"\
        None\"\n\n            formatted_list.append(\n                f\"{i+1}. ID:\
        \ {term_id}\\n\"\n                f\"   Label: {label}\\n\"\n            \
        \    f\"   Definition: {definition}\\n\"\n                f\"   Synonyms:\
        \ {synonyms}\"\n            )\n        return \"\\n\\n\".join(formatted_list)\n\
        \n    # --- Type hint updated to allow for float in confidence_score score\
        \ ---\n    def select_best_term(self, query: str, candidates: List[Dict[str,\
        \ Any]]) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Prompts Gemini\
        \ to select the best term and parses the JSON response.\n\n        Args:\n\
        \            query (str): The original user query.\n            candidates\
        \ (List[Dict[str, Any]]): The list of candidate documents.\n\n        Returns:\n\
        \            A dictionary with {\"chosen_id\": str, \"explanation\": str},\
        \ or None on failure.\n        \"\"\"\n        if not candidates:\n      \
        \      return None\n\n        candidate_str = self._format_candidates_for_prompt(candidates)\n\
        \        prompt = self.prompt_template.replace(\"[USER_ENTITY]\", query).replace(\"\
        [CANDIDATE_LIST]\", candidate_str)\n        \n        logger.info(f\"Sending\
        \ request to Gemini for query: '{query}'\")\n        try:\n            # ---\
        \ CORRECTED PART 2: The API Call ---\n            # Call generate_content\
        \ on the client.models service.\n            # Pass the prompt string to the\
        \ 'contents' parameter.\n            generation_config = {'temperature': 0}\n\
        \            response = self.client.models.generate_content(\n           \
        \     model=self.model_name,\n                contents=prompt,\n         \
        \       config=generation_config\n            )\n\n            # The rest\
        \ of your logic is good.\n            # Safety feedback check from your example\
        \ code is a good practice to add here if needed.\n            feedback = getattr(response,\
        \ 'prompt_feedback', None)\n            if feedback and any(r.blocked for\
        \ r in feedback.safety_ratings or []):\n                logger.warning(f\"\
        Request for query '{query}' was blocked by safety filters.\")\n          \
        \      return None\n\n            # Clean up the response text to extract\
        \ the JSON part\n            cleaned_response = response.text.strip().lstrip(\"\
        ```json\").rstrip(\"```\").strip()\n            \n            # Parse the\
        \ JSON response\n            result = json.loads(cleaned_response)\n\n   \
        \         # --- MODIFIED VALIDATION LOGIC ---\n            \n            #\
        \ 1. The 'chosen_id' key is mandatory. Fail if it's missing or null.\n   \
        \         if \"chosen_id\" not in result or result.get(\"chosen_id\") is None:\n\
        \                logger.error(\n                    \"LLM response is invalid:\
        \ Missing the mandatory 'chosen_id' key. Response: %s\",\n               \
        \     result\n                )\n                return None\n           \
        \ \n            # 2. Start building the result with the mandatory key.\n \
        \           validated_result = {\n                'chosen_id': result['chosen_id']\n\
        \            }\n\n            # 3. Handle optional 'explanation' with a specific\
        \ warning if missing.\n            if 'explanation' in result:\n         \
        \       validated_result['explanation'] = result['explanation']\n        \
        \    else:\n                logger.warning(\"LLM response missing 'explanation'\
        \ key. Using default value.\")\n                validated_result['explanation']\
        \ = 'No explanation provided.'\n\n            # 4. Handle optional 'confidence_score'\
        \ with distinct warnings.\n            if 'confidence_score' in result:\n\
        \                try:\n                    # Key exists, so try to convert\
        \ it\n                    validated_result['confidence_score'] = float(result['confidence_score'])\n\
        \                except (ValueError, TypeError):\n                    # Key\
        \ exists, but the value is not a valid float\n                    logger.warning(\n\
        \                        f\"Invalid confidence_score value in response: '{result.get('confidence_score')}'.\
        \ Defaulting to 0.0.\"\n                    )\n                    validated_result['confidence_score']\
        \ = 0.0\n            else:\n                # The 'confidence_score' key itself\
        \ is missing\n                logger.warning(\"LLM response missing 'confidence_score'\
        \ key. Defaulting to 0.0.\")\n                validated_result['confidence_score']\
        \ = 0.0\n\n            return validated_result\n            # --- END OF MODIFIED\
        \ VALIDATION LOGIC ---\n                \n        except json.JSONDecodeError:\n\
        \            logger.error(f\"Failed to decode JSON from LLM response: {response.text}\"\
        )\n            return None\n        except exceptions.GoogleAPIError as e:\
        \  # <-- Specific API error handling\n            logger.error(f\"A Google\
        \ API error occurred with the Gemini call: {e}\", exc_info=True)\n       \
        \     return None\n        except Exception as e:\n            logger.error(f\"\
        An unexpected error occurred with the Gemini API call: {e}\", exc_info=True)\n\
        \            return None"
      "ollama_selector.py": "# src/selectors/ollama_selector.py\nimport os\nimport\
        \ logging\nimport json\nfrom typing import List, Dict, Any, Optional\n\nimport\
        \ ollama # <-- New import\n\nfrom src.retriever.hybrid_retriever import HybridRetriever\n\
        from src import config\n\nlogger = logging.getLogger(__name__)\n\nclass OllamaSelector:\n\
        \    \"\"\"\n    Uses a local LLM via Ollama to select the best ontology term\n\
        \    from a list of candidates.\n    \"\"\"\n    def __init__(self, retriever:\
        \ HybridRetriever):\n        \"\"\"\n        Initializes the OllamaSelector.\n\
        \n        Args:\n            retriever (HybridRetriever): An initialized retriever\
        \ instance,\n                                         used to fetch full details\
        \ of candidate terms.\n        \"\"\"\n        self.retriever = retriever\n\
        \        self.prompt_template = self._load_prompt_template()\n        \n \
        \       # We get the model name from config, but no API key or client is needed\
        \ for Ollama.\n        self.model_name = config.OLLAMA_SELECTOR_MODEL_NAME\
        \ # Assumes you have this in your config\n        \n        # You might want\
        \ to add a check here to ensure the Ollama service is running.\n        try:\n\
        \            ollama.ps()\n            logger.info(\"Ollama service is running.\
        \ Selector initialized for model: %s\", self.model_name)\n        except Exception\
        \ as exc:\n            logger.error(\"Ollama service not detected. Please\
        \ ensure Ollama is running.\")\n            raise ConnectionError(\"Ollama\
        \ service not available.\") from exc\n\n\n    def _load_prompt_template(self)\
        \ -> str:\n        \"\"\"Loads the prompt template from the file.\"\"\"\n\
        \        # This method is unchanged\n        template_path = os.path.join(config.PROJECT_ROOT,\
        \ \"prompts\", \"strict_final_selection.tpl\")\n        try:\n           \
        \ with open(template_path, 'r', encoding='utf-8') as f:\n                return\
        \ f.read()\n        except FileNotFoundError:\n            logger.error(\"\
        Prompt template not found at %s\", template_path)\n            raise\n\n \
        \   def _format_candidates_for_prompt(self, candidates: List[Dict[str, Any]])\
        \ -> str:\n        \"\"\"Formats the list of candidate documents into a string\
        \ for the prompt.\"\"\"\n        # This method is unchanged\n        formatted_list\
        \ = []\n        for i, candidate in enumerate(candidates):\n            term_id\
        \ = candidate.get(\"id\")\n            if not term_id:\n                continue\n\
        \            \n            details = self.retriever.get_term_details(term_id)\n\
        \            if not details:\n                continue\n\n            label\
        \ = details.get(\"label\", \"N/A\")\n            definition = details.get(\"\
        definition\", \"No definition available.\")\n            synonyms = \"; \"\
        .join(details.get(\"synonyms\", [])) or \"None\"\n\n            formatted_list.append(\n\
        \                f\"{i+1}. ID: {term_id}\\n\"\n                f\"   Label:\
        \ {label}\\n\"\n                f\"   Definition: {definition}\\n\"\n    \
        \            f\"   Synonyms: {synonyms}\"\n            )\n        return \"\
        \\n\\n\".join(formatted_list)\n\n    def select_best_term(self, query: str,\
        \ candidates: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:\n       \
        \ \"\"\"\n        Prompts Ollama to select the best term and parses the JSON\
        \ response.\n\n        Args:\n            query (str): The original user query.\n\
        \            candidates (List[Dict[str, Any]]): The list of candidate documents.\n\
        \n        Returns:\n            A dictionary with {\"chosen_id\": str, \"\
        explanation\": str}, or None on failure.\n        \"\"\"\n        if not candidates:\n\
        \            return None\n\n        candidate_str = self._format_candidates_for_prompt(candidates)\n\
        \        # The full prompt is created just like before.\n        full_prompt\
        \ = self.prompt_template.replace(\"[USER_ENTITY]\", query).replace(\"[CANDIDATE_LIST]\"\
        , candidate_str)\n        \n        logger.info(\"Sending request to Ollama\
        \ for query: '%s' with model '%s'\", query, self.model_name)\n        try:\n\
        \            # --- CORRECTED OLLAMA CALL ---\n            # The entire prompt\
        \ goes into a single 'user' message.\n            # We use format='json' to\
        \ ensure the output is valid JSON.\n            response = ollama.chat(\n\
        \                model=self.model_name,\n                messages=[\n    \
        \                {\n                        'role': 'user',\n            \
        \            'content': full_prompt,\n                    },\n           \
        \     ],\n                format='json' # This is a key feature to get structured\
        \ output!\n            )\n\n            # The response content should be a\
        \ JSON string.\n            response_content = response['message']['content']\n\
        \            \n            # Parse the JSON response\n            result =\
        \ json.loads(response_content)\n            \n            # --- MODIFIED VALIDATION\
        \ LOGIC ---\n            \n            # 1. The 'chosen_id' key is mandatory.\
        \ Fail if it's missing or null.\n            if \"chosen_id\" not in result\
        \ or result.get(\"chosen_id\") is None:\n                logger.error(\n \
        \                   \"LLM response is invalid: Missing the mandatory 'chosen_id'\
        \ key. Response: %s\",\n                    result\n                )\n  \
        \              return None\n            \n            # 2. Start building\
        \ the result with the mandatory key.\n            validated_result = {\n \
        \               'chosen_id': result['chosen_id']\n            }\n\n      \
        \      # 3. Handle optional 'explanation' with a specific warning if missing.\n\
        \            if 'explanation' in result:\n                validated_result['explanation']\
        \ = result['explanation']\n            else:\n                logger.warning(\"\
        LLM response missing 'explanation' key. Using default value.\")\n        \
        \        validated_result['explanation'] = 'No explanation provided.'\n\n\
        \            # 4. Handle optional 'confidence' with distinct warnings.\n \
        \           if 'confidence_score' in result:\n                try:\n     \
        \               # Key exists, so try to convert it\n                    validated_result['confidence_score']\
        \ = float(result['confidence_score'])\n                except (ValueError,\
        \ TypeError):\n                    # Key exists, but the value is not a valid\
        \ float\n                    logger.warning(\n                        \"Invalid\
        \ confidence_score value in response: '%s'. Defaulting to 0.0.\",\n      \
        \                  result.get('confidence_score')\n                    )\n\
        \                    validated_result['confidence_score'] = 0.0\n        \
        \    else:\n                # The 'confidence_score' key itself is missing\n\
        \                logger.warning(\"LLM response missing 'confidence_score'\
        \ key. Defaulting to 0.0.\")\n                validated_result['confidence_score']\
        \ = 0.0\n\n            return validated_result\n            # --- END OF MODIFIED\
        \ VALIDATION LOGIC ---\n                \n        except json.JSONDecodeError:\n\
        \            # This is less likely with format='json' but is good practice\
        \ to keep.\n            logger.error(\"Failed to decode JSON from Ollama response:\
        \ %s\", response_content)\n            return None\n        except ollama.ResponseError\
        \ as e:\n            logger.error(\"An error occurred with the Ollama API\
        \ call: %s - %s\", e.status_code, e.error)\n            return None\n    \
        \    except (ConnectionError, TimeoutError) as e:\n            logger.error(\"\
        Network error during the Ollama call: %s\", e, exc_info=True)\n          \
        \  return None\n        except RuntimeError as e:\n            logger.error(\"\
        Runtime error during the Ollama call: %s\", e, exc_info=True)\n          \
        \  return None"
    "reranker":
      "__init__.py": |-
        # src/reranker/__init__.py
        # This file makes Python treat the directory 'reranker' as a package.

        from .llm_reranker import LLMReranker

        __all__ = ["LLMReranker"]
      "llm_reranker.py": "# src/reranker/llm_reranker.py\n\nimport json\nimport logging\n\
        from typing import List, Dict, Optional, Tuple, Any, Union\n\nfrom sentence_transformers\
        \ import CrossEncoder\nimport torch\n\nlogger = logging.getLogger(__name__)\n\
        if not logger.hasHandlers():\n    logging.basicConfig(level=logging.INFO,\
        \ format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\nclass\
        \ LLMReranker:\n    \"\"\"\n    Reranks documents using a CrossEncoder model.\
        \ It loads enriched documents\n    from one or more files and uses them to\
        \ rerank candidate documents for a given query.\n    \"\"\"\n\n    def __init__(self,\n\
        \                 model_name: str,\n                 enriched_docs_paths:\
        \ Union[str, List[str]],\n                 device: Optional[str] = None):\n\
        \        \"\"\"\n        Initializes the LLMReranker.\n\n        Args:\n \
        \           model_name (str): The name of the CrossEncoder model to use.\n\
        \            enriched_docs_paths (Union[str, List[str]]): A single path or\
        \ a list of paths\n                                                      \
        \   to JSON files containing enriched documents.\n            device (Optional[str]):\
        \ The device to run the model on (\"cuda\", \"cpu\").\n        \"\"\"\n  \
        \      self.model_name = model_name\n        self.enriched_docs_paths = enriched_docs_paths\n\
        \        self.device = device or (\"cuda\" if torch.cuda.is_available() else\
        \ \"cpu\")\n\n        try:\n            logger.info(f\"Loading CrossEncoder\
        \ model: {self.model_name} on {self.device}\")\n            self.model = CrossEncoder(self.model_name,\
        \ device=self.device, trust_remote_code=True)\n            model_max_length\
        \ = getattr(self.model.tokenizer, 'model_max_length', 512)\n            if\
        \ model_max_length > 10000: # Handle unrealistic default values\n        \
        \        model_max_length = 512\n            self.model.max_length = model_max_length\n\
        \            logger.info(f\"Set CrossEncoder max_length to: {self.model.max_length}\"\
        )\n        except Exception as e:\n            logger.error(f\"Failed to load\
        \ CrossEncoder model {self.model_name}: {e}\", exc_info=True)\n          \
        \  raise\n\n        self.doc_texts = self._load_enriched_documents()\n   \
        \     if not self.doc_texts:\n            logger.error(f\"No documents loaded\
        \ from {self.enriched_docs_paths}. Reranker will be ineffective.\")\n\n  \
        \  def _load_enriched_documents(self) -> Dict[str, str]:\n        \"\"\"\n\
        \        Loads enriched documents from one or more files and creates a single\n\
        \        mapping from document ID to text.\n\n        Returns:\n         \
        \   Dict[str, str]: A dictionary mapping document CURIEs to their enriched\
        \ text.\n        \"\"\"\n        doc_map = {}\n        paths_to_load = self.enriched_docs_paths\n\
        \        if isinstance(paths_to_load, str):\n            paths_to_load = [paths_to_load]\n\
        \n        for path in paths_to_load:\n            try:\n                with\
        \ open(path, 'r', encoding='utf-8') as f:\n                    enriched_data\
        \ = json.load(f)\n                \n                for item in enriched_data:\n\
        \                    if \"id\" in item and \"text\" in item:\n           \
        \             doc_map[item[\"id\"]] = item[\"text\"]\n            except FileNotFoundError:\n\
        \                logger.error(f\"Enriched documents file not found: {path}\"\
        )\n            except json.JSONDecodeError:\n                logger.error(f\"\
        Error decoding JSON from {path}\")\n            except Exception as e:\n \
        \               logger.error(f\"An unexpected error occurred while loading\
        \ {path}: {e}\", exc_info=True)\n        \n        logger.info(f\"Loaded a\
        \ total of {len(doc_map)} enriched documents from {len(paths_to_load)} file(s).\"\
        )\n        return doc_map\n\n    def rerank(self, query: str, documents: List[Dict[str,\
        \ Any]], top_n: Optional[int] = None) -> List[Dict[str, Any]]:\n        \"\
        \"\"\n        Reranks a list of documents based on their relevance to a query.\n\
        \        (This method requires no changes as it uses the pre-loaded self.doc_texts\
        \ map).\n        \"\"\"\n        if not query or not documents:\n        \
        \    return documents[:top_n] if top_n is not None else documents\n\n    \
        \    if not self.doc_texts:\n            logger.error(\"No enriched document\
        \ texts loaded. Cannot perform reranking.\")\n            return documents[:top_n]\
        \ if top_n is not None else documents\n\n        sentence_pairs: List[Tuple[str,\
        \ str]] = []\n        valid_documents_for_reranking: List[Dict[str, Any]]\
        \ = []\n\n        for doc in documents:\n            doc_id = doc.get(\"id\"\
        )\n            if doc_id:\n                doc_text = self.doc_texts.get(doc_id)\n\
        \                if doc_text:\n                    sentence_pairs.append((query,\
        \ doc_text))\n                    valid_documents_for_reranking.append(doc)\n\
        \                else:\n                    logger.warning(f\"Could not find\
        \ enriched text for document ID '{doc_id}'. Skipping for reranking.\")\n \
        \       \n        if not sentence_pairs:\n            logger.warning(\"No\
        \ valid document texts found for the given candidates.\")\n            return\
        \ []\n\n        logger.info(f\"Reranking {len(sentence_pairs)} document(s)\
        \ for query: '{query}'\")\n        scores = self.model.predict(sentence_pairs,\
        \ show_progress_bar=False, batch_size=32)\n\n        for i, doc in enumerate(valid_documents_for_reranking):\n\
        \            doc[\"rerank_score\"] = float(scores[i])\n\n        reranked_documents\
        \ = sorted(valid_documents_for_reranking, key=lambda x: x.get(\"rerank_score\"\
        , -float('inf')), reverse=True)\n\n        return reranked_documents[:top_n]\
        \ if top_n is not None else reranked_documents\n\n# --- Updated Example Usage\
        \ ---\nif __name__ == '__main__':\n    from pathlib import Path\n    import\
        \ random\n\n    PROJECT_ROOT_FOR_MAIN = Path(__file__).resolve().parent.parent.parent\n\
        \    if str(PROJECT_ROOT_FOR_MAIN) not in sys.path:\n        sys.path.append(str(PROJECT_ROOT_FOR_MAIN))\n\
        \n    try:\n        from src.config import ONTOLOGIES_CONFIG, RERANKER_MODEL_NAME,\
        \ EMBEDDING_DEVICE, DEFAULT_RERANK_K\n    except ImportError as e:\n     \
        \   logger.error(f\"Error importing from src.config: {e}. Please ensure config\
        \ is set up.\", exc_info=True)\n        sys.exit(1)\n\n    logger.info(\"\
        --- Running LLMReranker Example with Multi-Ontology Setup ---\")\n\n    #\
        \ --- Collect all enriched doc paths from config ---\n    all_doc_paths =\
        \ [\n        data['enriched_docs_path'] \n        for data in ONTOLOGIES_CONFIG.values()\
        \ \n        if \"enriched_docs_path\" in data and os.path.exists(data['enriched_docs_path'])\n\
        \    ]\n\n    if not all_doc_paths:\n        logger.error(\"No existing enriched\
        \ document files found based on ONTOLOGIES_CONFIG. Please run ingestion first.\"\
        )\n        sys.exit(1)\n\n    logger.info(f\"Found {len(all_doc_paths)} enriched\
        \ document file(s) to load.\")\n\n    # --- Load sample documents from the\
        \ first available file for the demo ---\n    sample_documents_from_retriever\
        \ = []\n    try:\n        with open(all_doc_paths[0], 'r', encoding='utf-8')\
        \ as f:\n            sample_data = json.load(f)\n        \n        num_to_sample\
        \ = min(len(sample_data), 5)\n        for i in range(num_to_sample):\n   \
        \         doc = sample_data[i]\n            sample_documents_from_retriever.append({\n\
        \                \"id\": doc.get(\"id\"),\n                \"label\": doc.get(\"\
        label\", \"N/A\"),\n                \"retriever_score\": random.uniform(0.5,\
        \ 1.0)\n            })\n        logger.info(f\"Created {len(sample_documents_from_retriever)}\
        \ sample documents for the demo from '{all_doc_paths[0]}'.\")\n    except\
        \ Exception as e:\n        logger.error(f\"Failed to create sample documents:\
        \ {e}\", exc_info=True)\n        sys.exit(1)\n\n    # --- Reranker Initialization\
        \ and Usage ---\n    try:\n        reranker = LLMReranker(\n            model_name=RERANKER_MODEL_NAME,\n\
        \            enriched_docs_paths=all_doc_paths, # Pass the list of paths\n\
        \            device=EMBEDDING_DEVICE\n        )\n\n        query = \"Garlic\"\
        \n        logger.info(f\"\\nOriginal sample documents for query '{query}':\"\
        )\n        for doc in sample_documents_from_retriever:\n            logger.info(f\"\
        \  ID: {doc.get('id', 'N/A')}, Label: {doc.get('label', 'N/A')}\")\n\n   \
        \     reranked_results = reranker.rerank(query, sample_documents_from_retriever,\
        \ top_n=DEFAULT_RERANK_K)\n\n        logger.info(f\"\\nReranked documents\
        \ (top {DEFAULT_RERANK_K}):\")\n        for doc in reranked_results:\n   \
        \         logger.info(f\"  ID: {doc.get('id')}, Label: {doc.get('label')},\
        \ Rerank Score: {doc.get('rerank_score', 0.0):.4f}\")\n\n    except Exception\
        \ as e:\n        logger.error(f\"An error occurred in the main example: {e}\"\
        , exc_info=True)\n\n    logger.info(\"--- LLMReranker Example Finished ---\"\
        )"
    "retriever":
      "__init__.py": ""
      "hybrid_retriever.py": "# src/retriever/hybrid_retriever.py\nimport json\nimport\
        \ os\nfrom whoosh.index import open_dir as open_whoosh_index\nfrom whoosh.qparser\
        \ import MultifieldParser, OrGroup\nfrom sentence_transformers import SentenceTransformer\n\
        import logging\nfrom typing import List, Optional # Added List and Optional\n\
        \n# --- Add project root to sys.path if running script directly ---\nif __name__\
        \ == '__main__':\n    import sys\n    PROJECT_ROOT_FOR_DIRECT_RUN = os.path.abspath(os.path.join(os.path.dirname(__file__),\
        \ '..', '..'))\n    if PROJECT_ROOT_FOR_DIRECT_RUN not in sys.path:\n    \
        \    sys.path.insert(0, PROJECT_ROOT_FOR_DIRECT_RUN)\n# --- End sys.path modification\
        \ ---\n\nfrom src.vector_store.faiss_store import FAISSVectorStore\nfrom src.config\
        \ import (\n    ONTOLOGIES_CONFIG,\n    EMBEDDING_MODEL_NAME,\n    DEFAULT_K_LEXICAL,\n\
        \    DEFAULT_K_VECTOR,\n)\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s\
        \ - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n\
        class HybridRetriever:\n    def __init__(self):\n        \"\"\"\n        Initializes\
        \ the HybridRetriever to work with multiple, separate ontologies\n       \
        \ defined in ONTOLOGIES_CONFIG.\n        \"\"\"\n        logger.info(\"Initializing\
        \ HybridRetriever for multiple ontologies...\")\n        \n        self.ontology_data_stores\
        \ = {}\n        self.whoosh_searchers = {}\n        self.whoosh_parsers =\
        \ {}\n        self.faiss_stores = {}\n        self.ontology_names = list(ONTOLOGIES_CONFIG.keys())\n\
        \        self.prefix_to_name_map = {v['prefix']: k for k, v in ONTOLOGIES_CONFIG.items()}\n\
        \n        logger.info(f\"Loading embedding model: {EMBEDDING_MODEL_NAME}\"\
        )\n        self.embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME,\
        \ trust_remote_code=True)\n        \n        for name, config_data in ONTOLOGIES_CONFIG.items():\n\
        \            logger.info(f\"--- Initializing resources for ontology: '{name}'\
        \ ---\")\n            \n            # ... (rest of the __init__ method is\
        \ unchanged) ...\n            dump_path = config_data['dump_json_path']\n\
        \            logger.info(f\"Loading ontology data from: {dump_path}\")\n \
        \           if not os.path.exists(dump_path):\n                raise FileNotFoundError(f\"\
        Ontology dump for '{name}' not found: {dump_path}\")\n            with open(dump_path,\
        \ 'r', encoding='utf-8') as f:\n                self.ontology_data_stores[name]\
        \ = json.load(f)\n            logger.info(f\"Loaded {len(self.ontology_data_stores[name])}\
        \ entries for '{name}'.\")\n\n            whoosh_dir = config_data['whoosh_index_dir']\n\
        \            logger.info(f\"Loading Whoosh index from: {whoosh_dir}\")\n \
        \           if not os.path.exists(whoosh_dir) or not os.listdir(whoosh_dir):\n\
        \                raise FileNotFoundError(f\"Whoosh index for '{name}' not\
        \ found or empty: {whoosh_dir}\")\n            \n            whoosh_ix = open_whoosh_index(whoosh_dir)\n\
        \            self.whoosh_searchers[name] = whoosh_ix.searcher()\n        \
        \    whoosh_fields = [\"label\", \"synonyms\", \"definition\", \"relations_text\"\
        ]\n            self.whoosh_parsers[name] = MultifieldParser(whoosh_fields,\
        \ schema=whoosh_ix.schema, group=OrGroup)\n            logger.info(f\"Whoosh\
        \ index for '{name}' loaded.\")\n\n            faiss_index_path = config_data['faiss_index_path']\n\
        \            faiss_metadata_path = config_data['faiss_metadata_path']\n  \
        \          logger.info(f\"Initializing FAISS store for '{name}' (index: {faiss_index_path},\
        \ metadata: {faiss_metadata_path})...\")\n            \n            faiss_store\
        \ = FAISSVectorStore(\n                index_path=faiss_index_path,\n    \
        \            metadata_path=faiss_metadata_path,\n                embeddings_file_path=None\n\
        \            )\n            if not faiss_store.index or not faiss_store.metadata:\n\
        \                 raise FileNotFoundError(f\"FAISS index or metadata for '{name}'\
        \ not found. Please build it first.\")\n            self.faiss_stores[name]\
        \ = faiss_store\n            logger.info(f\"FAISS store for '{name}' initialized.\"\
        )\n\n        logger.info(\"HybridRetriever initialized successfully for all\
        \ configured ontologies.\")\n\n    def _get_stores_to_query(self, store_dict,\
        \ target_ontologies):\n        \"\"\"Helper to select which stores (Whoosh/FAISS)\
        \ to query.\"\"\"\n        if target_ontologies is None:\n            # If\
        \ no specific targets, use all available stores\n            return store_dict.items()\n\
        \        \n        # Filter to only the targeted stores that actually exist\n\
        \        stores_to_query = []\n        for name in target_ontologies:\n  \
        \          if name in store_dict:\n                stores_to_query.append((name,\
        \ store_dict[name]))\n            else:\n                logger.warning(f\"\
        Requested ontology '{name}' not found in available stores. It will be skipped.\"\
        )\n        return stores_to_query\n\n    def _lexical_search(self, query_string,\
        \ limit=DEFAULT_K_LEXICAL, target_ontologies: Optional[List[str]] = None):\n\
        \        \"\"\"Performs lexical search on all or a subset of Whoosh indexes.\"\
        \"\"\n        all_results = []\n        if not query_string:\n           \
        \ return all_results\n\n        # ### CHANGED: Select which searchers to use\
        \ ###\n        searchers_to_query = self._get_stores_to_query(self.whoosh_searchers,\
        \ target_ontologies)\n        if not searchers_to_query:\n            logger.warning(\"\
        Lexical search: No valid target ontologies specified or found.\")\n      \
        \      return []\n\n        for name, searcher in searchers_to_query:\n  \
        \          try:\n                parser = self.whoosh_parsers[name]\n    \
        \            query = parser.parse(query_string)\n                search_results\
        \ = searcher.search(query, limit=limit)\n                \n              \
        \  for hit in search_results:\n                    hit_fields = hit.fields()\n\
        \                    term_curie = hit_fields.get('curie')\n              \
        \      if not term_curie: continue\n                    \n               \
        \     all_results.append({\n                        \"id\": term_curie, \"\
        label\": hit_fields.get('label', 'N/A'),\n                        \"score\"\
        : hit.score, \"source\": \"lexical\", \"source_ontology\": name,\n       \
        \             })\n            except Exception as e:\n                logger.error(f\"\
        Error during lexical search in '{name}' for '{query_string}': {e}\", exc_info=True)\n\
        \        \n        all_results.sort(key=lambda x: x['score'], reverse=True)\n\
        \        return all_results[:limit]\n\n    def _vector_search(self, query_string,\
        \ k=DEFAULT_K_VECTOR, target_ontologies: Optional[List[str]] = None):\n  \
        \      \"\"\"Performs vector search on all or a subset of FAISS indexes.\"\
        \"\"\n        all_results = []\n        if not query_string:\n           \
        \ return all_results\n\n        # ### CHANGED: Select which stores to use\
        \ ###\n        stores_to_query = self._get_stores_to_query(self.faiss_stores,\
        \ target_ontologies)\n        if not stores_to_query:\n            logger.warning(\"\
        Vector search: No valid target ontologies specified or found.\")\n       \
        \     return []\n\n        try:\n            query_vector = self.embedding_model.encode([query_string],\
        \ convert_to_numpy=True)\n            \n            for name, store in stores_to_query:\n\
        \                distances, _, metadata_items = store.search(query_vector,\
        \ k=k)\n                for i, item in enumerate(metadata_items):\n      \
        \              all_results.append({\n                        \"id\": item['id'],\
        \ \"label\": item['label'], \"score\": float(distances[i]),\n            \
        \            \"source\": \"vector\", \"source_ontology\": name,\n        \
        \            })\n        except Exception as e:\n            logger.error(f\"\
        Error during vector search for '{query_string}': {e}\", exc_info=True)\n\n\
        \        all_results.sort(key=lambda x: x['score'])\n        return all_results[:k]\n\
        \n    # ### CHANGED: Added 'target_ontologies' parameter ###\n    def search(self,\
        \ query_string, lexical_limit=DEFAULT_K_LEXICAL, vector_k=DEFAULT_K_VECTOR,\
        \ target_ontologies: Optional[List[str]] = None):\n        \"\"\"\n      \
        \  Performs hybrid search on all or a targeted subset of ontologies.\n\n \
        \       Args:\n            query_string (str): The search query.\n       \
        \     lexical_limit (int): Max number of results from lexical search.\n  \
        \          vector_k (int): Max number of results from vector search.\n   \
        \         target_ontologies (List[str], optional): A list of ontology names\
        \ to search\n                                                     (e.g., [\"\
        foodon\", \"chebi\"]).\n                                                 \
        \    If None, searches all ontologies.\n        \"\"\"\n        lexical_results\
        \ = self._lexical_search(query_string, limit=lexical_limit, target_ontologies=target_ontologies)\n\
        \        vector_results = self._vector_search(query_string, k=vector_k, target_ontologies=target_ontologies)\n\
        \        \n        return {\n            \"query\": query_string,\n      \
        \      \"lexical_results\": lexical_results,\n            \"vector_results\"\
        : vector_results,\n        }\n\n    def get_term_details(self, term_id: str):\n\
        \        matched_prefix = None\n        for prefix in self.prefix_to_name_map.keys():\n\
        \            if term_id.startswith(prefix):\n                if matched_prefix\
        \ is None or len(prefix) > len(matched_prefix):\n                    matched_prefix\
        \ = prefix\n        \n        if not matched_prefix:\n            logger.warning(f\"\
        Could not determine ontology for term_id '{term_id}'.\")\n            return\
        \ None\n            \n        ontology_name = self.prefix_to_name_map[matched_prefix]\n\
        \        term_data = self.ontology_data_stores.get(ontology_name, {}).get(term_id)\n\
        \        \n        if term_data:\n            term_data = dict(term_data)\n\
        \            term_data['id'] = term_id\n        return term_data\n\n    def\
        \ close(self):\n        # ... (Unchanged) ...\n        for name, searcher\
        \ in self.whoosh_searchers.items():\n            if searcher:\n          \
        \      searcher.close()\n                logger.info(f\"Whoosh searcher for\
        \ '{name}' closed.\")\n\n# ### CHANGED: Updated example usage to demonstrate\
        \ new functionality ###\nif __name__ == '__main__':\n    logger.info(\"Running\
        \ HybridRetriever example...\")\n    retriever = None\n    try:\n        retriever\
        \ = HybridRetriever()\n        \n        print(\"\\n\\n\" + \"=\"*80)\n  \
        \      print(\"✅ 1. Searching for 'cheese' across ALL ontologies (default\
        \ behavior)\")\n        print(\"=\"*80)\n        results = retriever.search(\"\
        cheese\", lexical_limit=2, vector_k=2)\n        print(json.dumps(results,\
        \ indent=2))\n\n        print(\"\\n\\n\" + \"=\"*80)\n        # Assuming you\
        \ have an ontology named 'foodon' in your config\n        print(\"✅ 2. Searching\
        \ for 'cheese' ONLY in the 'foodon' ontology\")\n        print(\"=\"*80)\n\
        \        results_foodon = retriever.search(\"cheese\", lexical_limit=2, vector_k=2,\
        \ target_ontologies=[\"foodon\"])\n        print(json.dumps(results_foodon,\
        \ indent=2))\n        \n        print(\"\\n\\n\" + \"=\"*80)\n        # Assuming\
        \ you have an ontology named 'chebi' in your config\n        print(\"✅ 3.\
        \ Searching for 'chemical entity' ONLY in the 'chebi' ontology\")\n      \
        \  print(\"=\"*80)\n        results_chebi = retriever.search(\"chemical entity\"\
        , lexical_limit=2, vector_k=2, target_ontologies=[\"chebi\"])\n        print(json.dumps(results_chebi,\
        \ indent=2))\n\n        print(\"\\n\\n\" + \"=\"*80)\n        print(\"❌ 4.\
        \ Searching with an invalid ontology name (should be skipped gracefully)\"\
        )\n        print(\"=\"*80)\n        results_invalid = retriever.search(\"\
        cheese\", lexical_limit=2, vector_k=2, target_ontologies=[\"non_existent_ontology\"\
        ])\n        print(json.dumps(results_invalid, indent=2)) # Should return empty\
        \ lists\n\n    except FileNotFoundError as e:\n        logger.error(f\"\\\
        nERROR: A required file was not found: {e}\", exc_info=True)\n    except Exception\
        \ as e:\n        logger.error(f\"\\nAn unexpected error occurred during example\
        \ run: {e}\", exc_info=True)\n    finally:\n        if retriever:\n      \
        \      retriever.close()"
    "tests":
      "__init__.py": ""
      "test_lexical_index.py": |-
        import pytest
        import os
        import json
        import tempfile
        import shutil
        from whoosh.index import open_dir
        from whoosh.qparser import QueryParser

        # Adjust path to import modules from src
        import sys
        sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

        from ingestion.parse_ontology import main as parse_ontology_main # Need to run parsing first
        from ingestion.build_lexical_index import build_index
        import config # Need config to override paths

        # Define a known CURIE and search terms from the test snippet
        TEST_CURIE = "FOODON:00001100" # apple
        TEST_LABEL = "apple"
        TEST_SYNONYM = "eating apple"
        TEST_DEFINITION_KEYWORD = "fruit" # Keyword from definition "The fruit of a Malus domestica tree."
        TEST_RELATION_KEYWORD = "apple tree" # Keyword from relations_text (target label)


        # Fixture to create a temporary directory for the test index
        @pytest.fixture(scope="module")
        def temp_index_dir():
            """Creates a temporary directory for the Whoosh index."""
            path = tempfile.mkdtemp(prefix="whoosh_test_index_")
            print(f"Created temp index dir: {path}")
            yield path
            print(f"Cleaning up temp index dir: {path}")
            shutil.rmtree(path)

        # Fixture to run the full ingestion pipeline on the test snippet and build index
        @pytest.fixture(scope="module")
        def built_test_index(temp_index_dir):
            """Runs parse_ontology and build_index on the test snippet."""
            # Use temp files/dirs for test data and index
            with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as tmp_json:
                test_json_path = tmp_json.name

            # Temporarily override config paths for the test
            original_ontology_file = config.ONTOLOGY_FILE
            original_dump_path = config.ONTOLOGY_DUMP_PATH
            original_index_dir = config.WHOOSH_INDEX_DIR

            config.ONTOLOGY_FILE = config.TEST_ONTOLOGY_FILE # Use the small test snippet
            config.ONTOLOGY_DUMP_PATH = test_json_path
            config.WHOOSH_INDEX_DIR = temp_index_dir

            try:
                # 1. Run parsing
                print("\nRunning parse_ontology_main for test index build...")
                parse_ontology_main()
                assert os.path.exists(test_json_path), "Ontology dump JSON not created!"

                # 2. Run index building
                print("\nRunning build_index for test index build...")
                build_index(test_json_path, temp_index_dir)
                assert os.path.exists(os.path.join(temp_index_dir, 'SCHEMA')), "Whoosh index not created!"

                # Yield the index directory path
                yield temp_index_dir

            finally:
                # Restore original paths
                config.ONTOLOGY_FILE = original_ontology_file
                config.ONTOLOGY_DUMP_PATH = original_dump_path
                config.WHOOSH_INDEX_DIR = original_index_dir
                # Clean up temporary JSON file
                if os.path.exists(test_json_path):
                    os.unlink(test_json_path)


        def test_index_exists(built_test_index):
            """Tests if the index directory was created and contains index files."""
            assert os.path.exists(built_test_index)
            assert os.path.exists(os.path.join(built_test_index, 'SCHEMA'))
            assert os.path.exists(os.path.join(built_test_index, 'MAIN')) # Or other index files

        def test_query_by_label_exact(built_test_index):
            """Tests exact search on the label field."""
            ix = open_dir(built_test_index)
            searcher = ix.searcher()
            # Query the 'label' field
            parser = QueryParser("label", ix.schema)
            query = parser.parse(TEST_LABEL) # "apple"
            results = searcher.search(query)

            print(f"\nSearch results for '{TEST_LABEL}': {results}")

            assert len(results) > 0, f"No results found for '{TEST_LABEL}'"
            # Check if the expected CURIE is among the results, preferably the top one
            assert results[0]['curie'] == TEST_CURIE
            assert results[0]['label'] == TEST_LABEL # Verify stored label

            searcher.close()
            ix.close() # Close index readers/writers

        def test_query_by_label_fuzzy(built_test_index):
            """Tests fuzzy search on the label field."""
            ix = open_dir(built_test_index)
            searcher = ix.searcher()
            parser = QueryParser("label", ix.schema)
            # Fuzzy search for "appl" (e.g. "appl~")
            query = parser.parse("appl~") # Whoosh fuzzy syntax
            results = searcher.search(query)

            print(f"\nFuzzy search results for 'appl~': {results}")

            assert len(results) > 0, "No results found for 'appl~'"
            assert results[0]['curie'] == TEST_CURIE
            assert results[0]['label'] == TEST_LABEL

            searcher.close()
            ix.close()

        def test_query_by_synonym(built_test_index):
            """Tests search on the synonyms field."""
            ix = open_dir(built_test_index)
            searcher = ix.searcher()
            # Query the 'synonyms' field
            parser = QueryParser("synonyms", ix.schema)
            query = parser.parse(TEST_SYNONYM) # "eating apple"
            results = searcher.search(query)

            print(f"\nSearch results for synonym '{TEST_SYNONYM}': {results}")

            assert len(results) > 0, f"No results found for synonym '{TEST_SYNONYM}'"
            assert results[0]['curie'] == TEST_CURIE

            searcher.close()
            ix.close()

        def test_query_by_definition_keyword(built_test_index):
            """Tests search on the definition field."""
            ix = open_dir(built_test_index)
            searcher = ix.searcher()
            # Query the 'definition' field
            parser = QueryParser("definition", ix.schema)
            query = parser.parse(TEST_DEFINITION_KEYWORD) # "fruit"
            results = searcher.search(query)

            print(f"\nSearch results for definition keyword '{TEST_DEFINITION_KEYWORD}': {results}")

            assert len(results) > 0, f"No results found for definition keyword '{TEST_DEFINITION_KEYWORD}'"
            assert results[0]['curie'] == TEST_CURIE

            searcher.close()
            ix.close()

        def test_query_by_relations_keyword(built_test_index):
            """Tests search on the flattened relations_text field."""
            ix = open_dir(built_test_index)
            searcher = ix.searcher()
            # Query the 'relations_text' field
            parser = QueryParser("relations_text", ix.schema)
            # Search for a keyword expected in the relations_text (e.g., target's label or part of target's CURIE)
            # Note: Indexing targets as just CURIEs requires searching for 'FOODON:00001101'
            # Indexing target labels requires adding that logic in build_lexical_index
            # Let's assume for now we search for the CURIE or part of it if indexed as text
            # Or if we enhance build_index to add target labels to relations_text:
            # relations_text = " ".join([f"{prop}: {' '.join(targets)} {' '.join(target_labels)}" for ...])
            # For simplicity with current build_index, let's search the target CURIE part
            query = parser.parse("00001101") # Search part of the target CURIE FOODON:00001101
            results = searcher.search(query)

            print(f"\nSearch results for relations keyword '00001101': {results}")

            assert len(results) > 0, f"No results found for relations keyword '00001101'"
            assert results[0]['curie'] == TEST_CURIE

            searcher.close()
            ix.close()
      "test_parse_ontology.py": |-
        import pytest
        import os
        import json
        import rdflib
        from rdflib import Graph

        # Adjust path to import modules from src
        import sys
        sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

        from ingestion.parse_ontology import (
            load_ontology,
            extract_labels_and_synonyms,
            extract_definitions,
            extract_hierarchy,
            extract_relations,
        )
        from config import TEST_ONTOLOGY_FILE, NAMESPACE_MAP, RELATION_PROPERTIES
        from utils.ontology_utils import uri_to_curie # Import if needed for assertions

        # Define a fixture to load the test graph once for all tests
        @pytest.fixture(scope="module")
        def test_graph():
            """Loads the test ontology snippet into a graph."""
            if not os.path.exists(TEST_ONTOLOGY_FILE):
                pytest.skip(f"Test ontology snippet not found at {TEST_ONTOLOGY_FILE}")
            try:
                return load_ontology(TEST_ONTOLOGY_FILE)
            except Exception as e:
                pytest.fail(f"Failed to load test ontology: {e}")

        # Define a known CURIE from the test snippet
        TEST_CURIE = "FOODON:00001100" # apple
        TEST_CURIE_PARENT1 = "FOODON:00001000" # plant-based food
        TEST_CURIE_PARENT2 = "FOODON:00002000" # Pome fruit
        TEST_CURIE_RELATION_TARGET = "FOODON:00001101" # apple tree (example target)
        TEST_RELATION_NAME = "part_of" # example relation name

        def test_load_ontology(test_graph):
            """Tests if the ontology loads and is an RDFLib Graph."""
            assert isinstance(test_graph, Graph)
            assert len(test_graph) > 0, "Test graph is empty!"
            print(f"Loaded test graph with {len(test_graph)} triples.")


        def test_extract_labels_and_synonyms(test_graph):
            """Tests extraction of labels and synonyms."""
            labels_synonyms = extract_labels_and_synonyms(test_graph)
            print(f"Extracted labels/synonyms: {labels_synonyms}")

            assert TEST_CURIE in labels_synonyms
            apple_data = labels_synonyms[TEST_CURIE]
            assert apple_data['label'] == "apple"
            assert "eating apple" in apple_data['synonyms']
            assert "apple fruit" in apple_data['synonyms']
            assert TEST_CURIE_PARENT1 in labels_synonyms # Should also extract its label/synonyms
            assert labels_synonyms[TEST_CURIE_PARENT1]['label'] == "plant-based food"


        def test_extract_definitions(test_graph):
            """Tests extraction of definitions."""
            definitions = extract_definitions(test_graph)
            print(f"Extracted definitions: {definitions}")

            assert TEST_CURIE in definitions
            assert "The fruit of a Malus domestica tree." in definitions[TEST_CURIE] # Use 'in' for substring check or exact match


        def test_extract_hierarchy(test_graph):
            """Tests extraction of hierarchy (parents and ancestors)."""
            hierarchy = extract_hierarchy(test_graph)
            print(f"Extracted hierarchy: {hierarchy}")

            assert TEST_CURIE in hierarchy
            apple_hierarchy = hierarchy[TEST_CURIE]

            # Check direct parents
            assert TEST_CURIE_PARENT1 in apple_hierarchy['parents']
            assert TEST_CURIE_PARENT2 in apple_hierarchy['parents']
            assert len(apple_hierarchy['parents']) >= 2 # At least 2 parents from the snippet

            # Check ancestors (should include parents and parents' ancestors)
            assert TEST_CURIE_PARENT1 in apple_hierarchy['ancestors']
            assert TEST_CURIE_PARENT2 in apple_hierarchy['ancestors']
            # Assuming plant-based food has no ancestors in this snippet, ancestors == parents here
            # If plant-based food had parents, they should appear in apple's ancestors.
            # For this snippet, ancestors should be the same as parents.
            assert set(apple_hierarchy['ancestors']) == set(apple_hierarchy['parents'])


        def test_extract_relations(test_graph):
            """Tests extraction of specific relations."""
            relations = extract_relations(test_graph, RELATION_PROPERTIES)
            print(f"Extracted relations: {relations}")

            assert TEST_CURIE in relations
            apple_relations = relations[TEST_CURIE]

            # Check if the specific relation from the snippet was found
            assert TEST_RELATION_NAME in apple_relations
            assert TEST_CURIE_RELATION_TARGET in apple_relations[TEST_RELATION_NAME]
            assert len(apple_relations[TEST_RELATION_NAME]) >= 1


        # You could add a test that runs the full main parsing flow using the test snippet
        # and checks the structure/content of the resulting JSON file.
        # Example (requires a temporary file):
        # import tempfile
        # @pytest.fixture
        # def temp_json_path():
        #     with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as tmp:
        #         yield tmp.name
        #     os.unlink(tmp.name)
        #
        # def test_main_parsing_flow(temp_json_path):
        #      # Temporarily override config paths for the test
        #      original_ontology_file = config.ONTOLOGY_FILE
        #      original_dump_path = config.ONTOLOGY_DUMP_PATH
        #      config.ONTOLOGY_FILE = TEST_ONTOLOGY_FILE
        #      config.ONTOLOGY_DUMP_PATH = temp_json_path
        #
        #      try:
        #          parse_ontology.main()
        #          assert os.path.exists(temp_json_path)
        #          with open(temp_json_path, 'r', encoding='utf-8') as f:
        #              data = json.load(f)
        #
        #          assert TEST_CURIE in data
        #          apple_data = data[TEST_CURIE]
        #          assert apple_data['label'] == 'apple'
        #          assert 'synonyms' in apple_data and len(apple_data['synonyms']) > 0
        #          assert 'definition' in apple_data and apple_data['definition'] is not None
        #          assert 'parents' in apple_data and len(apple_data['parents']) > 0
        #          assert 'ancestors' in apple_data and len(apple_data['ancestors']) > 0
        #          assert 'relations' in apple_data and len(apple_data['relations']) > 0
        #
        #      finally:
        #          # Restore original paths
        #          config.ONTOLOGY_FILE = original_ontology_file
        #          config.ONTOLOGY_DUMP_PATH = original_dump_path
    "utils":
      "__init__.py": ""
      "ontology_utils.py": "# src/utils/ontology_utils.py\nimport rdflib\nfrom rdflib\
        \ import URIRef, Namespace\nfrom typing import Optional, Dict, Union\n\n#\
        \ Adjusted: Import CURIE_PREFIX_MAP and use it as the default.\n# This assumes\
        \ that the script/module *importing* ontology_utils\n# has already ensured\
        \ that the project root (e.g., 'onto_rag') is on sys.path,\n# so that 'src'\
        \ is resolvable as a top-level package.\nfrom src.config import CURIE_PREFIX_MAP\
        \ # Corrected import\n\ndef uri_to_curie(uri: Union[str, URIRef], namespace_map:\
        \ Dict[str, str] = CURIE_PREFIX_MAP) -> str:\n    \"\"\"\n    Converts a full\
        \ URI to a CURIE (e.g., http://...#term -> PREFIX:term).\n    Assumes namespace_map\
        \ is structured as {base_uri_str: prefix_str}.\n    \"\"\"\n    uri_str =\
        \ str(uri) # Ensure it's a string\n\n    # Iterate through the provided namespace_map\
        \ (base_uri: prefix)\n    # Sort by length of base_uri descending to match\
        \ longest first (more specific)\n    # This helps avoid issues where one base_uri\
        \ is a prefix of another.\n    # e.g., \"http://purl.obolibrary.org/obo/\"\
        \ and \"http://purl.obolibrary.org/obo/FOODON_\"\n    sorted_namespace_map_items\
        \ = sorted(namespace_map.items(), key=lambda item: len(item[0]), reverse=True)\n\
        \n    for base_uri, prefix in sorted_namespace_map_items:\n        if uri_str.startswith(base_uri):\n\
        \            return f\"{prefix}:{uri_str[len(base_uri):]}\"\n\n    # Fallback\
        \ for common RDF/RDFS/OWL/XSD prefixes if not found in the main map\n    #\
        \ This local map is prefix: base_uri_str\n    common_rdf_prefixes = {\n  \
        \      \"rdf\": str(rdflib.RDF),\n        \"rdfs\": str(rdflib.RDFS),\n  \
        \      \"owl\": str(rdflib.OWL),\n        \"xsd\": str(rdflib.XSD),\n    }\n\
        \    for prefix, base_uri in common_rdf_prefixes.items():\n        if uri_str.startswith(base_uri):\n\
        \            return f\"{prefix}:{uri_str[len(base_uri):]}\"\n            \n\
        \    # If rdflib can make a qname (usually for registered namespaces)\n  \
        \  try:\n        g = rdflib.Graph()\n        # Bind known namespaces to help\
        \ compute_qname\n        # For namespace_map (base_uri: prefix), we need to\
        \ iterate as base_uri, prefix\n        for ns_uri_str_from_map, prefix_from_map\
        \ in namespace_map.items():\n             g.bind(prefix_from_map, Namespace(ns_uri_str_from_map))\n\
        \n        # Bind common RDF ones too, in case they weren't in namespace_map\
        \ or to ensure standard prefixes\n        g.bind(\"owl\", rdflib.OWL)\n  \
        \      g.bind(\"rdf\", rdflib.RDF)\n        g.bind(\"rdfs\", rdflib.RDFS)\n\
        \        g.bind(\"xsd\", rdflib.XSD)\n\n        # compute_qname might fail\
        \ if the URI doesn't match any bound namespace's base\n        # It returns\
        \ (prefix, namespace_uri, local_name)\n        qname_tuple = g.compute_qname(URIRef(uri_str))\n\
        \        return f\"{qname_tuple[0]}:{qname_tuple[2]}\"\n    except Exception:\
        \ # Broad except as compute_qname can raise various things or return unexpected\
        \ tuples\n        pass # If rdflib fails, just return the original URI string\n\
        \n    return uri_str # If no CURIE conversion possible, return original URI\
        \ string\n\ndef curie_to_uri(curie: str, namespace_map: Dict[str, str] = CURIE_PREFIX_MAP)\
        \ -> Optional[URIRef]:\n    \"\"\"\n    Converts a CURIE (e.g., PREFIX:term)\
        \ to a full rdflib.URIRef.\n    Assumes namespace_map is structured as {base_uri_str:\
        \ prefix_str}.\n    \"\"\"\n    if ':' not in curie:\n        # Try to see\
        \ if it's a default rdflib qname like \"rdf:type\" that rdflib can expand\n\
        \        # This part might be less common if CURIEs are always expected with\
        \ user-defined prefixes\n        try:\n            g = rdflib.Graph()\n  \
        \          # Bind namespaces from the map (base_uri: prefix)\n           \
        \ for ns_uri_str_from_map, prefix_from_map in namespace_map.items():\n   \
        \             g.bind(prefix_from_map, Namespace(ns_uri_str_from_map))\n  \
        \          # Bind common RDF ones\n            g.bind(\"owl\", rdflib.OWL)\n\
        \            g.bind(\"rdf\", rdflib.RDF)\n            g.bind(\"rdfs\", rdflib.RDFS)\n\
        \            g.bind(\"xsd\", rdflib.XSD)\n            \n            # If it's\
        \ something like \"owl:Class\", g.namespace_manager.expand_curie will work\n\
        \            expanded_uri = g.namespace_manager.expand_curie(curie)\n    \
        \        if str(expanded_uri) != curie: # Check if expansion actually happened\n\
        \                 return URIRef(expanded_uri)\n        except Exception:\n\
        \            pass # If expansion fails, proceed to manual lookup\n       \
        \ return None # Not a valid CURIE format for our map, and rdflib couldn't\
        \ expand\n\n    prefix_part, local_name = curie.split(':', 1)\n\n    # Iterate\
        \ through namespace_map (base_uri: prefix) to find the matching prefix\n \
        \   found_base_uri = None\n    for base_uri_key, prefix_val in namespace_map.items():\n\
        \        if prefix_val == prefix_part:\n            found_base_uri = base_uri_key\n\
        \            break\n    \n    if found_base_uri:\n        return URIRef(found_base_uri\
        \ + local_name)\n    else:\n        # Fallback: Try common RDF prefixes if\
        \ not in the custom map\n        # This local map is prefix: rdflib.Namespace\
        \ object\n        common_rdf_namespaces = {\n            \"rdf\": rdflib.RDF,\n\
        \            \"rdfs\": rdflib.RDFS,\n            \"owl\": rdflib.OWL,\n  \
        \          \"xsd\": rdflib.XSD,\n        }\n        if prefix_part in common_rdf_namespaces:\n\
        \            # Access items in the namespace like attributes: common_rdf_namespaces[prefix_part].type\n\
        \            # or by string concatenation: URIRef(str(common_rdf_namespaces[prefix_part])\
        \ + local_name)\n            return URIRef(str(common_rdf_namespaces[prefix_part])\
        \ + local_name)\n        return None # Prefix not found in custom map or common\
        \ RDF prefixes"
    "vector_store":
      "__init__.py": ""
      "faiss_store.py": "# src/vector_store/faiss_store.py\nimport faiss\nimport json\n\
        import numpy as np\nimport os\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO,\
        \ format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\
        \nclass FAISSVectorStore:\n    def __init__(self, index_path, metadata_path,\
        \ embeddings_file_path=None, dimension=None):\n        \"\"\"\n        Initializes\
        \ the FAISSVectorStore.\n        Tries to load an existing index and metadata.\
        \ If not found, and an\n        embeddings_file_path is provided, it will\
        \ attempt to build them.\n        \"\"\"\n        self.index_path = index_path\n\
        \        self.metadata_path = metadata_path\n        self.embeddings_file_path\
        \ = embeddings_file_path\n        self.dimension = dimension\n\n        self.index\
        \ = None\n        self.metadata = []\n\n        if os.path.exists(self.index_path)\
        \ and os.path.exists(self.metadata_path):\n            logger.info(f\"Loading\
        \ existing FAISS index from {self.index_path} and metadata from {self.metadata_path}\"\
        )\n            try:\n                self.load_store()\n            except\
        \ Exception as e:\n                logger.error(f\"Error loading existing\
        \ FAISS store: {e}. Will attempt to rebuild.\", exc_info=True)\n         \
        \       self._try_build_store()\n        elif self.embeddings_file_path and\
        \ os.path.exists(self.embeddings_file_path):\n            logger.info(f\"\
        FAISS index/metadata not found. Attempting to build from {self.embeddings_file_path}\"\
        )\n            self._try_build_store()\n        else:\n            message\
        \ = \"FAISS store cannot be loaded or built. \"\n            if self.embeddings_file_path:\n\
        \                message += f\"Embeddings file '{self.embeddings_file_path}'\
        \ not found.\"\n            else:\n                message += \"No embeddings\
        \ file path provided to build a new store.\"\n            logger.warning(message)\n\
        \n    def _try_build_store(self):\n        \"\"\"Helper method to attempt\
        \ building the store.\"\"\"\n        embeddings_data = self._load_embeddings_data()\n\
        \        if embeddings_data:\n            self.build_index_from_embeddings(embeddings_data)\n\
        \            if self.index is not None and self.metadata:\n              \
        \   self.save_store()\n            else:\n                logger.warning(\"\
        FAISS index or metadata not built successfully. Store will not be saved.\"\
        )\n        else:\n            logger.warning(f\"No embeddings data loaded\
        \ from {self.embeddings_file_path}, FAISS index not built.\")\n\n    def _load_embeddings_data(self):\n\
        \        \"\"\"Loads and validates embeddings data from the specified JSON\
        \ file.\"\"\"\n        if not self.embeddings_file_path or not os.path.exists(self.embeddings_file_path):\n\
        \            logger.error(f\"Embeddings file not found: {self.embeddings_file_path}\"\
        )\n            return None\n        try:\n            with open(self.embeddings_file_path,\
        \ 'r', encoding='utf-8') as f:\n                data = json.load(f)\n    \
        \        if not isinstance(data, list) or not data:\n                logger.error(f\"\
        Embeddings file {self.embeddings_file_path} is empty or not a list.\")\n \
        \               return None\n            if not all('id' in item and 'label'\
        \ in item and 'embedding' in item for item in data):\n                logger.error(f\"\
        Embeddings data in {self.embeddings_file_path} has an incorrect format.\"\
        )\n                return None\n            logger.info(f\"Loaded {len(data)}\
        \ embeddings from {self.embeddings_file_path}\")\n            return data\n\
        \        except Exception as e:\n            logger.error(f\"Failed to load\
        \ embeddings file {self.embeddings_file_path}: {e}\", exc_info=True)\n   \
        \         return None\n\n    def build_index_from_embeddings(self, embeddings_data):\n\
        \        \"\"\"Builds the FAISS index and prepares metadata from loaded embeddings\
        \ data.\"\"\"\n        if not embeddings_data:\n            logger.warning(\"\
        No embeddings data provided to build index.\")\n            return\n\n   \
        \     try:\n            vectors = [item['embedding'] for item in embeddings_data\
        \ if item.get('embedding')]\n            self.metadata = [{'id': item['id'],\
        \ 'label': item['label']} for item in embeddings_data if item.get('embedding')]\n\
        \            \n            if not vectors:\n                logger.warning(\"\
        No valid vectors found in embeddings_data after filtering.\")\n          \
        \      return\n                \n            vectors_np = np.array(vectors,\
        \ dtype='float32')\n\n            if not self.dimension:\n               \
        \ self.dimension = vectors_np.shape[1]\n            elif self.dimension !=\
        \ vectors_np.shape[1]:\n                raise ValueError(f\"Provided dimension\
        \ {self.dimension} does not match embeddings dimension {vectors_np.shape[1]}\"\
        )\n\n            self.index = faiss.IndexFlatL2(self.dimension)\n        \
        \    self.index.add(vectors_np)\n            logger.info(f\"FAISS index built\
        \ successfully with {self.index.ntotal} vectors of dimension {self.dimension}.\"\
        )\n        except Exception as e:\n            logger.error(f\"Error building\
        \ FAISS index: {e}\", exc_info=True)\n            self.index = None\n    \
        \        self.metadata = []\n\n    def save_store(self):\n        \"\"\"Saves\
        \ the FAISS index and metadata to their respective files.\"\"\"\n        if\
        \ self.index is None:\n            logger.warning(\"No FAISS index to save.\"\
        )\n            return\n\n        # *** THIS IS THE RESTORED SAFETY CHECK ***\n\
        \        if not self.metadata:\n            logger.warning(\"FAISS index exists\
        \ but metadata is empty. Aborting save to prevent inconsistent state.\")\n\
        \            return\n\n        os.makedirs(os.path.dirname(self.index_path),\
        \ exist_ok=True)\n        faiss.write_index(self.index, self.index_path)\n\
        \        logger.info(f\"FAISS index saved to {self.index_path}\")\n\n    \
        \    os.makedirs(os.path.dirname(self.metadata_path), exist_ok=True)\n   \
        \     with open(self.metadata_path, 'w', encoding='utf-8') as f:\n       \
        \     json.dump(self.metadata, f)\n        logger.info(f\"FAISS metadata saved\
        \ to {self.metadata_path}\")\n\n    def load_store(self):\n        \"\"\"\
        Loads the FAISS index and metadata from files.\"\"\"\n        self.index =\
        \ faiss.read_index(self.index_path)\n        self.dimension = self.index.d\n\
        \        logger.info(f\"FAISS index loaded. Index has {self.index.ntotal}\
        \ vectors of dim {self.index.d}.\")\n\n        with open(self.metadata_path,\
        \ 'r', encoding='utf-8') as f:\n            self.metadata = json.load(f)\n\
        \        logger.info(f\"FAISS metadata loaded. {len(self.metadata)} items.\"\
        )\n\n        if self.index.ntotal != len(self.metadata):\n            logger.warning(f\"\
        Mismatch between FAISS index size ({self.index.ntotal}) and metadata size\
        \ ({len(self.metadata)}).\")\n\n    def search(self, query_vector, k=5):\n\
        \        \"\"\"Performs a K-Nearest Neighbors search on the FAISS index.\"\
        \"\"\n        if self.index is None:\n            logger.error(\"FAISS index\
        \ not initialized. Cannot perform search.\")\n            return np.array([]),\
        \ np.array([]), []\n\n        if not isinstance(query_vector, np.ndarray):\n\
        \            query_vector = np.array(query_vector, dtype='float32')\n    \
        \    if query_vector.ndim == 1:\n            query_vector = np.expand_dims(query_vector,\
        \ axis=0)\n        \n        if query_vector.shape[1] != self.index.d:\n \
        \           raise ValueError(f\"Query vector dimension ({query_vector.shape[1]})\
        \ does not match index dimension ({self.index.d}).\")\n\n        distances,\
        \ faiss_indices = self.index.search(query_vector, k)\n        \n        all_results_metadata\
        \ = []\n        for i in range(faiss_indices.shape[0]):\n            query_results_metadata\
        \ = []\n            for j in range(faiss_indices.shape[1]):\n            \
        \    idx = faiss_indices[i][j]\n                if idx != -1 and idx < len(self.metadata):\n\
        \                    query_results_metadata.append(self.metadata[idx])\n \
        \           all_results_metadata.append(query_results_metadata)\n        \n\
        \        return (distances[0], faiss_indices[0], all_results_metadata[0])\
        \ if query_vector.shape[0] == 1 else (distances, faiss_indices, all_results_metadata)\n\
        \n# --- Updated build script ---\nif __name__ == '__main__':\n    # Add project\
        \ root to be able to import config\n    import sys\n    sys.path.insert(0,\
        \ os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n\
        \    \n    from src.config import ONTOLOGIES_CONFIG\n    import pathlib\n\n\
        \    logger.info(\"--- Building FAISS Stores for All Configured Ontologies\
        \ ---\")\n\n    for name, config_data in ONTOLOGIES_CONFIG.items():\n    \
        \    index_path = config_data.get('faiss_index_path')\n        metadata_path\
        \ = config_data.get('faiss_metadata_path')\n        embeddings_path = config_data.get('embeddings_path')\n\
        \n        logger.info(f\"\\n--- Processing Ontology: {name} ---\")\n\n   \
        \     if not all([index_path, metadata_path, embeddings_path]):\n        \
        \    logger.warning(f\"Configuration for '{name}' is missing one or more FAISS\
        \ paths. Skipping.\")\n            continue\n\n        pathlib.Path(index_path).unlink(missing_ok=True)\n\
        \        pathlib.Path(metadata_path).unlink(missing_ok=True)\n        logger.info(f\"\
        Removed old index files for '{name}' to ensure fresh build.\")\n\n       \
        \ if not os.path.exists(embeddings_path):\n            logger.error(f\"ERROR:\
        \ Embeddings file not found at {embeddings_path}. Cannot build FAISS index\
        \ for '{name}'. Skipping.\")\n            continue\n\n        store = FAISSVectorStore(\n\
        \            index_path=index_path,\n            metadata_path=metadata_path,\n\
        \            embeddings_file_path=embeddings_path\n        )\n\n        if\
        \ store.index:\n            logger.info(f\"Successfully built and saved FAISS\
        \ index for '{name}' with {store.index.ntotal} vectors.\")\n        else:\n\
        \            logger.error(f\"FAILED to build FAISS index for '{name}'. Check\
        \ logs for errors.\")\n    \n    logger.info(\"\\n--- All FAISS Store Building\
        \ Complete ---\")"
